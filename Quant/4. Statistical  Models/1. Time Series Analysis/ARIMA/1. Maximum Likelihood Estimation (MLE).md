MLE is a widely used statistical method for <mark style="background: #ABF7F7A6;">estimating the parameters of a statistical model. Given a set of observed data</mark>, MLE aims to find the values of the parameters that maximize the likelihood function, which measures the probability of observing the data given the parameter values.

The likelihood function is defined as the probability density function (PDF) or probability mass function (PMF) of the data, treated as a function of the parameters of the model. Mathematically, if we have a set of observations ${x_1, x_2, \ldots, x_n}$ that are assumed to be independent and identically distributed (i.i.d.) according to some PDF or PMF $f(x|\theta)$, where $\theta$ is a vector of parameters to be estimated, then the likelihood function is given by:

$L(\theta|x_1,x_2,\ldots,x_n) = \prod_{i=1}^n f(x_i|\theta)$

The MLE of $\theta$, denoted by $\hat{\theta}_{\text{MLE}}$, is the value of $\theta$ that maximizes the likelihood function. In other words, it is the set of parameter values that make the observed data the most probable.

To find the MLE, we typically take the logarithm of the likelihood function, which is known as the log-likelihood:

$\ell(\theta|x_1,x_2,\ldots,x_n) = \log L(\theta|x_1,x_2,\ldots,x_n) = \sum_{i=1}^n \log f(x_i|\theta)$

Then, we solve the optimization problem:

$\hat{\theta}_{\text{MLE}} = \arg\max_\theta \ell(\theta|x_1,x_2,\ldots,x_n)$

The MLE of $\theta$ can often be found analytically by setting the derivative of the log-likelihood function with respect to each parameter equal to zero and solving for the parameter values. However, in some cases, numerical optimization methods may be necessary.

MLE is widely used in various fields, including finance, economics, and engineering, among others. It is a popular method for estimating parameters of time series models, such as ARIMA and GARCH, as well as for estimating the parameters of asset pricing models, such as the CAPM and Black-Scholes model.