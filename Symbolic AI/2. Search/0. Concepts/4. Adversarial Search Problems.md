
### [[MiniMax]]


- Adversarial search problems involve<mark style="background: #FF5582A6;"> two or more agents competing against each other</mark>.

- The agents take turns making moves, and each agent tries to <mark style="background: #ADCCFFA6;">maximize their own utility or minimize their opponent's utility</mark>.

- Adversarial search problems are often used in games like chess, checkers, and Go, but can also be applied in other domains like cybersecurity.

- The minimax algorithm is a popular algorithm used for solving adversarial search problems. It <mark style="background: #D2B3FFA6;">assumes that both players play optimally and tries to minimize the maximum possible loss</mark>.

- The minimax algorithm can be improved by using <mark style="background: #FF5582A6;">alpha-beta pruning</mark>, which is a technique for reducing the number of nodes explored in the search tree.

- Adversarial search problems with incomplete information are more difficult to solve, as the agents do not have complete information about the state of the game.

- Monte Carlo Tree Search (MCTS) is a popular algorithm used for solving adversarial search problems with incomplete information. It works by repeatedly simulating games and using the results to guide the search towards more promising moves.

- MCTS has been successfully applied in games like poker and Go, where the state of the game is not fully observable.

---

### A game in AI can be represented as a tuple (S, A, P, R, O, T), where:

-   S is the set of all possible <mark style="background: #FFB8EBA6;">game states</mark>
-   A is the set of all possible <mark style="background: #FFF3A3A6;">actions</mark> that can be taken in each state
-   P: S × A → S is a transition function that takes a state and an action and returns the resulting state
-   R: S × A × S → ℤ is a reward function that takes a state, an action, and a resulting state and returns a numeric reward
-   O is the set of all possible <mark style="background: #D2B3FFA6;">outcomes</mark> of the game (e.g., win, lose, draw)
-   T is a function that maps each outcome in O to the corresponding terminal states in S

In addition, a game in AI is often assumed to be:

-   deterministic: the next state is fully determined by the current state and action
-   turn-taking: each player takes turns making a move
-   two-player: there are only two players
-   zero-sum: the total reward for one player is equal and opposite to the total reward for the other player
-   of perfect information: all players have complete knowledge of the game state at all times.

---
### Search

Adversarial search problems involve two or more agents or players that are in conflict with each other, where each agent aims to maximize its own utility or minimize its own loss. These types of problems are commonly encountered in game playing, such as chess, poker, and Go.

To develop a <mark style="background: #FFB8EBA6;">strategy for adversarial search problems</mark>, you can follow these general steps:

1.  Define the game: You need to define the game by specifying the initial state, the set of legal moves, and the rules for transitioning from one state to another.
    
2.  Define the evaluation function: An <mark style="background: #FF5582A6;">evaluation function</mark> is used to evaluate how good a given state is for a particular player. It assigns a numerical score to each state based on how advantageous it is for the player.
    
3.  Choose a search algorithm: There are various search algorithms that can be used to find the best move in a game, such as minimax, alpha-beta pruning, and Monte Carlo tree search. The choice of algorithm will depend on the size and complexity of the game, as well as the available computational resources.
    
4.  Implement the search algorithm: Once you have chosen a search algorithm, you need to implement it in code. This involves defining the search tree, which represents all possible moves and their resulting states.
    
5.  Apply the search algorithm: You can apply the search algorithm to the search tree to find the best move for a given player. The search algorithm will explore the search tree and determine the best move based on the evaluation function.
    
6.  Repeat steps 4 and 5: As the game progresses, you will need to update the search tree and reapply the search algorithm to find the best move at each turn.
    

Overall, developing a strategy for adversarial search problems involves carefully defining the game, choosing an appropriate search algorithm, and implementing it effectively to find the best move for a given player.


#### Example

Tic-Tac-Toe as an example of an adversarial search problem.

```
 X | O | X 
-----------
 O | X | O 
-----------
 X | O | X 
```

1.  Define the game: The game of Tic-Tac-Toe is played on a 3x3 grid. The game starts with an empty grid, and players take turns placing their symbol (either X or O) on an empty cell. The game ends when one player has three symbols in a row (horizontally, vertically, or diagonally) or when the grid is full and there is no winner.
    
2.  Define the evaluation function: An evaluation function for Tic-Tac-Toe could assign a positive score to a state where the player has three symbols in a row, a negative score to a state where the opponent has three symbols in a row, and a score of zero to a state where there is no winner yet.
    
3.  Choose a search algorithm: In the case of Tic-Tac-Toe, the search space is relatively small, so a simple minimax algorithm could be used to search the game tree.
    
4.  Implement the search algorithm: The search tree for Tic-Tac-Toe is relatively small, as there are only 9 possible moves at each turn. The search tree can be represented as a recursive tree structure, where each node represents a game state and each edge represents a legal move.
    
5.  Apply the search algorithm: At each turn, the minimax algorithm would explore the search tree to determine the best move for the current player. The algorithm would choose the move that maximizes the evaluation function for the current player, assuming that the opponent will choose the move that minimizes the evaluation function for the current player.
    
6.  Repeat steps 4 and 5: As the game progresses, the search tree will be updated to reflect the current game state, and the minimax algorithm will be reapplied to find the best move at each turn.
    

Overall, the strategy for playing Tic-Tac-Toe involves using a search algorithm, such as minimax, to explore the game tree and determine the best move at each turn based on the evaluation function. By doing so, the player can maximize their chances of winning the game.

#### Analyse:

-   **States**: The states of the game are the possible configurations of X's and O's on the 3x3 board. Each state can be represented as a 3x3 matrix, with empty cells represented by a special symbol (e.g., "E").
-   **Moves**: A move in Tic-Tac-Toe consists of placing an X or an O in an empty cell of the board.
-   **Result**: The result of a move is a new state of the game, where the board has been updated with the new X or O in the corresponding cell.
-   **Terminal test**: The game ends in a terminal state when one player has three symbols in a row (horizontally, vertically, or diagonally) or when the board is full and there is no winner. A terminal state is a state where the game is over and no further moves can be made.
-   **Utility function**: The utility function assigns a score to a terminal state based on whether it represents a win for X, a win for O, or a tie. One common utility function for Tic-Tac-Toe is to assign a score of 1 to a win for X, -1 to a win for O, and 0 to a tie.

---

### Minimax algorithm works for Tic-Tac-Toe:

1.  We start at the root node, which represents the current state of the game. If the game is over (i.e., a terminal state), we return the utility value of that state.
    
2.  If it's the turn of the MAX player (i.e., X), we evaluate the utility value of each child node (i.e., each possible move), by recursively applying the Minimax algorithm to each child node. We select the child node with the highest utility value, since this represents the best move for X.
    
3.  If it's the turn of the MIN player (i.e., O), we evaluate the utility value of each child node by recursively applying the Minimax algorithm to each child node. We select the child node with the lowest utility value, since this represents the best move for O.
    
4.  Once we have evaluated the utility values of all child nodes, we return the selected value (i.e., the maximum or minimum utility value, depending on whose turn it is).

---

[[Alpha-Beta]]

```
       A
     / | \
    B  C  D
   / \   / \
  E   F G   H
```

Assume that <mark style="background: #FF5582A6;">we are the maximising player</mark> and our <mark style="background: #FF5582A6;">opponent is the minimising player</mark>. Our goal is to find the best move in this game tree.

Without Alpha-Beta Pruning, we would explore every path in the game tree to find the optimal move. However, with Alpha-Beta Pruning, we can reduce the number of nodes that need to be explored.

At the start of the algorithm, we set <mark style="background: #FF5582A6;">alpha to negative infinity</mark> and <mark style="background: #FF5582A6;">beta to positive infinity</mark>. We start exploring the game tree from the root node A.

-   We first explore node B. We set alpha to the maximum of alpha and the value of B. Since B has not been explored yet, we set alpha to the value of B. We continue exploring the children of B.  
    
-   We explore node E. We set alpha to the maximum of alpha and the value of E. Since E has not been explored yet, we set alpha to the value of E. 
    
-   We backtrack to node B and explore node F. We set alpha to the maximum of alpha and the value of F. Since F has not been explored yet, we set alpha to the value of F.
    
-   We backtrack to node B and update its value to the maximum of its children, which is F. We continue exploring the children of A.
    
-   We explore node C. We set beta to the minimum of beta and the value of C. Since C has not been explored yet, we set beta to the value of C. We continue exploring the children of C.
    
-   We explore node G. We set beta to the minimum of beta and the value of G. Since G has not been explored yet, we set beta to the value of G.
    
-   We backtrack to node C and explore node D. We set beta to the minimum of beta and the value of D. Since D has not been explored yet, we set beta to the value of D.
    
-   We backtrack to node C and update its value to the minimum of its children, which is G. We update the value of node A to the maximum of its children, which is F.
    
-   Since alpha is greater than or equal to beta, we know that we can prune the entire subtree rooted at node D, since it cannot possibly lead to a better outcome than what we have already explored.
    
-   We update the value of node A to the maximum of its children, which is F.
    
-   We have explored all the nodes in the game tree and have determined that the best move is to choose the path that leads to node F.
    

By using Alpha-Beta Pruning, we were able to prune the subtree rooted at node D, which greatly reduced the number of nodes that needed to be explored.

---

### Stochastic game

a generalization of normal-form games, which involve multiple players who make decisions simultaneously. In a stochastic game, the environment is also involved in decision-making and <mark style="background: #FF5582A6;">introduces uncertainty</mark> into the outcome of actions. Stochastic games are typically defined by a set of states, a set of actions available to each player, a set of transition probabilities, and a reward function that depends on the state and the actions of all players.

### ExpectiMinimax

ExpectiMinimax is an <mark style="background: #BBFABBA6;">extension of the minimax algorithm</mark> for games where chance is involved. It is used to find the best move for an agent in games that <mark style="background: #FFB8EBA6;">involve random outcomes</mark>, such as card games or board games with dice.

The algorithm assumes that the agent is playing against an opponent who is also trying to maximize their own score. The goal of the agent is to choose a move that maximizes their expected score, taking into account the possible random outcomes of the game.

The ExpectiMinimax algorithm works by constructing a game tree that represents all possible outcomes of the game. At each node of the tree, the agent chooses a move that maximizes their expected score, assuming that the opponent will also choose the move that maximizes their own expected score. <mark style="background: #FFF3A3A6;">The expected score is calculated by taking the weighted average of the scores for all possible outcomes of the move</mark>, where the weights are the probabilities of each outcome.

The algorithm then recursively applies this process to each child node of the current node, alternating between maximizing the expected score for the agent and minimizing it for the opponent. This process continues until the algorithm reaches a terminal node, which represents the end of the game.

The ExpectiMinimax algorithm is a generalization of the minimax algorithm, which assumes that the game is deterministic and that the opponent is always trying to minimize the agent's score. By taking into account the random outcomes of the game, the ExpectiMinimax algorithm provides a more accurate evaluation of the best move for the agent.

- Chance is introduced by dice, card-shuffling, coin flipping. 
- “Chance” can be seen as <mark style="background: #FFB86CA6;">special kind of player</mark> whose move is the outcomes of a random event, which determines the space of legal moves down the tree. 
- Chance is not adversarial: the value of chance positions is the expectation (average) over all possible outcomes of the value of the result.

- Behavior is preserved only by positive linear transformation 
-  Hence <mark style="background: #FF5582A6;">Eval should be proportional</mark> to the expected payoff determined by Utility


**Complexity**

Time:  O(b^m * n^m) 
-  n is the maximum number of outcomes of a chance event