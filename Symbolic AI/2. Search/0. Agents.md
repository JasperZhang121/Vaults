
### PEAS:

To design a rational agent, we must specify the task environment

Consider, e.g., the task of designing a driverless taxi:

• Performance measure: – safety, destination, profits, legality, comfort, … 
• Environment: – streets/freeways, traffic, pedestrians, weather, … 
• Actuators: – steering, accelerator, brake, horn, blinkers, … 
• Sensors: – GPS, video, accelerometers, gauges, engine sensors, …

---

### Properties of Task Environments 

• Fully vs partially observable – do the agent sensors give access to all relevant information about the environment state? 
• Deterministic vs stochastic – is the next state completely determined by the current state and executed action? 
• Known vs unknown – does the agent know the environment’s laws of physics?
• Episodic vs sequential – is the next decision independent of the previous ones? • Static vs dynamic – can the environment change whilst the agent is deliberating? – Semi-dynamic:only the performance score changes. 
• Discrete vs continuous – can time, states, actions, percepts be represented in a discrete way? 
• Single vs multi-agent – is a single agent making decisions, or do multiple agents need to compete or cooperate to maximise interdependent performance measures? 5 Properties of Task Environments

----
### Agent types

- simple reflex agents 
`These agents select actions based solely on the current percept, without considering past percepts or future consequences. They use a set of condition-action rules, known as a rule-based system, to choose the appropriate action. For example, a simple reflex agent might turn on the air conditioning when the temperature exceeds a certain threshold.`

- reflex agents with state
`These agents maintain an internal state, which allows them to take into account past percepts when selecting actions. The internal state is updated based on the current percept and the chosen action. For example, a reflex agent with state might adjust the thermostat based on both the current temperature and whether the air conditioning has been turned on or off in the recent past.`

- goal-based agents 
`These agents select actions based on a desired goal state. They use a problem-solving approach to determine the sequence of actions that will lead to the desired goal state. Goal-based agents can handle complex situations and plan ahead. For example, a goal-based agent might plan a route to a destination by considering the current location, the desired destination, and the available transportation options.`

- utility-based agents
`These agents select actions based on a utility function, which assigns a numerical value to each possible state of the environment. The agent's goal is to maximize the expected utility of its actions. Utility-based agents take into account not only the immediate consequences of their actions but also their long-term effects. For example, a utility-based agent might choose to take a more expensive flight that arrives at a more convenient time, based on the utility it assigns to different combinations of price and convenience.`
