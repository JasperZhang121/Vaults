
Lazy learning, also known as <mark style="background: #ADCCFFA6;">instance-based learning</mark>, is a type of machine learning where the model is not explicitly trained but rather stores the training data and waits until a new input is given to it. The prediction is made by finding the most similar training examples to the new input and using their known output labels to make a prediction.

The k-Nearest Neighbor (k-NN) algorithm is a popular example of a lazy learner. In the k-NN algorithm, the output label of a new input is predicted based on the majority label of its k-nearest neighbors in the training data. Check: [[3. K-nearest Neighbors]]

Formally, given a training set of N tuples $D = {(x_1,y_1), (x_2,y_2), ..., (x_N,y_N)}$, where $x_i$ is an input tuple and $y_i$ is the corresponding output label, and a new input tuple x, the k-NN algorithm finds the k closest training tuples to x based on a distance metric $d(x, x_i)$. The output label of the new input x is then predicted as the majority label of the k closest tuples.

The distance metric can be defined in different ways, such as Euclidean distance, Manhattan distance, and cosine similarity. The value of k is a hyperparameter that needs to be tuned to achieve the best performance on the validation set.

Compared to other types of machine learning algorithms such as decision trees and neural networks, lazy learners have several advantages and disadvantages:

Advantages:

-   Simple to implement and understand
-   Can handle complex decision boundaries
-   Can adapt to changes in the data without requiring retraining

Disadvantages:

-   Require a lot of memory to store the training data
-   Slow to make predictions, especially for large datasets
-   Sensitive to the choice of distance metric and value of k

Despite its drawbacks, the k-NN algorithm is still widely used in many applications such as recommendation systems, image recognition, and anomaly detection.

