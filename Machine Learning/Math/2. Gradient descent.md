
In mathematics,Â **gradient descent**Â (also often calledÂ **steepest descent**) is aÂ [first-order](https://en.wikipedia.org/wiki/Category:First_order_methods "Category:First order methods")Â [iterative](https://en.wikipedia.org/wiki/Iterative_algorithm "Iterative algorithm")Â [optimization](https://en.wikipedia.org/wiki/Mathematical_optimization "Mathematical optimization")Â [algorithm](https://en.wikipedia.org/wiki/Algorithm "Algorithm")Â for finding aÂ [local minimum](https://en.wikipedia.org/wiki/Local_minimum "Local minimum")Â of aÂ [differentiable function](https://en.wikipedia.org/wiki/Differentiable_function "Differentiable function"). The idea is to take repeated steps in the opposite direction of theÂ [gradient](https://en.wikipedia.org/wiki/Gradient "Gradient")Â (or approximate gradient) of the function at the current point, because this is the direction of steepest descent. Conversely, stepping in the direction of the gradient will lead to aÂ [local maximum](https://en.wikipedia.org/wiki/Local_maximum "Local maximum")Â of that function; the procedure is then known as gradient ascent.

Gradient descent is generally attributed toÂ [Augustin-Louis Cauchy](https://en.wikipedia.org/wiki/Augustin-Louis_Cauchy "Augustin-Louis Cauchy"), who first suggested it in 1847.[[1]](https://en.wikipedia.org/wiki/Gradient_descent#cite_note-1)Â [Jacques Hadamard](https://en.wikipedia.org/wiki/Jacques_Hadamard "Jacques Hadamard")Â independently proposed a similar method in 1907.[[2]](https://en.wikipedia.org/wiki/Gradient_descent#cite_note-2)[[3]](https://en.wikipedia.org/wiki/Gradient_descent#cite_note-3)Â Its convergence properties for non-linear optimization problems were first studied byÂ [Haskell Curry](https://en.wikipedia.org/wiki/Haskell_Curry "Haskell Curry")Â in 1944,[[4]](https://en.wikipedia.org/wiki/Gradient_descent#cite_note-4)Â with the method becoming increasingly well-studied and used in the following decades.[[5]](https://en.wikipedia.org/wiki/Gradient_descent#cite_note-BP-5)[[6]](https://en.wikipedia.org/wiki/Gradient_descent#cite_note-AK82-6)

---

Gradient descent is an algorithm used to find the minimum value of a function. We will use the gradient descent algorithm to find the minimum value of the cost function ğ½(ğœƒ0, ğœƒ1).
The idea behind gradient descent is: at the beginning we randomly select a combination of parameters (ğœƒ0, ğœƒ1, . . We continue to do this until we reach a local minimum, however, because we haven't tried all the parameter combinations, so we still don't know whether the local minimum we get is the global minimum. Choosing different initial parameter combinations may find different local minimum.

![[Gradient descent.png]]

----

Batch gradient descent:

![[gradiant decs.png]]

ğ‘ is the learning rate, which determines how big a step we take down in the direction that can make the cost function decrease the most. In batch gradient descent, we subtract all parameters at the same time by multiplying the learning rate by Derivative of the cost function.

If ğ‘ is too small, that is, my learning rate is too small, the result is that I can only move a little bit, trying to get close to the lowest point, so it takes many steps to reach the lowest point, so if ğ‘ is too small , can be slow because it will move a little bit, and it will take many steps to reach the global minimum.

If ğ‘ is too large, then the gradient descent method may cross the lowest point, and may not even be able to converge, and the next iteration will move a big step, crossing once, crossing again, crossing the lowest point again and again, until you find that it is actually far from the lowest point The points are getting farther and farther away, so if ğ‘ is too large, it will cause no convergence, or even divergence.

----

![[gradient formula.png]]

Convex better than none-convex when doing the gradient descent for finding the global min:

![[convex.png]]

