----

Common techniques:

1.  Regularization: Regularization adds a penalty term to the loss function to discourage the model from fitting the noise in the data. Common regularization techniques include L1 and L2 regularization, also known as Lasso and Ridge regression, respectively.
    
2.  Early stopping: Early stopping involves monitoring the performance of the model on a validation set and stopping the training process when the performance on the validation set starts to decrease. This helps to prevent the model from learning the noise in the data.
    
3.  Cross-validation: Cross-validation provides an estimate of the generalization performance of the model and can be used to select the best model based on its ability to generalize to unseen data.
    
4.  Feature selection: Feature selection involves removing irrelevant or redundant features from the data, which can help to reduce overfitting.
    
5.  Ensemble methods: Ensemble methods, such as random forests and gradient boosting, can be used to reduce overfitting by combining the predictions of multiple models.

