In [machine learning](https://en.wikipedia.org/wiki/Machine_learning "Machine learning"), **support vector machines** (**SVMs**, also **support vector networks**) are [supervised learning](https://en.wikipedia.org/wiki/Supervised_learning "Supervised learning") models with associated learning [algorithms](https://en.wikipedia.org/wiki/Algorithm "Algorithm") that analyze data for [classification](https://en.wikipedia.org/wiki/Statistical_classification "Statistical classification") and [regression analysis](https://en.wikipedia.org/wiki/Regression_analysis "Regression analysis").

---

### Hyperplanes and Margin

Any hyperplane can be written as the set of points x satisfying: W<sup>T</sup>-b = 0

If the training data is linearly separable, we can select two parallel hyperplanes that separate the two classes of data, so that the distance between them is as large as possible. The region bounded by these two hyperplanes is called the "margin", and the maximum-margin hyperplane is the hyperplane that lies halfway between them. With a normalized or standardized dataset, these hyperplanes can be described by the equations:

-   W<sup>T</sup>-b = 1 (anything on or above this boundary is of one class, with label 1)
-   W<sup>T</sup>-b = -1 (anything on or below this boundary is of the other class, with label −1)

Margin = 2/ ||W||

Hard-margin vs Soft-margin

In SVM, the margin can be maximized in two different ways:

-   Hard-margin SVM: Used when the data is linearly separable, this method tries to maximize the margin <mark style="background: #FF5582A6;">without allowing any points to be misclassified</mark>. However, this can result in overfitting and is sensitive to outliers.
-   Soft-margin SVM: Used when the data is not linearly separable, this method <mark style="background: #FFB86CA6;">allows for some points to be misclassified</mark> in order to obtain a wider margin. A <mark style="background: #ADCCFFA6;">regularization parameter C is introduced</mark>, which controls the trade-off between maximizing the margin and allowing misclassifications.

#### Code: 

```python
#Import Library
from sklearn import svm
# create
model = svm.svc()
# train the model using the training sets and check score
model.fit(x,y)
model.score(x,y)
```

---

SVMs are one of the most successful classification methods for both linear and nonlinear data. It uses a nonlinear mapping to transform the original training data into a higher dimension. With the new dimension, it searches for the linear optimal separating hyperplane (i.e., “decision boundary”) and with an appropriate nonlinear mapping to a sufficiently high dimension, data from two classes can always be separated by a hyperplane. SVM finds this hyperplane using support vectors (“essential” training tuples) and margins (defined by the support vectors).

History and Applications

Vapnik and colleagues introduced the Support Vector Machine in 1992. The groundwork for SVM was laid by Vapnik & Chervonenkis’ statistical learning theory in the 1960s. SVM's features include slow training but high accuracy owing to their ability to model complex nonlinear decision boundaries (margin maximisation). SVMs can be used for classification and numeric prediction (regression). Some applications of SVM include handwritten digit recognition, object recognition, speaker identification, and benchmarking time-series prediction tests.

Algorithms

Training of SVM can be distinguished by:

-   Linearly separable case
-   Non-linearly separable case

---

### Linearly Separable Case

When the training data is linearly separable, SVM finds a hyperplane that separates the data into two classes. The hyperplane is chosen such that the margin between the two closest points from each class is maximized. This is called the maximum-margin hyperplane.

The margin is the distance between the two parallel hyperplanes that separate the data, and the support vectors are the data points that lie closest to these hyperplanes.

The optimization problem for finding the maximum-margin hyperplane can be formulated as a quadratic programming problem, which can be solved using an algorithm such as Sequential Minimal Optimization (SMO).

#### Maximum Marginal Hyperplane:

SVM algorithm searches for the hyperplane with the largest margin, known as Maximum Marginal Hyperplane (MMH).

Margin:

The margin of a hyperplane is defined as the <mark style="background: #ABF7F7A6;">perpendicular distance between the hyperplane and the closest point of a training tuple</mark>. The distance between the two parallel hyperplanes that separate the two classes of data is called the margin. The maximum margin hyperplane is the hyperplane that lies halfway between them.

Support Vectors:

Support vectors are the training tuples that determine the maximum margin hyperplane. These are the closest points to the hyperplane from the two different classes of data. The red-circled tuples in the below example image are the support vectors of the hyperplane.

Formal definition of hyperplanes and support vectors:

Two dimensional training tuple case:

In two dimensional space ($A_1-A_2$ plane), a hyperplane corresponds to a line, and every hyperplane can be written as:
$A_2 = a\times A_1 + b$
For a more general representation, if we replace A_1 and A_2 by  x_1 and x_2, then the above hyperplane can be rewritten as:
$0 = w_1\times x_1 + w_2 \times x_2 + w_0$,
where $w_1 = a, w_2 = -1, w_0 = b$.
We can represent any hyperplane(line) in two dimensional space with $w_1, w_2$, and $w_0$.
In the linearly separable case, every training tuple satisfies the following condition:
H1 (positive class)
If $w_1 \times x_1 + w_2 \times x_2 + w_0 \geq +1$
H2 (negative class):
If $w_1 \times x_1 + w_2 \times x_2 + w_0 \leq -1$
Support vector: Therefore, every training tuple that satisfies $w_1 \times x_1 + w_2 \times x_2 + w_0 = \pm 1$ is a support vector.
