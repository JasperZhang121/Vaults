In [machine learning](https://en.wikipedia.org/wiki/Machine_learning "Machine learning"), **support vector machines** (**SVMs**, also **support vector networks**) are [supervised learning](https://en.wikipedia.org/wiki/Supervised_learning "Supervised learning") models with associated learning [algorithms](https://en.wikipedia.org/wiki/Algorithm "Algorithm") that analyze data for [classification](https://en.wikipedia.org/wiki/Statistical_classification "Statistical classification") and [regression analysis](https://en.wikipedia.org/wiki/Regression_analysis "Regression analysis").

---

### Hyperplanes and Margin

Any hyperplane can be written as the set of points x satisfying: W<sup>T</sup>-b = 0

If the training data is linearly separable, we can select two parallel hyperplanes that separate the two classes of data, so that the distance between them is as large as possible. The region bounded by these two hyperplanes is called the "margin", and the maximum-margin hyperplane is the hyperplane that lies halfway between them. With a normalized or standardized dataset, these hyperplanes can be described by the equations:

-   W<sup>T</sup>-b = 1 (anything on or above this boundary is of one class, with label 1)
-   W<sup>T</sup>-b = -1 (anything on or below this boundary is of the other class, with label −1)

Margin = 2/ ||W||

Hard-margin vs Soft-margin

In SVM, the margin can be maximized in two different ways:

-   Hard-margin SVM: Used when the data is linearly separable, this method tries to maximize the margin without allowing any points to be misclassified. However, this can result in overfitting and is sensitive to outliers.
-   Soft-margin SVM: Used when the data is not linearly separable, this method allows for some points to be misclassified in order to obtain a wider margin. A regularization parameter C is introduced, which controls the trade-off between maximizing the margin and allowing misclassifications.

#### Code: 

```python
#Import Library
from sklearn import svm
# create
model = svm.svc()
# train the model using the training sets and check score
model.fit(x,y)
model.score(x,y)
```

---

SVMs are one of the most successful classification methods for both linear and nonlinear data. It uses a nonlinear mapping to transform the original training data into a higher dimension. With the new dimension, it searches for the linear optimal separating hyperplane (i.e., “decision boundary”) and with an appropriate nonlinear mapping to a sufficiently high dimension, data from two classes can always be separated by a hyperplane. SVM finds this hyperplane using support vectors (“essential” training tuples) and margins (defined by the support vectors).

History and Applications

Vapnik and colleagues introduced the Support Vector Machine in 1992. The groundwork for SVM was laid by Vapnik & Chervonenkis’ statistical learning theory in the 1960s. SVM's features include slow training but high accuracy owing to their ability to model complex nonlinear decision boundaries (margin maximisation). SVMs can be used for classification and numeric prediction (regression). Some applications of SVM include handwritten digit recognition, object recognition, speaker identification, and benchmarking time-series prediction tests.

Algorithms

Training of SVM can be distinguished by:

-   Linearly separable case
-   Non-linearly separable case

---

### Linearly Separable Case




