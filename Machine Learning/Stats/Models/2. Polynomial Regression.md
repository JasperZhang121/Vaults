
Polynomial Regression is a regression model that extends the linear regression model by adding polynomial features to the independent variables. It is used when the relationship between the independent and dependent variables is non-linear.

The polynomial regression equation can be represented as:

$$ y = \beta_0 + \beta_1 * x + \beta_2 * x^2 + ... + \beta_d * x^d + \epsilon $$

where:

-   `y` is the dependent variable
-   `x` is the independent variable
-   `β` are the coefficients of the polynomial
-   `ε` is the error term
-   `x^k` is the `k`th power of `x`

The goal of polynomial regression is to find the optimal coefficients `β` that minimize the sum of squared errors between the predicted values and the actual values of `y`. This can be achieved using various optimization algorithms, such as gradient descent or least squares.

----
```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures

# generate some data
np.random.seed(0)
X = np.random.rand(100, 1)
y = X ** 2 + np.random.rand(100, 1)

# add polynomial features
poly_features = PolynomialFeatures(degree=2)
X_poly = poly_features.fit_transform(X)

# fit a linear regression model
regressor = LinearRegression()
regressor.fit(X_poly, y)

# make predictions on a grid
X_grid = np.linspace(0, 1, 100).reshape(-1, 1)
X_grid_poly = poly_features.fit_transform(X_grid)
y_pred = regressor.predict(X_grid_poly)

# plot the results
plt.scatter(X, y, color="red")
plt.plot(X_grid, y_pred, color="blue")
plt.show()
```



