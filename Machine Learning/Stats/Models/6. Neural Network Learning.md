
Neural networks are a type of machine learning model that are based on the <mark style="background: #D2B3FFA6;">structure and function of the human brain</mark>. They consist of a set of connected nodes, where each node is an input, output, or hidden layer, and each connection between nodes has a weight associated with it. During training, the weights are adjusted to allow the network to correctly output the desired output for a given input. The classic training algorithm for neural networks is called backpropagation.

### Convolutional neural networks (CNNs) 

are a type of neural network that are especially well-suited for image recognition and other problems where the input has a spatial structure. They use specific architectures and connection patterns to learn features at different scales and locations in the input.

- Convolution operation:

In a convolutional layer, the convolution operation is used to <mark style="background: #FFB8EBA6;">apply a set of filters to the input image</mark>. This operation can be represented mathematically as follows:

$$\begin{equation} S(i,j) = (I * K)(i,j) = \sum_{m}\sum_{n}I(m,n)K(i-m,j-n) \end{equation}$$

where $S(i,j)$ is the output of the convolution operation at position $(i,j)$, I is the input image, K is the filter/kernel, and * denotes the convolution operation.

-   Pooling operation:

The pooling operation is used to downsample the feature maps obtained from the convolutional layer. The most common pooling operation is max pooling, which takes the maximum value in each pooling region. Mathematically, max pooling can be defined as follows:

$$\begin{equation} M(i,j) = \max_{(m,n)\in R_{i,j}}S(m,n) \end{equation}$$

where M(i,j) is the output of the pooling operation at position (i,j), R_{i,j} is the pooling region centered at (i,j), and S is the input feature map.

-   Rectified Linear Unit (ReLU) activation:

ReLU is a popular activation function used in CNNs. It is defined as follows:

$$\begin{equation} f(x) = \max(0,x) \end{equation}$$

where x is the input to the activation function.

-   Softmax activation:

Softmax is commonly used as the activation function for the output layer in classification problems. It maps the output of the last hidden layer to a probability distribution over the classes. Mathematically, softmax can be defined as follows:

$$\begin{equation} P(y=j|x) = \frac{\exp(z_j)}{\sum_{k=1}^{K}\exp(z_k)} \end{equation}$$

where P(y=j|x) is the probability of the j-th class given the input x, z_j is the j-th element of the output vector z of the last hidden layer, and K is the number of classes.


### Strengths & Weakness

**Strengths** of neural networks as a classifier include their <mark style="background: #FFB86CA6;">high tolerance to noisy data</mark>, ability to <mark style="background: #ABF7F7A6;">classify untrained patterns, and suitability for continuous-valued inputs and outputs</mark>. They have been successful on a wide range of real-world data, such as hand-written letters, images, and voice. Neural network algorithms are inherently parallel and can take advantage of modern hardware to speed up training and inference. Techniques have also been developed to extract rules from trained neural networks.

**Weaknesses** of neural networks include their <mark style="background: #FF5582A6;">long training time</mark> and the need to determine <mark style="background: #D2B3FFA6;">a number of parameters empirically</mark>, such as network topology or structure. They also tend to overfit when there are a large number of nodes, which implies a large number of weights to be learned. Neural networks are generally <mark style="background: #FF5582A6;">difficult to interpret</mark>, as the learned weights and hidden units have no clear symbolic meaning, and they are often referred to as "black box" models.


