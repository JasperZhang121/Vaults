![[logistic cost func.png]]


----
The cost function for logistic regression is called the log-loss or cross-entropy loss. It is defined as follows:

$$J(\theta) = -\frac{1}{m} \sum_{i=1}^{m} [y^{(i)}log(h_{\theta}(x^{(i)})) + (1-y^{(i)})log(1-h_{\theta}(x^{(i)}))]$$

where $h_{\theta}(x^{(i)})$ is the hypothesis function, which represents the predicted probability that $y = 1$ given the input features $x$ for the $i^{th}$ training example. The hypothesis function is defined as:

$$h_{\theta}(x^{(i)}) = g(\theta^T x^{(i)})$$

where $g(z)$ is the sigmoid function:

$$g(z) = \frac{1}{1 + e^{-z}}$$

The goal is to find the values of $\theta$ that minimize the cost function $J(\theta)$. This is typically done using optimization algorithms such as gradient descent. The gradient of the cost function with respect to each $\theta_j$ is given by:

$$\frac{\partial J(\theta)}{\partial \theta_j} = \frac{1}{m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)}) - y^{(i)}) x_j^{(i)}$$
