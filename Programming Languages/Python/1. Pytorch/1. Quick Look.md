### Tensors

Tensors are multi-dimensional arrays and are the primary building blocks in PyTorch. They are similar to NumPy arrays, but they can be used on GPUs to accelerate computing.

```python
import torch

# Creating a tensor
x = torch.tensor([1, 2, 3, 4, 5])
print(x)

# Creating a random tensor of shape (3, 3)
y = torch.randn(3, 3)
print(y)

# Create a new tensor based on x's size, filled with ones
y = x.new_ones(5, 3)  # specifying size is optional if it's the same as x
print(y)

# Create a tensor with the same size as x, with elements sampled from a normal distribution
z = torch.randn_like(x)
print(z)

# Get the size of tensor x
print(x.size())
```


### **Operations**

You can perform many operations on tensors similar to those on NumPy arrays.

```python
a = torch.tensor([1, 2])
b = torch.tensor([3, 4])

# Addition
c = a + b
print(c)  # tensor([4, 6])

# Multiplication
d = a * b
print(d)  # tensor([3, 8])
```


```python
# Direct addition
result = x + y

# Torch add
result = torch.add(x, y)

# In-place addition
y.add_(x)

# Providing an output tensor
result = torch.empty_like(x)
torch.add(x, y, out=result)

# Subtraction
sub_result = x - y

# Multiplication
mul_result = x * y

# Division
div_result = x / y

```


```python
# Selecting the first column of all rows
first_col = x[:, 0]
print(first_col)

# Reshape x to 2 rows and 3 columns
x_reshaped = x.view(2, 3)
print(x_reshaped)

# Assuming x is a 1-element tensor
single_val_tensor = torch.tensor([5])
number = single_val_tensor.item()
print(number)
```


### Convert

```python
# Tensor to NumPy Array
a = torch.ones(5)
b = a.numpy()
print(b)

# NumPy Array to Tensor
import numpy as np
a = np.ones(5)
b = torch.from_numpy(a)
print(b)

# Assuming a GPU is available
if torch.cuda.is_available():
    x = x.to('cuda')
    print(x)
```


### Autograd

```python
# Create tensors
x = torch.randn(3, requires_grad=True)

# Perform some operations
y = x * 2
z = y.mean()
z.backward()  # Computes the gradient

print(x.grad)  # Access gradients

# Detaching
x_detached = x.detach()

# Using with torch.no_grad() for operations that shouldn't track gradients
with torch.no_grad():
    x = x * 2
```

### **Automatic Differentiation**

PyTorch supports automatic differentiation, which is very useful when training models.

```python
x = torch.tensor(2.0, requires_grad=True)
y = x**2
y.backward()  # Compute gradients
print(x.grad)  # dy/dx = 2*x = 4.0
```


### **Neural Networks**

PyTorch's `nn` module provides layers and loss functions used for building and training neural networks.

```python
import torch.nn as nn
import torch.nn.functional as F

class SimpleNet(nn.Module):
    def __init__(self):
        super(SimpleNet, self).__init__()
        self.fc = nn.Linear(1, 1)  # Single-layer perceptron

    def forward(self, x):
        return self.fc(x)
```

### **Training a Model**

Once you have a model, you can train it using data. The general training loop in PyTorch goes like this:

```python
model = SimpleNet()
criterion = nn.MSELoss()  # Mean Squared Error for regression tasks
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)  # Stochastic Gradient Descent

# Dummy data and target
inputs = torch.tensor([[1.0], [2.0], [3.0]])
targets = torch.tensor([[2.0], [4.0], [6.0]])

# Training loop
for epoch in range(1000):
    outputs = model(inputs)
    loss = criterion(outputs, targets)
    optimizer.zero_grad()  # Clear previous gradients
    loss.backward()        # Compute gradients
    optimizer.step()       # Update model parameters
```


### **GPU Support**

PyTorch makes it easy to move computations to the GPU.

```python
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
model.to(device)
inputs, targets = inputs.to(device), targets.to(device)
```


