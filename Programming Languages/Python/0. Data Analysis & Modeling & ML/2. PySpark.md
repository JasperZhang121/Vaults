### Importing PySpark
```python
from pyspark.sql import SparkSession
from pyspark.sql import Row
```

### Creating a SparkSession
```python
# Initialize a SparkSession
spark = SparkSession.builder \
    .appName("PySpark Example") \
    .getOrCreate()
```

### Creating DataFrames
#### From a List of Rows
```python
# Create DataFrame from a list of Rows
data = [Row(name="Alice", age=34), Row(name="Bob", age=45)]
df = spark.createDataFrame(data)
```

#### From a Dictionary
```python
# Create DataFrame from a dictionary
data = [{"name": "Alice", "age": 34}, {"name": "Bob", "age": 45}]
df = spark.createDataFrame(data)
```

#### From a CSV File
```python
# Read DataFrame from a CSV file
df = spark.read.csv("path/to/file.csv", header=True, inferSchema=True)
```

#### From a JSON File
```python
# Read DataFrame from a JSON file
df = spark.read.json("path/to/file.json")
```

### Viewing Data
```python
# Show the first few rows
df.show()

# Print the schema
df.printSchema()

# Get the column names
df.columns

# Get the data types of each column
df.dtypes
```

### Basic DataFrame Operations
#### Selecting Columns
```python
# Select specific columns
df.select("name", "age").show()
```

#### Filtering Rows
```python
# Filter rows based on condition
df.filter(df['age'] > 40).show()
```

#### Adding Columns
```python
# Add a new column
df.withColumn("new_age", df["age"] + 10).show()
```

#### Renaming Columns
```python
# Rename a column
df.withColumnRenamed("age", "years").show()
```

#### Dropping Columns
```python
# Drop a column
df.drop("age").show()
```

### Grouping and Aggregation
```python
# Group by column and count
df.groupBy("age").count().show()

# Aggregate functions
from pyspark.sql.functions import avg, max, min

df.groupBy("name").agg(
    avg("age").alias("avg_age"),
    max("age").alias("max_age"),
    min("age").alias("min_age")
).show()
```

### Joins
```python
# Create another DataFrame for join example
data2 = [Row(name="Alice", city="New York"), Row(name="Bob", city="San Francisco")]
df2 = spark.createDataFrame(data2)

# Inner join
df.join(df2, on="name", how="inner").show()

# Left join
df.join(df2, on="name", how="left").show()

# Right join
df.join(df2, on="name", how="right").show()
```

### Handling Missing Data
```python
# Drop rows with any null values
df.na.drop().show()

# Fill null values
df.na.fill({"age": 0, "name": "unknown"}).show()
```

### User-Defined Functions (UDFs)
```python
from pyspark.sql.functions import udf
from pyspark.sql.types import IntegerType

# Define a Python function
def add_one(x):
    return x + 1

# Convert the Python function to a UDF
add_one_udf = udf(add_one, IntegerType())

# Use the UDF
df.withColumn("new_age", add_one_udf(df["age"])).show()
```

### Saving DataFrames
#### Save as CSV
```python
# Save DataFrame as a CSV file
df.write.csv("path/to/save/csv", header=True)
```

#### Save as JSON
```python
# Save DataFrame as a JSON file
df.write.json("path/to/save/json")
```

### Working with RDDs
#### Creating an RDD
```python
# Create an RDD from a list
rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5])
```

#### Basic RDD Operations
```python
# Map transformation
rdd2 = rdd.map(lambda x: x * 2)

# Filter transformation
rdd3 = rdd2.filter(lambda x: x > 5)

# Collect action
rdd3.collect()
```

#### Converting RDD to DataFrame
```python
# Convert RDD to DataFrame
row_rdd = rdd.map(lambda x: Row(number=x))
df_from_rdd = spark.createDataFrame(row_rdd)
df_from_rdd.show()
```

### Spark SQL
#### Creating a Temporary View
```python
# Create a temporary view
df.createOrReplaceTempView("people")

# Run SQL queries
spark.sql("SELECT * FROM people WHERE age > 30").show()
```

### Configuring Spark
```python
# Set Spark configuration
spark.conf.set("spark.sql.shuffle.partitions", "50")
```

### Closing the SparkSession
```python
# Stop the SparkSession
spark.stop()
```
