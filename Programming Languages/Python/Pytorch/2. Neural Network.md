### Understanding `nn.Module`

In PyTorch, the `nn.Module` class is the base class for all neural network modules. Your models should subclass this class. It offers a lot of functionalities including:

- Keeping track of all the weights.
- Simplifying GPU usage (`model.to('cuda')`).
- Implementing utility functions such as `model.train()` and `model.eval()` for toggling dropout/batch norm behavior.

### Layers in `nn`

PyTorch's `nn` module provides pre-defined layers which you can use to build your neural network architectures. Here's a brief rundown of the more common layers:

1. **Linear Layers**:
    - `nn.Linear`: Fully connected layer.
2. **Convolution Layers**:
    - `nn.Conv1d`: Applies a 1D convolution.
    - `nn.Conv2d`: Applies a 2D convolution. Widely used in image processing.
    - `nn.Conv3d`: Applies a 3D convolution.
3. **Pooling Layers**:
    - `nn.MaxPool1d`, `nn.MaxPool2d`, `nn.MaxPool3d`: Max pooling layers.
    - `nn.AvgPool1d`, `nn.AvgPool2d`, `nn.AvgPool3d`: Average pooling layers.
4. **Normalization Layers**:
    - `nn.BatchNorm1d`, `nn.BatchNorm2d`: Batch normalization layers.
    - `nn.LayerNorm`: Layer normalization.
5. **Recurrent Layers**:
    - `nn.RNN`: Basic RNN layer.
    - `nn.LSTM`: Long Short-Term Memory layer.
    - `nn.GRU`: Gated Recurrent Unit layer.
6. **Dropout Layers**:
    - `nn.Dropout`, `nn.Dropout2d`, `nn.Dropout3d`: Dropout layers for regularization.
7. **Embedding Layers**:
    - `nn.Embedding`: A simple lookup table that stores embeddings.
8. **Activation Functions** (although many are also available in `torch.nn.functional`):
    - `nn.ReLU`, `nn.LeakyReLU`, `nn.PReLU`, `nn.ReLU6`
    - `nn.Sigmoid`, `nn.Tanh`
    - `nn.Softmax`, `nn.LogSoftmax`
9. **Loss Functions**:
    - `nn.MSELoss`: Mean squared error loss.
    - `nn.CrossEntropyLoss`: Cross entropy loss which combines `nn.LogSoftmax` and `nn.NLLLoss`.
    - `nn.BCELoss`: Binary cross entropy loss.
    - `nn.L1Loss`: L1 loss.

### Constructing Neural Networks

When creating a custom neural network, the general steps include:

1. **Define the network structure**: This is done by creating a new class that inherits from `nn.Module`. Inside this class, you'll use the predefined layers from `nn` to construct your neural network. The layers are typically defined in the `__init__` method.
    
2. **Define the forward pass**: You must implement the `forward` method for your custom class, which defines the forward pass of the network. This dictates how data flows through the layers you've defined.

### Key Principles

1. **Composition**: Neural networks in PyTorch are usually built using a composition of pre-existing modules or layers. For instance, a convolutional neural network might be composed of alternating `nn.Conv2d` and `nn.MaxPool2d` layers.
    
2. **Sequential Wrapper**: For networks that are a simple feed-forward sequence of layers, `nn.Sequential` is a handy wrapper:

```python
model = nn.Sequential(
    nn.Conv2d(1, 20, 5),
    nn.ReLU(),
    nn.Conv2d(20, 64, 5),
    nn.ReLU()
)
```

3. **Parameter Management**: Every subclass of `nn.Module` automatically tracks the layers defined in it as attributes (and their parameters). You can get all parameters with `model.parameters()` and specific ones with `model.named_parameters()`.
    
4. **Module Containers**: Apart from `nn.Sequential`, PyTorch provides other containers like `nn.ModuleList` and `nn.ModuleDict` to handle more complex scenarios.
    
5. **Custom Layers**: For more complex networks or non-standard operations, you can also define custom layers. As long as your custom layer subclass `nn.Module` and defines a `forward` method, it can act just like a standard PyTorch layer.

### Example
#### **Convolutional Neural Network (CNN)**

```python
class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)
        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)
        self.fc1 = nn.Linear(7*7*64, 128)
        self.fc2 = nn.Linear(128, 10)  # 10 classes for output

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.max_pool2d(x, 2)
        x = F.relu(self.conv2(x))
        x = F.max_pool2d(x, 2)
        x = x.view(-1, 7*7*64)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x
```

#### 2. **RNN with GRU**

```python
class GRUModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, num_classes):
        super(GRUModel, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, num_classes)

    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        out, _ = self.gru(x, h0)
        out = self.fc(out[:, -1, :])
        return out
```

#### 3. **LSTM**

```python
class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, num_classes):
        super(LSTMModel, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, num_classes)

    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        out, _ = self.lstm(x, (h0, c0))
        out = self.fc(out[:, -1, :])
        return out
```

#### 4. **Transformer**

```python
class TransformerModel(nn.Module):
    def __init__(self, vocab_size, d_model, nhead, num_encoder_layers, num_classes):
        super(TransformerModel, self).__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers)
        self.fc = nn.Linear(d_model, num_classes)

    def forward(self, x):
        x = self.embedding(x)
        x = self.transformer.encoder(x)
        x = self.fc(x[:, -1])
        return x
```