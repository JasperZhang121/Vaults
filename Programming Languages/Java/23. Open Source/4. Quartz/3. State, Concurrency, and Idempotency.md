### The Problem Trio
1. **Retaining small state** between runs (counters, cursors, last timestamps).
2. **Avoiding overlaps** that corrupt shared state or double-process external side effects.
3. **Defining failure semantics** (do we advance state on errors?).

### `@PersistJobDataAfterExecution` — Success-only Write-Back
- **Behavior**: After `execute()` **returns successfully**, Quartz writes the **current `JobDetail.JobDataMap`** back to the store (RAM or JDBC), so the next run sees your updates.
- **Important**:
  - Only modifications to **`context.getJobDetail().getJobDataMap()`** are persisted.
  - Changes to **`context.getMergedJobDataMap()`** are **not** persisted by this annotation (it’s a read-optimized merged view of job + trigger data).
  - If `execute()` throws an exception (including `JobExecutionException`), **no persistence occurs**. This is ideal for “do not advance cursor on failure.”

### `@DisallowConcurrentExecution` — Per-JobDetail Serialization
- **Behavior**: Quartz **prevents concurrent executions of the same `JobDetail`** (same `JobKey`). If a trigger fires while a run is active, the new firing waits or misfires per policy.
- **Granularity**: It is **not** a class-wide lock. Different `JobDetail`s that use the same job class **can** run in parallel.
- **Cluster-aware**: With `JDBCJobStore` and clustering enabled, the non-concurrency guarantee holds across nodes.

### The Golden Pair (modern replacement for legacy `StatefulJob`)
Use the two annotations **together** for stateful, incremental, and cursor-driven tasks:

```java
import org.quartz.*;

@PersistJobDataAfterExecution
@DisallowConcurrentExecution
public class IncrementalIngestJob implements Job {
  public static final String CURSOR = "cursor";

  @Override
  public void execute(JobExecutionContext ctx) throws JobExecutionException {
    JobDataMap map = ctx.getJobDetail().getJobDataMap();  // must write here to persist
    long cursor = map.getLong(CURSOR);

    try {
      long next = ingestFrom(cursor);  // implement idempotent ingestion
      map.put(CURSOR, next);           // persisted only if success
    } catch (Exception e) {
      // no write-back on failure; safe to retry from same cursor
      throw new JobExecutionException(e);
    }
  }
}
```

#### Design Guidance for Safe State

- Keep `JobDataMap` **small and simple** (primitives, strings, tiny structs, or foreign keys). For JDBC, it’s serialized; avoid large or version-fragile objects.
    
- If multiple triggers feed the same `JobDetail`, they share the state—be explicit about the intended coordination.
    
- For **global** serialization across multiple job definitions, use an **external lock** (DB row, Redis/Redisson lock) or route all work through a single `JobDetail`.
    

#### Idempotency & Side-Effects

- Ensure the job can be **re-executed** without harmful duplication:
    
    - Use natural keys / version checks / “upsert” semantics.
        
    - For external APIs, add rate limiting and de-duplication keys.
        
- Favor “**state as pointer**” (cursor/timestamp) in `JobDataMap`; store business-critical outputs in your own persistent tables.
    

#### Anti-Patterns

- Writing to `getMergedJobDataMap()` and expecting durability — **won’t persist**.
    
- Assuming class-level mutual exclusion — **it’s per `JobDetail`**.
    
- Storing large domain objects in `JobDataMap` — risky under serialization and evolution.