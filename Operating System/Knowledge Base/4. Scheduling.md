
Scheduling refers to the process of deciding when a task or process should be executed by the computer. The aim of scheduling is to distribute the available processing time effectively among multiple tasks or processes, with the goal of maximizing the overall performance of the system.

There are several different scheduling algorithms used in computer systems, including:

-   First-Come-First-Served (FCFS)
-   Shortest Job First (SJF)
-   Priority Scheduling
-   Round Robin Scheduling

Each scheduling algorithm has its own advantages and disadvantages and the appropriate algorithm depends on the specific requirements of the system. The operating system determines the order in which processes are executed based on the scheduling algorithm used.

----
## Scheduling Levels

1.  Long-Term Scheduling: This level determines which processes should be executed, based on the priority, system load and available memory. The goal is to keep the system fully utilized and the process queue as short as possible.
    
2.  Medium-Term Scheduling: This level aims to temporarily remove processes that are not currently executing, but have not completed yet, from the CPU to free up resources. The swapped-out processes are saved in the main memory and are eventually swapped back in when the CPU becomes available.
    
3.  Short-Term Scheduling: This level makes the decision of which process should execute next, based on the process priority, CPU burst time, and other scheduling algorithms. This level is executed frequently to ensure that the CPU utilization is optimized.
    
4.  Micro-Scheduling: This level is executed by the CPU itself and is responsible for scheduling the instructions within a process. It determines which instruction should be executed next and ensures that the CPU pipelines are fully utilized.


-----
<span style="color:yellow">The Multi-Level Feedback Queue:</span>

Basic Rules:
- If Priority(A) > Priority(B), A runs (B doesnâ€™t).
- If Priority(A) = Priority(B), A & B run in RR.
- When a job enters the system, it is placed at the highest priority (the topmost queue).
- After some time period S, move all the jobs in the system to the topmost queue.
- After some time period S, move all the jobs in the system to the topmost queue.

<span style="color:yellow">Multiprocessor Scheduling:</span>
Difference between single-CPU hardware and multi-CPU hardware centers around the use of hardware caches, and exactly how data is shared across multiple processors.

Caches are small, fast memories that (in general) hold copies of popular data that is found in the main memory of the system. Main memory, in contrast, holds all of the data, but access to this larger memory is slower. By keeping frequently accessed data in a cache, the system can make the large, slow memory appear to be a fast one.

Caches are thus based on the notion of locality, of which there are two kinds: temporal locality and spatial locality. The idea behind temporal locality is that when a piece of data is accessed, it is likely to be accessed again in the near future; imagine variables or even instructions themselves being accessed over and over again in a loop. The idea behind spatial locality is that if a program accesses a data item at address x, it is likely to access data items near x as well.

