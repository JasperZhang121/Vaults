
When analysis or data mining needs to operate over big data, we need to think about the computational cost of all stages of the processing.

When assessing the character of data, when summarising data, and when developing measures of similarity and interestingness of data and data patterns, some measures are computationally more difficult to compute than others.

The classification of measure functions as _distributive,_ _algebraic_ or _holistic_ is useful to understand their computational complexity.

---

**Distributive**

Suppose the data is partitioned into _n_ sets, and the function is applied to each of those sets. Then, suppose the function is applied to those _n_ results. If the result is the same as applying the function to all of the data without partitioning, then the function is _distributive_. e.g., count(), sum(), min(), max()

Such computations can be made efficient by distributing the computation.

**Algebraic**

A function is _algebraic_ if it can be computed by a function with _M_ arguments (where _M_ is a bounded integer), each of which is obtained by applying a distributive function.

e.g. average() can be computed from sum() and count().

e.g. min_N() and max_N (_N_ minimum/maximum values for integer _N_)  

e.g. standard_deviation()

Such computations can take advantage of the distributive sub-functions to be made efficient.

**Holistic**

A function is holistic if there is no constant bound on the storage size needed to describe sub-functions. That is, there is no algebraic function with _M_ arguments (where _M_ is a bounded integer) that characterises the computation.

e.g.  median(), mode(), rank()

Such computations are difficult to compute efficiently and often efficient _approximations_ are used instead.