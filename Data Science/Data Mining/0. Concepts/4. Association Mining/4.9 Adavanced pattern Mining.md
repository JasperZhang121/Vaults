
Algorithms other than a-priori have been developed to <mark style="background: #FF5582A6;">reduce database scans</mark> or <mark style="background: #FF5582A6;">reduce main memory usage</mark> and these may scale better over large datasets.

Some of the most well-known are:

-   FP-growth, frequent pattern growth
-   Mining closed and max-patterns

Some algorithms rely on representing transactions in a _vertical_ form  as <item, set of TIDs> instead of the _horizontal_ form <TID, set of items> that we saw previously. However, many of these aternative approaches use the apriori property like the apriori algorithm.

**Mining Advanced Patterns**

So far, we have spoken of very simple pattern mining where all the data in a transaction corresponds to multiple values drawn from a single nominal domain. Many extensions have been developed that  apply more structure over the input data handled and the rules produced.  For example,

**Multilevel association rules**: employing a concept hierarchy to discover rules at different levels of abstraction,  and to do association mining at each level  in a top-down manner to prune some lower-level, low-support items from the search.  

**Multidimensional associations:** replacing items in itemsets by instantiated relational predicates,  so that properties of  key objects in the transaction can also participate in mining (e.g. the price of each item purchased) and that mined rules become more expressive.  

**Rare patterns and negative patterns:** looking for patterns  that are surprisingly _infrequent_ or that are strongly negatively correlated by lift.  Apriori is inefficient in this scenario.

**Compressed or approximate patterns**: looking for the _top-k_ most frequent patterns or constraint-based patterns (for which the constraints can be used while pattern mining to reduce the search space), or clustering patterns so that a cluster representative itemset can  stand for  all the patterns in the cluster more efficiently.  

**Sequential patterns**: where the order of items is significant, for example in text or DNA sequences.  

**Graph patterns:** where frequent sub-graphs may be discovered in graph  network structures.

---

#### Extended data types 

Until now, we have assumed that all _items_ are pretty much the same, i.e. they  can all be seen as alternative values of a single nominal variable (say, _item_) without any further structure. We can view the problem as single-dimensional analysis over the dimension _item_, although multiple _items_ occur in a transaction.  This assumption seriously limits the scope of application of association mining.  

**Multi-dimensional patterns over nominal data**

we mixed (controllable) lifestyle attributes with disease attributes for a person as indistinguishable items for a **single-dimensional rule.** Such rules are also called **Boolean association rules** since the binary  presence or absence of an item is represented.   Some advanced pattern mining methods rely on explicitly distinguishing the dimensions for **multidimensional association rules**, but the apriori algorithm does not do this.  

A simple solution to distinguish the attributes  for single-dimensional algorithms like apriori is to transform the values of nominal attributes to explicit attribute-value pairs and so to transfer all distinct nominal domains to the one nominal item domain as required by a-priori.

Example

For the domains _Disease = (diabetes, cerebrovascular, liver)_,  and _Lifestyle = (drinking, overweight, smoking, exercise)_ a transaction such as {_diabetes, overweight, exercise_} would be transformed to the transaction _{Disease**=**diabetes, Lifestyle=overweight, Lifestyle=exercise}_ and these can then be treated homogenously as if from a single domain.

This transformation need not influence the data mining algorithm, that is, a single-dimensional algorithm like a-priori can be used. It could, however,  affect  the measures for interestingness of rules and how you would like results to be presented.

**Ordinal data**

Typically,  frequent itemset mining does not take account of any meaningful order amongst the values for categorical data. Therefore ordinal data can be treated as nominal data (as above), or alternatively, where the number of alternative values is very large, like for continuous data (as below).

**Quantitative (continuous)  data**

A **quantitative association rule**  is one where, based on items that take on **quantitative values** drawn from a  numeric domain, rules include attribute values partitioned into discrete intervals, such as age _18-25_. There are two broad approaches that can be used: _static_, or the more sophisticated _dynamic_.  

**Static**

One approach to mining over continuous data is to transform quantitative attributes  into discrete, pre-determined nominal attributes  in advance. 

Example:

For the domain _Age = (0..120)_, a value such as "_25_" would be transformed to the item _Age=18-25_ and a value  "_99_"  would be transformed to the item _Age=75 and over._

The pre-determined method is generally most approriate where the intervals have meaning in the domain, generally related to where consequent actions can be targeted. For example, partitioning _age_  into the year groups  that correspond to educational stages might be helpful for targeting education-related interventions.

This transformation need not influence the data mining algorithm, that is, a nominal-only algorithm like _apriori_ can then be used effectively.

**Dynamic**

A more sophisticated approach is to cluster attribute values into bins relying on the data distribution, and  then further combining these bins during the mining process. In this case we are looking for a dynamic discretisation that interacts with the mining algorithm to choose  good value ranges  that, for example, maximise the confidence of rules. That is, you need to choose a mining method that is aware of the continuous values and an off-the-shelf apriori algorithm will not do.

One approach to this problem is to integrate the frequent item-set discovery with a data warehouse cube structure, so that the aggregate counts over multiple dimensions in the cube can be retrieved in a top-down fashion, (either drilling down over concept hierarchies or drilling down  to introduce additional dimensions) and stopping  the drill-down when a  count fails minimum support (because the apriori condition ensures that no lower cube will satisfy minimum support) and using the dimensional labels at that level as the bins for nominal items.  

An alternative typical approach is to integrate a _clustering_ method (covered later in the course). For each quantitative attribute a clustering algorithm is applied to each dimension to to satisfy minimum support. Then for each such cluster, we  look to combine it with another dimension to find a 2-D cluster with minimum support, and so on to higher dimensions. The range of attribute values that  occur in a cluster then define the discrete bins for nominal items. The apriori condition tells us that we need not look to combine a cluster with another once it fails to satisfy minimum support.

**Interpretation of rules over transformed data**

If the data has been transformed as discussed here, then it is important to be aware of this when interpreting the rules and the evaluation metrics for the rules.  For example, rules that contain no _Disease_ item may be considered irrelevant and so uninteresting, irrespective of the interestingness measures. For another example, when some quantitative variable say _age_, has been split into two intervals by the median value, and so every transaction has either one interval value or the other, both items will be highly frequent with 50% support. Assuming the other items are more scattered, pretty much _every_ frequent itemset must include one _age_ or the other, but never both, so you should be very careful in concluding that the presence of one of them in a frequent itemset is somehow significant.