
Handle missing attribute values
- Assign the most common value of the attribute
- Assign probability to each of the possible values

Attribute construction
- Create new attributes based on existing ones that are sparsely represented
- This reduces fragmentation, repetition, and replication

Continuous target variable
- In this case the tree is called a _regression tree_, the leaf node classes are represented by their mean values, and  the tree performs prediction (using that mean value) rather than classification.

Probabilistic classifier
- Instead of majority voting to assign a class label to a leaf node, the _proportion_ of training data objects of each class in the leaf node can be interpreted to as the _probability_ of the class, and this probability can be assigned to the classification  for unknown objects falling in that node at use-time.

---

Extracting rules from decision trees is a useful process for understanding the decision-making process and explaining the results to stakeholders. A decision tree can be <mark style="background: #FFB86CA6;">transformed into a set of if-then rules</mark>, which can be used to classify new data. The rules can also be analyzed to identify patterns and gain insights into the data.

The process of rule extraction involves traversing the decision tree from the root node to the leaf nodes and creating rules for each path. The rule for each path is created by combining the conditions along the path to form an if-then statement. The condition of each internal node is combined using the logical AND operator, while the condition at the leaf node is used as the output or decision.

The rules can be simplified by removing redundant conditions and combining similar rules. This process is called rule pruning and can be performed manually or using algorithms. The resulting rules can be expressed in a human-readable format and used to explain the decision-making process to stakeholders

**Properties of extracted rules**

-   Mutually exclusive
    -   We cannot have rule conflicts here because no two rules will be triggered for the same tuple.
    -   We have one rule per leaf, and any tuple can map to only one leaf.
-   Exhaustive:
	-   There is one rule for each possible attribute–value combination, so that this set of rules does not require a default rule.
	-   The order of rules is irrelevan

