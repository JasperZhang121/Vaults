
>1


| age    | club | wins |
|--------|------|------|
| child  | red  | yes  |
| child  | blue | no   |
| youth  | red  | yes  |
| adult  | red  | no   |
| adult  | blue | yes  |


Consider the Athletics dataset, D, above which is a training set for a decision tree classifier aiming to predict the variable "wins" from age and club membership. Let p_i be the probability that an arbitrary tuple in D belongs to class C_i, of m classes estimated by $p_i=\frac{|C_{i, D}|}{|D|}$. Then the information needed to classify a tuple in D is defined by $\it{Info}(D) = - \sum_{i=1}^{m} p_i\it{log}_2(p_i)$ and after using attribute A to split D into v partitions, the information needed is $\it{Info}_A(D) = \sum_{j=1}^{v}\frac{|D_j|}{|D|}\times\it{Info}(D_j)$ So the information gain by splitting is defined as $\it{Gain}(A) = \it{Info}(D) - \it{Info_A}(D)$



#### 1
>What is the information gain for splitting  D on the three categories of  attribute "age"? 

To calculate the information gain for a given attribute A, we first need to calculate the information needed to classify a tuple in D, $\it{Info}(D)$, and the information needed to classify a tuple in D after using A to split D into v partitions, $\it{Info_A}(D)$. We can then calculate the information gain as the difference between these two values.

$\it{Info}(D)$ is the entropy of the class distribution in D, and can be calculated as:

$\it{Info}(D) = - \sum_{i=1}^{m} p_i\it{log}_2(p_i)$

where p_i is the probability that an arbitrary tuple in D belongs to class C_i, and m is the number of classes.

In this case, we have two classes: "yes" and "no". So we can calculate \it{Info}(D) as:

$\it{Info}(D) = - \frac{3}{5}\it{log}_2\left(\frac{3}{5}\right) - \frac{2}{5}\it{log}_2\left(\frac{2}{5}\right) = 0.971$

Next, we need to calculate $\it{Info_A}(D)$, which is the weighted average of the entropy of the class distribution in each partition created by splitting D on attribute A. It can be calculated as:

$\it{Info_A}(D) = \sum_{j=1}^{v}\frac{|D_j|}{|D|}\times\it{Info}(D_j)$

where $|D_j|$ is the number of tuples in partition j, and |D| is the total number of tuples in D.

To calculate the information gain for each attribute, we need to split D on that attribute and calculate $\it{Info_A}(D)$ for each resulting partition. We can then calculate the information gain as the difference between $\it{Info}(D$) and $\it{Info_A}(D)$ for each attribute.

For example, if we split D on the attribute "age", we get three partitions:

-   Partition 1: age = "child", with 2 tuples (1 "yes" and 1 "no")
-   Partition 2: age = "youth", with 1 tuple (1 "yes" and 0 "no")
-   Partition 3: age = "adult", with 2 tuples (1 "yes" and 1 "no")

The entropy of the class distribution in each partition can be calculated as:

-   $Partition 1: \it{Info}(D_1) = - \frac{1}{2}\it{log}_2\left(\frac{1}{2}\right) - \frac{1}{2}\it{log}_2\left(\frac{1}{2}\right) = 1.0$
-   $Partition 2: \it{Info}(D_2) = - \frac{1}{1}\it{log}_2\left(\frac{1}{1}\right) - \frac{0}{1}\it{log}_2\left(\frac{0}{1}\right) = 0$
-   $Partition 3: \it{Info}(D_3) = - \frac{1}{2}\it{log}_2\left(\frac{1}{2}\right) - \frac{1}{2}\it{log}_2\left(\frac{1}{2}\right) = 1.0$

Next, we need to calculate the weighted average of the entropy of the class distribution in each partition:

$\it{Info_A}(D) = \frac{2}{5}\times\it{Info}(D_1) + \frac{1}{5}\times\it{Info}(D_2) + \frac{2}{5}\times\it{Info}(D_3) = 0.8$

Therefore, the information gain of splitting the data on "age" into three partitions with the updated class distribution is:

$\it{Gain}("age") = \it{Info}(D) - \it{Info_A}(D) = 0.971 - 0.8 = 0.171$

This means that splitting the data on "age" into three partitions with the updated class distribution results in a reduction in the entropy of the class distribution in D by 0.171 bits, which is the amount of information gained by using the attribute "age" to split the data.


#### 2

>If we split the data on the attribute "club" into two partitions based on the categories "red" and "blue", we get the following partitions:

To calculate the entropy of the class distribution in each partition, we can use the same formula as before:

-   $Partition 1: \it{Info}(D_1) = - \frac{2}{3}\it{log}_2\left(\frac{2}{3}\right) - \frac{1}{3}\it{log}_2\left(\frac{1}{3}\right) = 0.918$
-   $Partition 2: \it{Info}(D_2) = - \frac{1}{2}\it{log}_2\left(\frac{1}{2}\right) - \frac{1}{2}\it{log}_2\left(\frac{1}{2}\right) = 1.0$

Next, we need to calculate the weighted average of the entropy of the class distribution in each partition:

$\it{Info_A}(D) = \frac{3}{5}\times\it{Info}(D_1) + \frac{2}{5}\times\it{Info}(D_2) = 0.871$

Therefore, the information gain of splitting the data on the attribute "club" into two categories with the updated class distribution is:

$\it{Gain}("club") = \it{Info}(D) - \it{Info_A}(D) = 0.971 - 0.871 = 0.100$

This means that splitting the data on the attribute "club" into two categories with the updated class distribution results in a reduction in the entropy of the class distribution in D by 0.100 bits, which is the amount of information gained by using the attribute "club" to split the data.


#### 3
>-

A higher information gain from splitting an attribute indicates that the attribute is more useful for predicting the target variable, and splitting on that attribute will result in a greater reduction in the entropy of the class distribution in the data.

In the context of a decision tree classifier, we want to choose the attribute that provides the highest information gain as the root of the tree, as this will result in the most effective split of the data and the most accurate predictions for the target variable.

So in the case of the Athletics dataset, we saw that splitting on the attribute "age" resulted in a higher information gain than splitting on the attribute "club", indicating that "age" is a more useful attribute for predicting "wins". Therefore, we would choose "age" as the root of the decision tree and split the data accordingly.

