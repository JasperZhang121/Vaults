
### Confusion Matrix

-   The confusion matrix is a tool for <mark style="background: #BBFABBA6;">evaluating the performance of a classifier</mark> on a binary classification problem.
-   It is a 2 by 2 matrix where each entry represents the number of tuples classified as belonging to a particular class (positive or negative) versus the actual class of those tuples.
-   The entries of the confusion matrix are calculated as follows:
    -   True Positives (TP): number of positive tuples that were correctly labeled as positive by the classifier.
    -   True Negatives (TN): number of negative tuples that were correctly labeled as negative by the classifier.
    -   False Positives (FP): number of negative tuples that were incorrectly labeled as positive by the classifier.
    -   False Negatives (FN): number of positive tuples that were incorrectly labeled as negative by the classifier.
-   The TP and TN entries indicate correct predictions, while the FP and FN entries indicate incorrect predictions.
-   The confusion matrix can be used to calculate <mark style="background: #FFF3A3A6;">various evaluation metrics such as accuracy, precision, recall, and F1-score</mark>.
-   Accuracy is the proportion of correctly labeled tuples out of the total number of tuples. However, it can be misleading if the dataset is imbalanced.
-   Precision is the proportion of true positive predictions out of all positive predictions, while recall (or sensitivity) is the proportion of true positive predictions out of all actual positive tuples.
-   F1-score is the harmonic mean of precision and recall, and is useful when you want to balance precision and recall.
-   It's important to choose an appropriate evaluation metric based on the specific problem and the domain of application.
-   Confusion matrices can also be used for multiclass classification problems by extending the matrix to include all possible combinations of predicted and actual classes.

**General Format:**

| Actual Class (rows) \ Predicted Class (columns) | C1=Positive | C2=Negative |
| --- | --- | --- |
| C1=Positive | True Positives (TP) | False Negatives (FN) |
| C2=Negative | False Positives (FP) | True Negatives (TN) |

---

### Evaluation Measures:
| Evaluation Measure | Formula |
|--------------------|---------|
| Accuracy           | $accuracy = \frac{TP+TN}{P+N}$ |
| Error rate         | $error \ rate = \frac{FP+FN}{P+N}$ |
| Precision          | $precision = \frac{TP}{TP+FP}$ |
| Sensitivity (Recall)| $sensitivity = \frac{TP}{P}$ |
| Specificity        | $specificity = \frac{TN}{TN+FP}$ |
| F1-score           | $F1 = 2 \cdot \frac{precision \cdot recall}{precision + recall}$ |
| $F_\beta$, where beta is a non-negative real number       | $F_\beta = (1+\beta^2)\frac{precision \cdot recall}{(\beta^2 \cdot precision) + recall}$ |


### Class Imbalance Problem

The Class Imbalance Problem arises when the <mark style="background: #FFF3A3A6;">distribution of classes in a dataset is not balanced</mark>. This can lead to skewed performance evaluation, particularly if the evaluation measure used is accuracy. When dealing with a <mark style="background: #ABF7F7A6;">rare class, a high accuracy may not indicate good classification performance</mark>. Instead, evaluation measures such as sensitivity, specificity, precision, recall, and F1-score can be used to better assess the performance of the classifier.

For example, consider a cancer classification problem where only a small proportion of patients have cancer. A classifier with high accuracy may still fail to identify many cancer patients, leading to a <mark style="background: #ADCCFFA6;">high rate of false negatives</mark>. In this case, sensitivity (the proportion of actual positives that are correctly identified by the classifier) becomes a more relevant evaluation measure than specificity (the proportion of actual negatives that are correctly identified by the classifier).

Precision and recall are alternative measures that can be useful in evaluating classification performance, particularly when the classes are imbalanced. Precision measures the proportion of true positive predictions among all positive predictions, while recall measures the proportion of true positive predictions among all actual positive tuples. These measures are often combined into an F1-score, which provides a single measure of performance that balances precision and recall. However, it's important to choose the appropriate evaluation metric for the specific problem and domain of application, as different evaluation measures may emphasize different aspects of classification performance.

---

### Estimating a classifierâ€™s accuracy

