
### Confusion Matrix

-   The confusion matrix is a tool for <mark style="background: #BBFABBA6;">evaluating the performance of a classifier</mark> on a binary classification problem.
-   It is a 2 by 2 matrix where each entry represents the number of tuples classified as belonging to a particular class (positive or negative) versus the actual class of those tuples.
-   The entries of the confusion matrix are calculated as follows:
    -   True Positives (TP): number of positive tuples that were correctly labeled as positive by the classifier.
    -   True Negatives (TN): number of negative tuples that were correctly labeled as negative by the classifier.
    -   False Positives (FP): number of negative tuples that were incorrectly labeled as positive by the classifier.
    -   False Negatives (FN): number of positive tuples that were incorrectly labeled as negative by the classifier.
-   The TP and TN entries indicate correct predictions, while the FP and FN entries indicate incorrect predictions.
-   The confusion matrix can be used to calculate <mark style="background: #FFF3A3A6;">various evaluation metrics such as accuracy, precision, recall, and F1-score</mark>.
-   Accuracy is the proportion of correctly labeled tuples out of the total number of tuples. However, it can be misleading if the dataset is imbalanced.
-   Precision is the proportion of true positive predictions out of all positive predictions, while recall (or sensitivity) is the proportion of true positive predictions out of all actual positive tuples.
-   F1-score is the harmonic mean of precision and recall, and is useful when you want to balance precision and recall.
-   It's important to choose an appropriate evaluation metric based on the specific problem and the domain of application.
-   Confusion matrices can also be used for multiclass classification problems by extending the matrix to include all possible combinations of predicted and actual classes.

| Actual Class (rows) \ Predicted Class (columns) | C1=Positive | C2=Negative |
| --- | --- | --- |
| C1=Positive | True Positives (TP) | False Negatives (FN) |
| C2=Negative | False Positives (FP) | True Negatives (TN) |


Various evaluation measures can be defined using a confusion matrix with four primitive measures:

-   Accuracy: The proportion of correctly labeled tuples out of the total number of tuples. It can be calculated as:
    
    $accuracy = \frac{TP+TN}{P+N}$
    
-   Error rate: The proportion of incorrectly labeled tuples out of the total number of tuples. It can be calculated as:
    
    $error \ rate = \frac{FP+FN}{P+N}$
    
-   Precision: The proportion of true positive predictions out of all positive predictions. It can be calculated as:
    
    $precision = \frac{TP}{TP+FP}$
    
-   Recall (or sensitivity): The proportion of true positive predictions out of all actual positive tuples. It can be calculated as:
    
    $recall = \frac{TP}{TP+FN}$
    
-   Specificity: The proportion of true negative predictions out of all actual negative tuples. It can be calculated as:
    
    $specificity = \frac{TN}{TN+FP}$
    
-   F1-score: The harmonic mean of precision and recall, and is useful when you want to balance precision and recall. It can be calculated as:
    
    $F1 = 2 \cdot \frac{precision \cdot recall}{precision + recall}$
