
![[ridge.png]]

Ridge Regression is a regularized linear regression model that is used to address the issue of multicollinearity in linear regression. It works by adding a L2 regularization term to the loss function of linear regression, which helps to prevent overfitting by penalizing large coefficients.

The L2 regularization term is defined as the sum of the squares of the coefficients, multiplied by a regularization strength parameter $\lambda$ (λ). The loss function for Ridge Regression is given by:

$$ Loss(y, y_{pred}) = \sum((y - y_{pred})^2) + \lambda * \sum(w^2) $$

where:

-   `y` is the actual target value
-   `y_pred` is the predicted value
-   `w` is the vector of coefficients
-   `λ` is the regularization strength

The goal of Ridge Regression is to find the coefficients that minimize this loss function.

Ridge Regression can be implemented in Python using the `Ridge` class from the `sklearn.linear_model` module in scikit-learn library.

---

```python
import numpy as np

A = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])

rank = np.linalg.matrix_rank(A)
print("Rank of matrix A:", rank)
```

If the rank of the matrix is equal to the minimum number of its rows and columns, the matrix is said to have full rank. In the case of this example, the matrix `A` has full rank, as its rank is equal to its minimum number of rows and columns (3). If the rank of the matrix is less than its minimum number of rows and columns, the matrix is said to be rank-deficient.