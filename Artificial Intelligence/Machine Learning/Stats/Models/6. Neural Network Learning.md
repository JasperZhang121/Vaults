
Neural networks are a type of machine learning model that are based on the <mark style="background: #D2B3FFA6;">structure and function of the human brain</mark>. They consist of a set of connected nodes, where each node is an input, output, or hidden layer, and each connection between nodes has a weight associated with it. During training, the weights are adjusted to allow the network to correctly output the desired output for a given input. The classic training algorithm for neural networks is called <mark style="background: #FFF3A3A6;">backpropagation</mark>.

### Strengths & Weakness

**Strengths** of neural networks as a classifier include their <mark style="background: #FFB86CA6;">high tolerance to noisy data</mark>, ability to <mark style="background: #ABF7F7A6;">classify untrained patterns, and suitability for continuous-valued inputs and outputs</mark>. They have been successful on a wide range of real-world data, such as hand-written letters, images, and voice. Neural network algorithms are inherently parallel and can take advantage of modern hardware to speed up training and inference. Techniques have also been developed to extract rules from trained neural networks.

**Weaknesses** of neural networks include their <mark style="background: #FF5582A6;">long training time</mark> and the need to determine <mark style="background: #D2B3FFA6;">a number of parameters empirically</mark>, such as network topology or structure. They also tend to overfit when there are a large number of nodes, which implies a large number of weights to be learned. Neural networks are generally <mark style="background: #FF5582A6;">difficult to interpret</mark>, as the learned weights and hidden units have no clear symbolic meaning, and they are often referred to as "black box" models.


### Multi-Layer Feed-Forward Neural Network

![[Multi.png]]

The <mark style="background: #FFB8EBA6;">inputs to the network correspond to the attributes measured</mark> for each training tuple. Inputs are fed simultaneously into the units making up the input layer. They are then weighted and fed simultaneously to a hidden layer. The <mark style="background: #FFB86CA6;">number of hidden layers is arbitrary</mark> (1 hidden layer in the example above). The number of hidden units is arbitrary (3 hidden units in the example above). The weighted outputs of the last hidden layer are input to units making up the output layer, which emits the network's prediction.

The network is feed-forward: none of the weights cycles back to an input unit or to an output unit of a previous layer. From a statistical point of view, networks perform nonlinear regression: given enough hidden units and enough training samples, they can closely approximate any function (output).

### Defining a Network Topology

1.  Decide the <mark style="background: #ADCCFFA6;">network topology</mark>:
    
    -   Specify the number of units in the input layer: usually one input unit per attribute in the data (but nominal attributes can have one input per value).
    -   The number of hidden layers (at least one, and commonly only one).
    -   The number of units in each hidden layer.
    -   The number of units in the output layer.
    
1.  Output, one unit per response variable. In a typical binary classification problem, only one output unit is used and a threshold is applied to the output value to select the class label. The value can be interpreted as a probability of belonging to the positive class, and in this way, a neural network can be used as a probabilistic model. For classification of more than two classes, one output unit per class is used and the highest-valued class is selected for the label.
    
3.  Choose an <mark style="background: #ADCCFFA6;">activation function</mark> for each hidden and output unit .
    
4.  Usually randomly, determine initial values for the weights.

Once a network has been trained, if its accuracy is unacceptable, try a different network topology or a different set of initial weights or maybe different activation functions. Use trial-and-error or there are methods that systematically search for a high-performing topology.


### Prediction with Neural Network

1.  Initialise the weights by training

-   The weights in the network are set to small numbers (e.g., ranging from −1.0 to 1.0, or −0.5 to 0.5) by training.
-   Each unit has a bias associated with it, as explained later. The biases are similarly set to small numbers by training.
-   Each testing tuple is first normalised to [0.0 ~ 1.0]. Consider a normalised tuple X, processed by the following steps.

2.  Propagate the inputs forward

-   First, the testing tuple is normalised to [0.0 ~ 1.0] and the normalised tuple X is fed to the network’s input layer. The inputs pass through the input units, unchanged.
-   Hidden layer
    -   Input of hidden unit: all outputs of the previous layer, e.g. if the hidden unit is in the first hidden layer, then the inputs are $x_1, x_2, ..., x_n$.
    -   Output of hidden unit j: weighted linear combination of its input followed by an activation function
        -   $O_j = f(\sum_{i=1}^{n}w_{ij}\cdot x_i + \theta_j)$
        -   where $w_{ij}$ is the weight of the connection from input i in the previous layer to unit j
        -   $\theta_j$ is the bias variable of unit j
        -   $f(\cdot)$ is a non-linear activation function which will be described later.
    -   If there is more than one hidden layer, the above procedure will be repeated until the final hidden layer will result outputs.
-   Output layer
    -   The outputs of the final hidden layer will be used as inputs of the output layer.
    -   The number of output units are determined by a task
    -   If our goal is to predict a single numerical variable, then one output unit is enough
    -   Final output $O_k$:
        -   $O_k = \sum_{j=1}^{h}w_{jk}\cdot O_j + \theta_k$ or $O_k = f(\sum_{j=1}^{h}w_{jk}\cdot O_j + \theta_k)$
    -   $O_k$ is the final predicted value given $x_1, x_2, ..., x_n$.

Computation graph of hidden unit j

-   Graphical illustration of the computational flow of hidden unit j: The inputs to unit j are outputs from the previous layer. These are multiplied by their corresponding weights to form a weighted sum, which is added to the bias $\theta_j$ associated with unit j. A nonlinear activation function f is applied to the net input. Again the output of this unit will be used as an input of the successive layer.

Non-linear activation function

-   An activation function imposes a certain non-linearity in a neural network. There are several possible choices of activation functions, but sigmoid (or logistic) function is the most widely used activation function.
-   Sigmoid function
    -   Sigmoid functions have domain of all real numbers, with return value monotonically increasing most often from 0 to 1.
    -   $f(x) = \frac{1}{1+e^{-x}}$
    -   Also referred to as a squashing function, because it maps the input domain onto the range of 0 to 1
-   Other activation functions
    -   Hyperbolic tangent function (tanh)
    -   Softmax function (use for output nodes for classification problems to push the output value towards binary 0 or 1)
    -   ReLU
    -   etc.

![[hidden_unit.png]]


### Backpropagation

1.  Each training tuple is first normalized to [0.0 ~ 1.0].
    
2.  The error is propagated backward by updating the weights and biases to reflect the error of the network's prediction for the dependent variable for a training tuple.
    
3.  For unit k in the output layer, its error $E_k$ is computed by: $E_k = O_k(1-O_k)(T_k-O_k)$ where $O_k$ is the actual output of unit k, and $T_k$ is the known target value of the given training tuple.
    
4.  To compute the error of a hidden layer unit j, the weighted sum of the errors of the units connected to unit k in the next layer is considered: $E_j = O_j(1-O_j) * sum_k(E_k * w_{jk})$ where $w_{jk}$ is the weight of the connection from unit j to a unit k in the next higher layer.
    
5.  The weights and biases are updated to reflect the propagated errors. Each weight $w_{jk}$ in the network is updated by the following equations, where $\Delta w_{jk}$ is the change in weight $w_{jk}$: $\Delta w_{jk} =\ell * E_k * O_j w_{jk} = w_{jk} + \Delta w_{jk}$ Each bias $\theta_j$ in the network is updated by the following equations, where $\Delta\theta_j$ is the change in bias $\theta_j$: $\Delta\theta_j = \ell * E_j \theta_j = \theta_j + \Delta\theta_j$
    
6.  The parameter $\ell$ is the learning rate, typically a value between 0.0 and 1.0. If the learning rate is too small, then learning will occur at a very slow pace. If the learning rate is too large, then oscillation between inadequate solutions may occur. A rule of thumb is to set the learning rate to 1/t, where t is the number of iterations through the training set so far.
    
7.  The weight and bias increments can be accumulated in variables, so that the weights and biases are updated after all the tuples in the training set have been presented. Alternatively, the backpropagation algorithm given here updates the weights and biases immediately after the presentation of each tuple.
    
8.  Either way, we keep updating until we have a satisfactory error, presenting each tuple of the training set many times over. In theory, the mathematical derivation of backpropagation employs epoch updating, yet in practice, case updating is more common because it tends to yield more accurate results.

### Terminate training

- When your weights  and biases are changing very little , ie. all the $\Delta$$s$ are small; or 
- Accuracy on the training set is good enough; or
- A prespecified number of epochs have passed, or
- Your error on a validation dataset tested after each epoch is changing very little.


### Performance Evaluation

For numerical prediction tasks, neural networks can be evaluated using methods such as <mark style="background: #ABF7F7A6;">mean squared error, mean absolute error, and correlation coefficient</mark>.

For classification tasks, neural networks can be evaluated using methods such as <mark style="background: #FFF3A3A6;">confusion matrix, accuracy, precision, recall, F1 score, and ROC curve</mark>.

Neural networks can also be used as probabilistic classifiers, and methods such as ROC analysis can be used to evaluate their performance.

Interpretability of Neural Networks:

One of the major drawbacks of neural networks is their <mark style="background: #BBFABBA6;">lack of interpretability</mark>. The acquired knowledge in the form of a network of units connected by weighted links is difficult for humans to interpret. However, there are some methods that can be used to extract easier-to-interpret rules from the network, such as:

1.  Rule extraction by network pruning: This method involves <mark style="background: #ADCCFFA6;">simplifying the network structure by removing weighted links that have the least effect</mark> on the trained network. The set of input and activation values are then studied to derive rules describing the relationship between the input and hidden unit layers.
    
2.  Sensitivity analysis for interpretability: This method involves assessing the impact that a given input variable has on a network output. The input to the variable is varied while the remaining input variables are fixed at some value. The knowledge gained from this analysis can be represented in rules, such as “IF X decreases 5% THEN Y increases 8%.”

### Other Neural Net Architectures

![[neuralnetworks.png]]


