
Same as Least Square [[3. Cost function - Least Square]]

The normal equation is a closed-form solution to the linear regression problem, which is used to find the optimal coefficients of the linear model that minimize the sum of squared errors.

![[normal Equation.png]]

---
```R
θ = (X^T * X)^-1 * X^T * y
```

where:

-   `θ` is the coefficient vector of the linear regression model, with dimensions `(n + 1) x 1`, where `n` is the number of independent variables.
-   `X` is the design matrix, with dimensions `m x (n + 1)`, where `m` is the number of observations and `n` is the number of independent variables. The design matrix includes an extra column of ones to represent the intercept term.
-   `y` is the dependent variable vector, with dimensions `m x 1`, where `m` is the number of observations.
-   `^T` represents the transpose operation, and `^-1` represents the inverse operation.

The normal equation finds the values of the coefficients `θ` that minimize the sum of squared errors by solving for `θ` directly, without the need for iterative optimization algorithms like gradient descent.


```python
import numpy as np

# Define the design matrix
X = np.array([[1, 2], [1, 3], [1, 4]])

# Define the dependent variable vector
y = np.array([[3], [4], [5]])

# Calculate the transpose of the design matrix
XT = np.transpose(X)

# Calculate the product of the transpose of the design matrix and the design matrix
XTX = np.dot(XT, X)

# Calculate the inverse of the product of the transpose of the design matrix and the design matrix
XTX_inv = np.linalg.inv(XTX)

# Calculate the product of the inverse of the product of the transpose of the design matrix and the design matrix and the transpose of the design matrix
XTX_inv_XT = np.dot(XTX_inv, XT)

# Calculate the product of the inverse of the product of the transpose of the design matrix and the design matrix and the transpose of the design matrix and the dependent variable vector
theta = np.dot(XTX_inv_XT, y)

# Print the result
print(theta)
```
