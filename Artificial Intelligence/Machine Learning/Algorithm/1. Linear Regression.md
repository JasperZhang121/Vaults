
In [statistics](https://en.wikipedia.org/wiki/Statistics "Statistics"), **linear regression** is a [linear](https://en.wikipedia.org/wiki/Linearity "Linearity") approach for modelling the relationship between a [scalar](https://en.wikipedia.org/wiki/Scalar_(mathematics) "Scalar (mathematics)") response and one or more explanatory variables (also known as [dependent and independent variables](https://en.wikipedia.org/wiki/Dependent_and_independent_variables "Dependent and independent variables")). The case of one explanatory variable is called _[simple linear regression](https://en.wikipedia.org/wiki/Simple_linear_regression "Simple linear regression")_; for more than one, the process is called **multiple linear regression**.

### Details:

-   Linear Regression models the relationship between <mark style="background: #ABF7F7A6;">one dependent variable and multiple explanatory variables</mark>.
-   The input to the model includes independent, explanatory, and predictor variables, while the output is a dependent, target, or response variable.
-   Regression provides a way to predict a continuous variable given other variables, which are typically also continuous.
-   Ordinal independent or dependent variables can be mapped to numeric values, and nominal variables, including binary, can also be mapped to numeric values, but this requires careful assessment of their effect on results.
-   Types of relationships considered in regression include deterministic and statistical relationships.
-   A <mark style="background: #FF5582A6;">deterministic relationship is one where the relationship between explanatory and dependent variables is perfect</mark>.
-   A statistical relationship is one where the relationship between explanatory and dependent variables is not perfect.


### Simple Linear Regression

a statistical method used to study relationships between two continuous variables, where one variable is regarded as the predictor and the other as the response variable. Simple linear regression models this relation between predictor and response variables using the following equation:

$\hat{y} = w_0 + w_1\times x$

where $w_0$ (y-intercept or bias) and $w_1$ (slope, or weight) are coefficient parameters, and $\hat{y}$ is the predicted response variable based on the predictor variable $x$.

To estimate the proper values of $w_0$ and $w_1$, we define the <mark style="background: #FFB8EBA6;">sum of square error</mark> as:

$E = \sum_{i=1}^{n} ( y_i - (w_0 + w_1\times x_i))^2,$

where $\hat{y}$ is the predicted response variable, and the sum of square error computes the sum of the squared differences across all training data points.

The performance of the linear regression model can be evaluated using the following methods:

-   Mean Absolute Error (MAE): the <mark style="background: #FFB86CA6;">average of the individual errors</mark>, each one computed as the absolute value of the difference between the actual value and the predicted value.
$$\begin{equation} MAE = \frac{\sum_{i=1}^{n} \lvert( y_i - \hat{y}_i)\rvert }{n} \end{equation}$$
-   Root Mean Square Error (RMSE): <mark style="background: #BBFABBA6;">the square root of the average of squared errors for each value</mark> of the dependent variable, where error is the simple difference of the actual value and predicted value.
$$\begin{equation} RMSE = \sqrt{\frac{\sum_{i=1}^{n} ( y_i - \hat{y}_i)^2 }{n}} \end{equation}$$
-   R-squared: <mark style="background: #FFF3A3A6;">the proportion of the dependent variable variance that is explained by the linear model</mark>. It is equivalently the square of the Pearson's correlation between the predicted values and the actual values.

$$ \begin{equation} R^2 = 1 - \frac{\sum_{i=1}^{n} ( y_i - \hat{y}_i)^2 }{\sum_{i=1}^{n} (y_i- \bar y)^2} \end{equation}$$

In addition, plotting the predictions vs observations on a Cartesian plane and visually assessing if the points appear to line up can also be useful in evaluating the performance of the model. All of these approaches should be applied to test data rather than training data for a proper assessment, and multifold cross-validation may be applied using the quantitative performance measures above in place of accuracy.

#### Multiple Linear Regression

In many applications, it is more general to have more than one predictor variable.

Multiple linear regression is a<mark style="background: #BBFABBA6;"> direct extension of the linear regression model</mark>. 

Let's assume that we have n predictor variables for each response variable, i.e., $(x_1,x_2,...,x_n, y)$.

In the multiple linear regression model, the relation between a response variable and predictor variables are modelled as:

$\hat{y} = w_0 + w_1\times x_1 + w_2 \times x_2 ... + w_n \times x_n$

Again, parameters $w_0,.., w_n$, with  $w_0$ is the bias and $w_1,.., w_n$ are the weights, can be estimated in least square fashion:

$E = \sum_{i=1}^{|D|} (y_i - \hat{y}_i)^2$

A <mark style="background: #FFB86CA6;">least square method will find optimal parameters</mark> to minimise the squared error $E$. Again, a loss function other than $E$ could be used instead to optimise parameters.

##### Significance of the weight

Most regression libraries or programs support significance testing for each attribute. Usually, significant attributes are denoted by *** (or ** or *, depending on the significance level). Note that the numerical value of the corresponding weight is not an indication of the significance of the attribute, as it is driven by the range of values of the attribute.

The <mark style="background: #FFB8EBA6;">significant attributes indicate that the effect (or influence) of the attribute is not just random</mark>! In other words, those significant attributes have high influence on predicting your response variable. This alone may be valuable information, even if you discard the linear model itself.

##### Performance evaluation

The methods for evaluation of simple linear regression apply straightforwardly to the multiple variable case.


Examples:

```python
# import and prepare
import pandas as pd
import numpy as np

data=pd.read_csv('example.csv')
from sklearn.linear_model import LinearRegression

# create linear regression model
lr_model = LinearRegression()

# set training set
x = data['x'].values.reshape(-1, 1)
y = data['y'].values.reshape(-1, 1)

# train 
lr_model.fit(x,y)

# y_predict
y_predi = lr_model.predict(x)
```


