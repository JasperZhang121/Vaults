
SVMs are one of the most successful classification methods for both linear and nonlinear data. It uses a nonlinear mapping to transform the original training data into a higher dimension. With the new dimension, it searches for the linear optimal separating hyperplane (i.e., “decision boundary”) and with an appropriate nonlinear mapping to a sufficiently high dimension, data from two classes can always be separated by a hyperplane. SVM finds this hyperplane using support vectors (“essential” training tuples) and margins (defined by the support vectors).

**History and Applications**

Vapnik and colleagues introduced the Support Vector Machine in 1992. The groundwork for SVM was laid by Vapnik & Chervonenkis’ statistical learning theory in the 1960s. SVM's features include slow training but high accuracy owing to their ability to model complex nonlinear decision boundaries (margin maximisation). SVMs can be used for classification and numeric prediction (regression). Some applications of SVM include handwritten digit recognition, object recognition, speaker identification, and benchmarking time-series prediction tests.

**Algorithms**

Training of SVM can be distinguished by:

-   Linearly separable case
-   Non-linearly separable case

---

### Linearly Separable Case

Linearly separable case refers to the scenario where <mark style="background: #FFB86CA6;">two classes of data can be completely separated by a straight line or hyperplane in the feature space</mark>. That is, there exists a hyperplane that can perfectly separate the two classes of data with a margin, which is the distance between the hyperplane and the closest data points from each class.

![[lssvm.png]]

In the context of Support Vector Machines (SVMs), the linearly separable case is a scenario where the SVM can effectively learn the decision boundary separating the two classes by <mark style="background: #ADCCFFA6;">maximizing the margin between them</mark>.

SVMs work by identifying the optimal hyperplane that separates the two classes in the training data with the largest margin possible. In the linearly separable case, the SVM algorithm can find a unique hyperplane that can perfectly separate the two classes of data.

### Hyperplanes and support vectors:

Two-dimensional training tuple case:
In two-dimensional space ($A_1-A_2$ plane), a hyperplane corresponds to a line, and every hyperplane can be written as:
$$A_2 = a \times A_1 + b$$
For a more general representation, if we replace $A_1$ and $A_2$ by  $x_1$ and $x_2$, then the above hyperplane can be rewritten as:
$$0 = w_1 \times x_1 + w_2 \times x_2 + w_0,$$
where $w_1 = a$, $w_2 = -1$, $w_0 = b$.

We can represent any hyperplane (line) in two-dimensional space with $w_1$, $w_2$, and $w_0$.

In the linearly separable case, every training tuple satisfies the following condition:
H1 (positive class):
If $w_1 \times x_1 + w_2 \times x_2 + w_0 \geq +1$
H2 (negative class):
If $w_1 \times x_1 + w_2 \times x_2 + w_0 \leq -1$

Support vector:
Therefore, every training tuple that satisfies $w_1 \times x_1 + w_2 \times x_2 + w_0 = \pm 1$ is a support vector.

N-dimensional training tuple case:
Let $X = (x_1, x_2, x_3, \dots, x_n)$ be a training tuple with class label $y_i$. Then a separating hyperplane can be written as:
$$W X^\top + w_0 = 0$$
where $W=\{w_1, w_2, \dots, w_n\}$ is a weight vector and $w_0$ is a scalar (bias).
$$W X^\top =W \cdot X = \sum_{i=1}^{n} w_i \times x_i$$
The hyperplane defining the sides of the margin:
H1: $w_0 + W X^\top\geq 1$ for $y_i = +1$, and
H2: $w_0 + W X^\top\leq -1$ for $y_i = -1$
These two equations can be combined into one equation:
$$y_i (w_0 + W X^\top) \geq 1 \quad \forall i$$
This equation can be solved as a constrained (convex) quadratic optimization problem that maximizes the margins to estimate the weights $W$ from the training set, and is the SVM version of training the model.

Classify test tuple using trained model:
During the testing phase, the trained model classifies a new tuple $X$ using the rules:

Using hyperplane:
H1 (positive class):
If $w_0 + W X^\top \geq 0$
Then $X$ will be classified as a positive class.
H2 (negative class):
If $w_0 + W X^\top < 0$
Then $X$ will be classified as a negative class.

Alternatively, we can use the support vectors $X_i$, each with class label $y_i$, to classify test tuples. For test tuple $X$, 
$$d(X) = \sum_{i=1}^{\ell} y_i \alpha_i X_i X^\top +b_0$$
where $\ell$ is the number of support vectors, and $\alpha$ and $b_0$ are automatically determined by the optimization/training algorithm.

If the sign of $d(\boldsymbol{X})$ is positive then $\boldsymbol{X}$ is classified as $H_1$, otherwise $H_2$.

Note that we need to keep only the support vectors for testing. This fact will be used in the non-linearly separable case.

#### Support vector machines (SVMs) are effective on high-dimensional data for several reasons:

1.  Complexity: The complexity of a trained SVM classifier is characterized by the <mark style="background: #FFB8EBA6;">number of support vectors rather than the dimensionality of the data</mark>. Support vectors are the essential or critical training examples that lie closest to the decision boundary (i.e., the maximum margin hyperplane). If all other training examples are removed and the training is repeated, the same separating hyperplane would be found from the support vectors alone.
    
2.  Generalization: The number of support vectors found can be used to compute an upper bound on the expected error rate of the SVM classifier, which is <mark style="background: #FFF3A3A6;">independent of the data dimensionality</mark>. Thus, an SVM with<mark style="background: #ABF7F7A6;"> a small number of support vectors can have good generalization</mark>, even when the dimensionality of the data is high.
    
3.  Non-linear mapping: SVMs are effective on high-dimensional data because they use a non-linear mapping to transform the original training data into a higher dimension. With an appropriate non-linear mapping to a sufficiently high dimension, data from two classes can always be separated by a hyperplane.
    
4.  Margin maximization: SVMs are designed to <mark style="background: #FF5582A6;">maximize the margin</mark> between the classes. This helps to prevent overfitting and allows the SVM to generalize well to new data.

### Linearly Inseparable Case

In some cases, the data may not be linearly separable, meaning it's not possible to draw a hyperplane that perfectly separates the two classes. In this case, we need to allow for some misclassification errors in order to find a separating hyperplane. This is done by introducing a slack variable ξ_i for each training example, which allows it to be on the wrong side of the hyperplane with a penalty.

![[dataset_nonsep.png]]

The objective of the SVM is to <mark style="background: #FFB8EBA6;">minimize the sum of the slack variables</mark>, subject to the constraint that all training examples are classified correctly (or as correctly as possible) by the hyperplane. This leads to a modified optimization problem, where the goal is to minimize the following equation: 
$C\sum_{i=1}^n \xi_i + \frac{1}{2}\left|w\right|^2 subject to y_i(w^\top x_i + b) \geq 1 - \xi_i for i = 1, ..., n \xi_i \geq 0 for i = 1, ..., n$

Here, C is a hyperparameter that determines the <mark style="background: #BBFABBA6;">trade-off between minimizing the sum of the slack variables and maximizing the margin</mark>. A larger value of C results in a more complex model that tries to minimize misclassification errors as much as possible. The solution to this optimization problem leads to a hyperplane that may still misclassify some training examples, but does so with the smallest possible sum of slack variables.

In practice, we can use a <mark style="background: #ADCCFFA6;">kernel function</mark> to map the input data into a higher-dimensional space, where it may become separable. The kernel function allows us to perform the same optimization on the transformed data, without explicitly computing the transformation. In this case, the hyperplane we find is in the higher-dimensional space, but can be projected back to the original space using the kernel function.

**Kernel Trick**

The idea is to obtain linear separation by **mapping the data to a higher dimensional space**. Let's see the example first:

![[data_2d_to_3d.png]]

- Left: A dataset in $\mathbb{R}^2$, not linearly separable. 
- Right: The same dataset transformed by the transformation: $[x_1, x_2] = [x_1, x_2, x_1^2 + x_2^2]$

![[data_2d_to_3d_hyperplane.png]]

Hyperplane (green plane) that linearly separates two classes in the higher dimensional space.

In the above example, we can train a linear SVM classifier that successfully finds a good decision boundary in $\mathbb{R}$.

However, we are given the dataset in $\mathbb{R}^2$. The challenge is to find a <mark style="background: #FFB8EBA6;">transformation</mark> $\phi$: $\mathbb{R}^2 \rightarrow \mathbb{R}^3$, such that the transformed dataset is linearly separable in $\mathbb{R}^3$. For example, consider the transformation function that maps a 2-dimensional space to a 3-dimensional space: $\phi([x_1, x_2]) = [x_1, x_2, x_1^2 + x_2^2]$, which,  after applied to every point in the original tuples yields the linearly separable dataset in our example.

#### Kernels

The decision rule used in the linearly separable case, for test vector  X  was
$d(X) = \sum_{i=1}^{\ell} y_i \alpha_i X_i X^\top +b_0$
Now it is converted, in the higher-dimensional space to
$d(X) = \sum_{i=1}^{\ell} y_i \alpha_i \phi(X_i) \cdot \phi(X)^\top +b_0$
But all these dot-products over potentially very high-dimensional vectors are expensive.
We define a kernel function $K(.,.)$ on the data in the original lower-dimensional space:
Let $X_i$, $X_j$ be vectors. Then $K(X_i, X_j) = \phi(X_i) \cdot \phi(X_j)^\top$
Now,  every $\phi(X_i) \cdot \phi(X_j)^\top$ can be replaced by  K(X_i, X_j) in the training algorithm and decision rule.  We do not need to make calculations over the higher-dimensional transformed vectors.  We do not even need a formulation of $\phi$, as a kernel function is enough for determining the SVM. But we do need the kernel function to have certain properties,  so that not just any function will do.
Here are some widely used kernel functions. They do result in different classifiers, although they are commonly of similar accuracy and the only way to find the best one is to try it.

Polynomial kernel of degree h: $K(X_i, X_j) = (X_i \cdot X_j^\top + 1)^h$
Gaussian radial basis function kernel: $K(X_i, X_j) = e^{-dist(X_i, X_j)^2 / 2\sigma^2}$
Sigmoid Kernel: $K(X_i, X_j) = tanh(\kappa X_i \cdot X_j^\top - \delta)$
Linear Kernel: $K(X_i, X_j) = X_i \cdot X_j^\top$


### Comparison between neural network and SVM

Decision hyperplanes found by non-linear SVM are similar to those found by neural network classifiers. e.g. SVM with Gaussian radial basis function is the same as a radial basis function neural network. e.g. A sigmoid kernel SVM is the same as a two-layer neural network (i.e with no hidden layers).

|   | Neural Network | Support Vector Machine (SVM) |
| - | - | - |
| Model Type | Non-linear | Linear or Non-linear |
| Training time | Slow for large datasets | Fast for large datasets |
| Interpretability | Less interpretable | More interpretable |
| Regularization | Weight decay, early stopping | C and γ parameters |
| Complexity | Can handle complex relationships between variables | Simpler relationships between variables |
| Overfitting | Prone to overfitting | Less prone to overfitting |
| Hyperparameters | Many hyperparameters | Few hyperparameters |
| Scalability | Less scalable | More scalable |

#### Extra
- Neural Network: 
	- Nondeterministic algorithm which finds local minima  

	- Generalises well but doesn't have strong mathematical foundation

	- Can easily be learned in incremental fashion

	- To learn complex functions, use a  multi-layer neural network, but adding more layers adds more training time  

	- Gets more complex with higher dimensions and can easily overfit.

- SVM
	- Deterministic algorithm that finds the global minimum  

	- Nice generalisation property. Tends not to overfit.  

	- Hard to learn - learned in batch mode using quadratic programming techniques

	- Using kernels can learn very complex functions

	- Well suited to high-dimensional data

---
#### Code: 

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm

# Create the dataset
X = np.array([[1, 2], [2, 3], [3, 1], [4, 3], [5, 2], [6, 1]])
y = np.array([1, 1, 1, -1, -1, -1])

# Create and fit the SVM model
model = svm.SVC(kernel='linear')
model.fit(X, y)

# Plot the decision boundary
w = model.coef_[0]
b = model.intercept_[0]
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),
                     np.arange(y_min, y_max, 0.01))
Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
plt.contourf(xx, yy, Z, alpha=0.8)
plt.scatter(X[:, 0], X[:, 1], c=y)
plt.xlabel('X1')
plt.ylabel('X2')
plt.title('SVM Classifier')
plt.show()
```
