
In [statistics](https://en.wikipedia.org/wiki/Statistics "Statistics"), **naive Bayes classifiers** are a family of simple "[probabilistic classifiers](https://en.wikipedia.org/wiki/Probabilistic_classification "Probabilistic classification")" based on applying [Bayes' theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem "Bayes' theorem") with strong (naive) [independence](https://en.wikipedia.org/wiki/Statistical_independence "Statistical independence") assumptions between the features (see [Bayes classifier](https://en.wikipedia.org/wiki/Bayes_classifier "Bayes classifier")). They are among the simplest [Bayesian network](https://en.wikipedia.org/wiki/Bayesian_network "Bayesian network") models, but coupled with [kernel density estimation](https://en.wikipedia.org/wiki/Kernel_density_estimation "Kernel density estimation"), they can achieve high accuracy levels.

---

The Naive Bayesian Classifier assumes that the features used in the classification task are independent of each other, which is a strong assumption and is not always true in practice. However, despite this limitation, the Naive Bayesian Classifier is still widely used due to its simplicity and good performance on many tasks.


### Architecture

The Naive Bayesian Classifier works by calculating the probability of a class given a set of features. It does this by <mark style="background: #FFB8EBA6;">calculating the prior probability of each class</mark>, which is the probability of each class occurring in the dataset, and the likelihood of each feature given each class, which is the probability of each feature occurring in each class.

Using Bayes theorem, the Naive Bayesian Classifier calculates the posterior probability of each class given the features, and selects the class with the highest probability as the predicted class.


### Training

The Naive Bayesian Classifier is trained using a labeled dataset, where each instance is labeled with a class. The algorithm calculates the prior probability and likelihood of each feature given each class, based on the frequency of occurrence of each feature in each class.


### Applications

The Naive Bayesian Classifier is commonly used in text classification tasks, such as spam filtering and sentiment analysis. It can also be used in other classification tasks where the features are assumed to be independent, such as medical diagnosis and fraud detection.


### Limitations

The Naive Bayesian Classifier's <mark style="background: #ADCCFFA6;">assumption of feature independence</mark> is a strong limitation and can lead to inaccurate predictions if this assumption is not met. In addition, the algorithm requires a large amount of training data to accurately estimate the probabilities, and may suffer from overfitting if the dataset is too small.


### Formula

The Naive Bayes classifier makes a naive assumption that the input features are conditionally independent given the class, which allows us to simplify the likelihood term as:

$$P(x|c) = P(x1|c) * P(x2|c) * ... * P(xn|c)$$

where:

-   xi is the ith input feature.

Therefore, the Naive Bayes classifier can be written as:

$$P(c|x) = P(c) * P(x1|c) * P(x2|c) * ... * P(xn|c) / P(x)$$

### Code

```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
X, y = load_iris(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)
gnb = GaussianNB()
y_pred = gnb.fit(X_train, y_train).predict(X_test)
```

