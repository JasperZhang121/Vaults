
The Naive Bayes Classifier is a probabilistic machine learning algorithm that is commonly used for classification tasks. It is based on Bayes' theorem and assumes that the features are conditionally independent given the class. Despite its simplicity, Naive Bayes can be very effective in various domains, especially with high-dimensional data.

The Naive Bayesian Classifier assumes that <mark style="background: #ABF7F7A6;">the features used in the classification task are independent of each other</mark>, which is a strong assumption and is not always true in practice. However, despite this limitation, the Naive Bayesian Classifier is still widely used due to its simplicity and good performance on many tasks.


### Architecture

The Naive Bayesian Classifier works by calculating the probability of a class given a set of features. It does this by <mark style="background: #FFB8EBA6;">calculating the prior probability of each class</mark>, which is the probability of each class occurring in the dataset, and the likelihood of each feature given each class, which is the probability of each feature occurring in each class.

Using Bayes theorem, the Naive Bayesian Classifier calculates the posterior probability of each class given the features, and selects the class with the highest probability as the predicted class.


### Training

The Naive Bayesian Classifier is trained using a labeled dataset, where each instance is labeled with a class. The algorithm calculates the prior probability and likelihood of each feature given each class, based on the frequency of occurrence of each feature in each class.


### Applications

The Naive Bayesian Classifier is commonly used in text classification tasks, such as spam filtering and sentiment analysis. It can also be used in other classification tasks where the features are assumed to be independent, such as medical diagnosis and fraud detection.


### Limitations

The Naive Bayesian Classifier's <mark style="background: #ADCCFFA6;">assumption of feature independence</mark> is a strong limitation and can lead to inaccurate predictions if this assumption is not met. In addition, the algorithm requires a large amount of training data to accurately estimate the probabilities, and may suffer from overfitting if the dataset is too small.


### Formula

The Naive Bayes classifier makes a naive assumption that the input features are conditionally independent given the class, which allows us to simplify the likelihood term as:

$$P(x|c) = P(x1|c) * P(x2|c) * ... * P(xn|c)$$

where:

-   xi is the ith input feature.

Therefore, the Naive Bayes classifier can be written as:

$$P(c|x) = P(c) * P(x1|c) * P(x2|c) * ... * P(xn|c) / P(x)$$

----

### Steps:

1.  **Training Phase**:
    
    -   Collect and preprocess the training data: Gather a labeled dataset where each instance is associated with a known class label.
    
    -   Estimate class probabilities: Calculate the prior probability of each class based on the frequency or proportion of instances in the training data. The prior probability of class C is denoted as P(C).
    
    -   Estimate feature probabilities: For each feature in the dataset, calculate the likelihood of observing a particular value given each class. This is done by counting the occurrences of each feature value within each class. The likelihood of feature X taking on value V given class C is denoted as P(X=V|C).
    
		**Formulas:**
	-   Prior Probability: P(C) = <mark style="background: #FFB8EBA6;">Number of instances in class C / Total number of instances</mark>
	-   Likelihood: P(X=V|C) = <mark style="background: #ADCCFFA6;">Number of instances in class C with feature X = V / Number of instances in class C</mark>

2.  **Prediction Phase**:
    
    -   Collect and preprocess the new data: Obtain the new, unlabeled data for which you want to make predictions. Preprocess the data to match the format of the training data.
        
    -   Calculate the conditional probabilities: For each class, calculate the conditional probability of observing the given features. This is done by multiplying the likelihoods of the observed feature values for that class. The conditional probability of class C given features X is denoted as P(C|X).
        
    -   Apply Bayes' theorem: Use Bayes' theorem to calculate the posterior probability of each class given the observed features. Bayes' theorem states that the posterior probability is proportional to the product of the prior probability and the conditional probability:
        
        P(C|X) = (P(X|C) * P(C)) / P(X)
        
    -   Select the predicted class: Choose the class with the highest posterior probability as the predicted class for the new instance.


### Code Example

```python
import numpy as np
from sklearn.naive_bayes import GaussianNB

# Create the feature matrix
X = np.array([[2, 3], [3, 4], [4, 5], [2, 1], [3, 2], [4, 3]])
# Create the target labels
y = np.array(['Spam', 'Spam', 'Not Spam', 'Spam', 'Not Spam', 'Not Spam'])

# Create a Naive Bayes Classifier
clf = GaussianNB()

# Fit the model with the training data
clf.fit(X, y)

# Create a new observation to predict its class
new_observation = np.array([[3, 3]])

# Calculate the posterior probabilities for the new observation
posterior_probs = clf.predict_proba(new_observation)

# Display the posterior probabilities
print("Posterior Probabilities:")
for i, class_label in enumerate(clf.classes_):
    print(f"{class_label}: {posterior_probs[0][i]}")
```


### Written Example:

| Email   | K1  | K2  | L      | Class    |
|---------|-----|-----|--------|----------|
| Email 1 | Yes | No  | Short  | Spam     |
| Email 2 | No  | Yes | Long   | Not Spam |
| Email 3 | Yes | Yes | Medium | Not Spam |
| Email 4 | No  | No  | Long   | Spam     |


Build a Bayesian Classifier to predict the class (spam or not spam) for a new email with features (K1 = Yes, K2 = No, L = Medium).

**Training Phase**:

1.  Estimate class probabilities:
    
    -   P(Spam) = 2/4 = 0.5
    -   P(Not Spam) = 2/4 = 0.5
2.  Estimate feature probabilities:
    
    -   P(K1 = Yes | Spam) = 1/2 = 0.5
    -   P(K1 = Yes | Not Spam) = 1/2 = 0.5
    -   P(K2 = No | Spam) = 1/2 = 0.5
    -   P(K2 = No | Not Spam) = 1/2 = 0.5
    -   P(L = Medium | Spam) = 0/2 = 0
    -   P(L = Medium | Not Spam) = 1/2 = 0.5

**Prediction Phase**:

For the new email with features (K1 = Yes, K2 = No, L = Medium):

1.  Calculate the <mark style="background: #FFB86CA6;">conditional probabilities</mark> for each class:
    
    -   P(Spam | K1 = Yes, K2 = No, L = Medium) = (P(K1 = Yes | Spam) * P(K2 = No | Spam) * P(L = Medium | Spam) * P(Spam)) / P(X)
    
    -   P(Not Spam | K1 = Yes, K2 = No, L = Medium) = (P(K1 = Yes | Not Spam) * P(K2 = No | Not Spam) * P(L = Medium | Not Spam) * P(Not Spam)) / P(X)
    
1.  Apply Bayes' theorem:
    
    -   Calculate the posterior probabilities for each class by dividing the conditional probabilities by P(X):
        
        P(Spam | X) ∝ P(Spam) * P(K1 = Yes | Spam) * P(K2 = No | Spam) * P(L = Medium | Spam) P(Not Spam | X) ∝ P(Not Spam) * P(K1 = Yes | Not Spam) * P(K2 = No | Not Spam) * P(L = Medium | Not Spam)
        
3.  Select the predicted class:
    
    -   Choose the class with the highest posterior probability. In this case, if P(Spam | X) > P(Not Spam | X), classify the email as spam; otherwise, classify it as not spam.