
Logistic regression is a popular statistical model used for binary classification problems. It is commonly used when the dependent variable is categorical, taking on two possible outcomes (e.g., yes/no, true/false, 0/1). Logistic regression allows us to predict the probability of an observation belonging to a specific class based on one or more independent variables.

### Key Concepts

- **Logistic Function (Sigmoid)**: Logistic regression uses the <mark style="background: #FFB8EBA6;">logistic function (also known as the sigmoid function) to model the relationship between the independent variables and the probability of the outcome</mark>. The logistic function maps any real-valued number to a value between 0 and 1, representing the probability.

- **Logit Transformation**: The logit transformation is the inverse of the logistic function. It transforms the probability of the outcome to a linear function of the independent variables. The logit transformation allows us to model the relationship between the independent variables and the log-odds (logarithm of the odds) of the outcome.

- **Maximum Likelihood Estimation (MLE)**: Logistic regression estimates the model parameters using the maximum likelihood estimation method. The goal is to find the parameter values that maximize the likelihood of observing the given data. The MLE method calculates the probabilities of observing the actual outcomes and adjusts the parameter values to increase the likelihood of the observed data. [[1. Maximum Likelihood Estimation (MLE)]]

- **Odds Ratio**: In logistic regression, the odds ratio is used to interpret the effect of independent variables on the probability of the outcome. It represents the change in odds for a one-unit increase in the independent variable. An odds ratio greater than 1 indicates a positive effect, while an odds ratio less than 1 indicates a negative effect.

### Formula

1. Logistic Function (Sigmoid):

$$\sigma(z) = \frac{1}{1 + e^{-z}}$$


2. Logit Transformation:

$${logit}(p) = \ln\left(\frac{p}{1-p}\right)$$
$$\ln\left(\frac{P}{1-P}\right) = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_nX_n$$
$$P = \frac{1}{1 + e^{-(\beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_nX_n)}}$$
3. Logistic Regression Hypothesis:

$$h_\theta(x) = \sigma(\theta^Tx) = \frac{1}{1 + e^{-\theta^Tx}}$$


4. Cost Function (Negative Log-Likelihood):

$$J(\theta) = -\frac{1}{m}\sum_{i=1}^{m}\left[y^{(i)}\log(h_\theta(x^{(i)})) + (1-y^{(i)})\log(1 - h_\theta(x^{(i)}))\right]$$


5. Gradient Descent Update Rule:


$$\theta_j := \theta_j - \alpha \frac{\partial J(\theta)}{\partial \theta_j}$$

where:
- $\theta_j$ represents the j-th parameter of the model.
- $\alpha$ is the learning rate, determining the step size of the updates.
- $J(\theta)$ is the cost function.
- $\frac{\partial J(\theta)}{\partial \theta_j}$ denotes the partial derivative of the cost function with respect to $\theta_j$.


The cost function for logistic regression is given by: 

$$ J(\theta) = -\frac{1}{m}\sum_{i=1}^{m}\left[y^{(i)}\log(h_\theta(x^{(i)})) + (1-y^{(i)})\log(1 - h_\theta(x^{(i)}))\right] $$

To perform the gradient descent update, we need to calculate the partial derivative of the cost function with respect to each parameter $\theta_j$. Let's start with the derivative term:

$$\frac{\partial J(\theta)}{\partial \theta_j} = \frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}$$

where: 
- $h_\theta(x^{(i)})$ represents the predicted probability of the positive class for the i-th training example. 
- $y^{(i)}$ represents the true label of the i-th training example. 
- $x_j^{(i)}$ represents the j-th feature of the i-th training example.


6. Partial Derivative of the Cost Function:

$$\frac{\partial J(\theta)}{\partial \theta_j} = \frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}$$

7. Odds Ratio:

$${Odds Ratio} = e^{\beta_j}$$


8. Maximum Likelihood Estimation (MLE):

$$\text{MLE}(\theta) = \prod_{i=1}^{m}h_\theta(x^{(i)})^{y^{(i)}}(1 - h_\theta(x^{(i)}))^{1 - y^{(i)}}$$


### Model Training and Evaluation

1. **Data Preparation**: Preprocess the data by encoding categorical variables, handling missing values, and scaling the features if necessary.

2. **Model Creation**: Create a logistic regression model using appropriate libraries such as scikit-learn. Specify any hyperparameters and regularization settings.

3. **Train the Model**: Fit the logistic regression model to the training data using the `fit()` method. The model will learn the optimal parameter values that maximize the likelihood of the data.

4. **Model Evaluation**: Evaluate the performance of the trained model using evaluation metrics such as accuracy, precision, recall, and F1-score. Use techniques like cross-validation to assess the model's generalization capability.

5. **Prediction**: Use the trained logistic regression model to make predictions on new, unseen data using the `predict()` or `predict_proba()` methods. The `predict()` method returns the predicted class labels, while `predict_proba()` provides the predicted probabilities for each class.

### Example Code

Example code snippet for logistic regression using scikit-learn:

```python
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load and preprocess the data
X, y = load_data()
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and train the logistic regression model
logistic_model = LogisticRegression()
logistic_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = logistic_model.predict(X_test)

# Evaluate the model's performance
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
