K-nearest Neighbors (KNN) is a popular supervised learning algorithm used for both classification and regression tasks. It is a non-parametric method that makes predictions based on the similarity of data points in the feature space. The basic idea behind KNN is to find the K nearest neighbors of a given data point and use their labels (in the case of classification) or values (in the case of regression) to make predictions.

---

In the k-nearest neighbors (k-NN) algorithm, the training examples are vectors in a multidimensional feature space, each with a class label. During the training phase, the algorithm stores the feature vectors and class labels of the training samples. In the classification phase, an unlabeled vector (a query or test point) is classified by assigning the label which is most frequent among the k training samples nearest to that query point.

The value of k is a user-defined constant, and a commonly used distance metric for continuous variables is Euclidean distance: 

$$d(\mathbf{x}_i, \mathbf{x}_j) = \sqrt{\sum_{k=1}^p (x_{i,k} - x_{j,k})^2}$$

For discrete variables, such as text classification, the overlap metric (or Hamming distance) can be used. In other contexts, such as gene expression microarray data, correlation coefficients like Pearson and Spearman can be employed as a metric for k-NN. The classification accuracy of k-NN can be improved by learning the distance metric with specialized algorithms such as Large Margin Nearest Neighbor or Neighbourhood components analysis.

However, a drawback of the basic "majority voting" classification is that it can be <mark style="background: #BBFABBA6;">skewed by class imbalance</mark>. This means that examples of a more frequent class tend to dominate the prediction of the new example because they tend to be common among the k nearest neighbors due to their large number. To overcome this problem, one solution is to weight the classification by taking into account the distance from the test point to each of its k nearest neighbors. The class (or value, in regression problems) of each of the k nearest points is multiplied by a weight proportional to the inverse of the distance from that point to the test point. Another solution is abstraction in data representation, where each node in a self-organizing map (SOM) is a representative (a center) of a cluster of similar points, regardless of their density in the original training data. k-NN can then be applied to the SOM.

The k-NN algorithm is known as a "lazy learner" because it doesn't build a model or make generalizations during training. Instead, it memorizes the training data and uses it during classification. This makes it computationally inexpensive during the training phase, but potentially slower during the classification phase as it has to compare the test point to all training points.


### Evaluate the performance 

Use various evaluation metrics depending on the problem type (classification or regression) and the specific requirements of your task. Here are some common evaluation techniques for KNN:

1.  Classification Metrics:
    
    -   Accuracy: It measures the proportion of correctly classified instances out of the total instances in the dataset. Accuracy is a commonly used metric for evaluating classification models, but it may not be sufficient in cases of imbalanced datasets.
    -   Confusion Matrix: A confusion matrix provides a more detailed analysis of the model's performance by showing the counts of true positive, true negative, false positive, and false negative predictions. From the confusion matrix, you can calculate metrics like precision, recall, and F1-score.
    -   Precision: It measures the proportion of true positive predictions out of the total predicted positive instances. Precision is useful when you want to focus on minimizing false positive predictions.
    -   Recall (Sensitivity): It measures the proportion of true positive predictions out of the actual positive instances in the dataset. Recall is useful when you want to focus on minimizing false negative predictions.
    -   F1-score: It is the harmonic mean of precision and recall, providing a balanced measure of the model's performance. F1-score combines precision and recall into a single metric.
2.  Regression Metrics:
    
    -   Mean Squared Error (MSE): It calculates the average squared difference between the predicted and actual values. MSE is commonly used for regression tasks and penalizes larger errors more heavily.
    -   Root Mean Squared Error (RMSE): It is the square root of the MSE and provides the measure of the average deviation between the predicted and actual values.
    -   Mean Absolute Error (MAE): It calculates the average absolute difference between the predicted and actual values. MAE is less sensitive to outliers compared to MSE.
3.  Cross-Validation:
    
    -   K-Fold Cross-Validation: Split the dataset into K equally sized folds, then train and evaluate the model K times, each time using a different fold as the test set and the remaining folds as the training set. This helps in getting a more reliable estimate of the model's performance by reducing the variance.


Characteristics of KNN

-  By averaging k-nearest neighbours
-  Extremely slow when classifying test tuples:
	With $|D|$ training tuples, $|D|$ comparisons are required to find k-nearest neighbourhood
	For example, SVM only requires $\ell$ comparisons where $\ell$ is the number of support vectors
- Partial distance method:
	Compute a distance on a subset of n attributes.
	If the distance exceeds a threshold, further distance computation will be halted
	Otherwise keep computing the distance on the remaining attributes


### Case-Based Reasoning

-   **CBR(Case-Based Reasoning)**: Uses a database of problem solutions to solve new problems
-   Store symbolic description (tuples or cases)â€”not points in a Euclidean space
-   Applications: Customer-service (product-related diagnosis), legal ruling
-   Methodology
    -   Instances represented by <mark style="background: #FFB8EBA6;">rich symbolic descriptions</mark>
    -   If there is an identical training case, given a test case, the solution of the training case will be returned
    -   If not, search for similar cases, multiple retrieved cases may be combined
    -   Tight coupling between case retrieval, knowledge-based reasoning, and problem solving
-   Challenges
    -   Find a good similarity metric
    -   Indexing based on syntactic similarity measure, and when failure, backtracking, and adapting to additional cases


### Genetic Algorithms

Genetic Algorithms (GAs) are a class of <mark style="background: #ABF7F7A6;">optimization algorithms</mark> that are inspired by the principles of evolution and natural selection. They are used to solve optimization problems that involve finding the best solution among a large set of possible solutions.

The main idea behind GAs is to<mark style="background: #CACFD9A6;"> mimic the process of natural selection</mark>, which involves survival of the fittest. In the case of GAs, a population of potential solutions is created, and then the solutions that are better suited to the problem at hand are identified and selected for reproduction. These solutions are then combined to create new solutions, which are evaluated and the process repeats until a satisfactory solution is found.

The basic steps involved in a GA are:

1.  Initialization: A population of potential solutions is created randomly.
2.  Evaluation: Each solution in the population is evaluated against a fitness function that measures its suitability for the problem at hand.
3.  Selection: The fittest solutions are selected for reproduction.
4.  Crossover: The selected solutions are combined to create new solutions.
5.  Mutation: The new solutions are mutated to introduce new genetic material into the population.
6.  Evaluation: The fitness function is applied to the new solutions.
7.  Termination: The algorithm stops when a satisfactory solution is found or a pre-determined number of generations have been evaluated.

GAs have been applied to a wide range of optimization problems, including engineering design, scheduling, and financial portfolio optimization. They have also been used in machine learning for feature selection, parameter tuning, and neural network training.

In summary, GAs provide a powerful optimization technique that can be applied to a wide range of problems. However, they are computationally expensive and require careful tuning of the parameters to achieve good results.


----
Example

```python
from sklearn.neighbors import KNeighborsClassifier

# load dataset
X = [[0, 0], [0, 1], [1, 0], [1, 1]]
y = [0, 1, 1, 0]

# create KNN classifier
knn = KNeighborsClassifier(n_neighbors=3)

# fit the model using the training data
knn.fit(X, y)

# predict the class of new data point
X_new = [[0.5, 0.5]]
y_pred = knn.predict(X_new)

print("Predicted class label for the new data point: ", y_pred)
```