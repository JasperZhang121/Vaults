In [statistics](https://en.wikipedia.org/wiki/Statistics "Statistics"), the **_k_-nearest neighbors algorithm** (**_k_-NN**) is a [non-parametric](https://en.wikipedia.org/wiki/Non-parametric_statistics "Non-parametric statistics") [supervised learning](https://en.wikipedia.org/wiki/Supervised_learning "Supervised learning") method first developed by [Evelyn Fix](https://en.wikipedia.org/wiki/Evelyn_Fix "Evelyn Fix") and [Joseph Hodges](https://en.wikipedia.org/wiki/Joseph_Lawson_Hodges_Jr. "Joseph Lawson Hodges Jr.") in 1951, and later expanded by [Thomas Cover](https://en.wikipedia.org/wiki/Thomas_M._Cover "Thomas M. Cover"). It is used for [classification](https://en.wikipedia.org/wiki/Statistical_classification "Statistical classification") and [regression](https://en.wikipedia.org/wiki/Regression_analysis "Regression analysis"). In both cases, the input consists of the _k_ closest training examples in a [data set](https://en.wikipedia.org/wiki/Data_set "Data set").

---

In the k-nearest neighbors (k-NN) algorithm, the training examples are vectors in a multidimensional feature space, each with a class label. During the training phase, the algorithm stores the feature vectors and class labels of the training samples. In the classification phase, an unlabeled vector (a query or test point) is classified by assigning the label which is most frequent among the k training samples nearest to that query point.

The value of k is a user-defined constant, and a commonly used distance metric for continuous variables is Euclidean distance: 

$$d(\mathbf{x}_i, \mathbf{x}_j) = \sqrt{\sum_{k=1}^p (x_{i,k} - x_{j,k})^2}$$

For discrete variables, such as text classification, the overlap metric (or Hamming distance) can be used. In other contexts, such as gene expression microarray data, correlation coefficients like Pearson and Spearman can be employed as a metric for k-NN. The classification accuracy of k-NN can be improved by learning the distance metric with specialized algorithms such as Large Margin Nearest Neighbor or Neighbourhood components analysis.

However, a drawback of the basic "majority voting" classification is that it can be <mark style="background: #BBFABBA6;">skewed by class imbalance</mark>. This means that examples of a more frequent class tend to dominate the prediction of the new example because they tend to be common among the k nearest neighbors due to their large number. To overcome this problem, one solution is to weight the classification by taking into account the distance from the test point to each of its k nearest neighbors. The class (or value, in regression problems) of each of the k nearest points is multiplied by a weight proportional to the inverse of the distance from that point to the test point. Another solution is abstraction in data representation, where each node in a self-organizing map (SOM) is a representative (a center) of a cluster of similar points, regardless of their density in the original training data. k-NN can then be applied to the SOM.

The k-NN algorithm is known as a "lazy learner" because it doesn't build a model or make generalizations during training. Instead, it memorizes the training data and uses it during classification. This makes it computationally inexpensive during the training phase, but potentially slower during the classification phase as it has to compare the test point to all training points.


Characteristics of KNN

-  By averaging k-nearest neighbours
-  Extremely slow when classifying test tuples:
	With $|D|$ training tuples, $|D|$ comparisons are required to find k-nearest neighbourhood
	For example, SVM only requires $\ell$ comparisons where $\ell$ is the number of support vectors
- Partial distance method:
	Compute a distance on a subset of n attributes.
	If the distance exceeds a threshold, further distance computation will be halted
	Otherwise keep computing the distance on the remaining attributes

----
Example

```python
from sklearn.neighbors import KNeighborsClassifier

# load dataset
X = [[0, 0], [0, 1], [1, 0], [1, 1]]
y = [0, 1, 1, 0]

# create KNN classifier
knn = KNeighborsClassifier(n_neighbors=3)

# fit the model using the training data
knn.fit(X, y)

# predict the class of new data point
X_new = [[0.5, 0.5]]
y_pred = knn.predict(X_new)

print("Predicted class label for the new data point: ", y_pred)
```

### Case-Based Reasoning

-   **CBR(Case-Based Reasoning)**: Uses a database of problem solutions to solve new problems
-   Store symbolic description (tuples or cases)—not points in a Euclidean space
-   Applications: Customer-service (product-related diagnosis), legal ruling
-   Methodology
    -   Instances represented by <mark style="background: #FFB8EBA6;">rich symbolic descriptions</mark>
    -   If there is an identical training case, given a test case, the solution of the training case will be returned
    -   If not, search for similar cases, multiple retrieved cases may be combined
    -   Tight coupling between case retrieval, knowledge-based reasoning, and problem solving
-   Challenges
    -   Find a good similarity metric
    -   Indexing based on syntactic similarity measure, and when failure, backtracking, and adapting to additional cases


### Genetic Algorithms

Genetic Algorithms (GAs) are a class of <mark style="background: #ABF7F7A6;">optimization algorithms</mark> that are inspired by the principles of evolution and natural selection. They are used to solve optimization problems that involve finding the best solution among a large set of possible solutions.

The main idea behind GAs is to<mark style="background: #CACFD9A6;"> mimic the process of natural selection</mark>, which involves survival of the fittest. In the case of GAs, a population of potential solutions is created, and then the solutions that are better suited to the problem at hand are identified and selected for reproduction. These solutions are then combined to create new solutions, which are evaluated and the process repeats until a satisfactory solution is found.

The basic steps involved in a GA are:

1.  Initialization: A population of potential solutions is created randomly.
2.  Evaluation: Each solution in the population is evaluated against a fitness function that measures its suitability for the problem at hand.
3.  Selection: The fittest solutions are selected for reproduction.
4.  Crossover: The selected solutions are combined to create new solutions.
5.  Mutation: The new solutions are mutated to introduce new genetic material into the population.
6.  Evaluation: The fitness function is applied to the new solutions.
7.  Termination: The algorithm stops when a satisfactory solution is found or a pre-determined number of generations have been evaluated.

GAs have been applied to a wide range of optimization problems, including engineering design, scheduling, and financial portfolio optimization. They have also been used in machine learning for feature selection, parameter tuning, and neural network training.

In summary, GAs provide a powerful optimization technique that can be applied to a wide range of problems. However, they are computationally expensive and require careful tuning of the parameters to achieve good results.