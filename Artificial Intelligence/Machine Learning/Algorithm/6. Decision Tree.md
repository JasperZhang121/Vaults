	A decision tree is a tree-like model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is commonly used in operations research, specifically in decision analysis, to help identify a strategy most likely to reach a goal.

A decision tree consists of nodes and branches. Each internal node represents a test on an attribute, each branch represents the outcome of the test, and each leaf node represents a class label. The paths from the root to the leaf represent classification rules based on the attributes, and each leaf represents the class label of a data point.

The general algorithm for decision tree induction is as follows:

1.  Select the best attribute to split the dataset based on some measure of impurity, such as entropy or Gini index.
    
2.  Split the dataset into subsets based on the selected attribute.
    
3.  Recursively apply steps 1 and 2 to each subset until all instances in a subset belong to the same class or have the same attribute values.

![[structure-of-a-decision-tree.png]]


Decision trees can be used for both classification and regression problems. In classification problems, the class label is a categorical variable, and in regression problems, the target variable is a continuous variable.

Decision trees have several advantages, such as being easy to interpret and visualize, handling both continuous and categorical variables, and handling missing values. However, they also have some limitations, such as being prone to overfitting and being sensitive to the training data.

The following are some commonly used formulas in decision tree algorithms:

-   **Entropy**: a measure of impurity in a set of examples.
$$Entropy = -\sum_{i=1}^n p_i log_2(p_i)$$
-   where $X$ is a set of examples, $n$ is the number of classes, and $p_i$ is the proportion of examples in class $i$.

-   **Information gain**: the expected reduction in entropy by splitting a set of examples based on an attribute.

$$Gini Index = \sum_{i=1}^n p_i (1-p_i)$$
where $X$ is a set of examples, $A$ is an attribute, $\mathrm{Values}(A)$ is the set of possible values of attribute $A$, $X_v$ is the subset of examples in $X$ with value $v$ for attribute $A$, and $|X_v|$ and $|X|$ are the number of examples in $X_v$ and $X$, respectively.

- **Gini index**: a measure of impurity in a set of examples, similar to entropy.

$$Gini Index = \sum_{i=1}^n p_i (1-p_i)$$
where $X$ is a set of examples, $n$ is the number of classes, and $p_i$ is the proportion of examples in class $i$.



