
Decision Trees are <mark style="background: #FF5582A6;">non-parametric</mark> supervised machine learning algorithms used for both classification and regression tasks. A decision tree is built through a process called recursive partitioning, where the data is repeatedly split into subsets based on the values of one of its features, such that the resulting subsets are as homogeneous as possible in terms of the target variable.

### Architecture

A decision tree consists of a root node, which represents the entire dataset, and a series of decision nodes and leaf nodes that represent the subsets of the data. At each decision node, the <mark style="background: #FFF3A3A6;">algorithm chooses a feature to split the data </mark>and assigns a threshold value to that feature. The data is then partitioned into subsets based on the value of the feature relative to the threshold. This process is repeated r<mark style="background: #D2B3FFA6;">ecursively until each subset is homogeneous, or a predefined stopping criterion is met</mark>.

### Training

The goal of training a decision tree is to create a tree that can accurately predict the target variable. The training process involves recursively splitting the dataset into smaller subsets, selecting the best feature to split on at each node, and continuing this process until each leaf node is pure or until a stopping criterion is met.

### Applications

Decision trees are widely used in various fields such as finance, marketing, healthcare, and engineering for classification and prediction tasks. Some examples of applications of decision trees include customer segmentation, fraud detection, disease diagnosis, and fault detection in industrial processes.

### Limitations

One of the main limitations of decision trees is that they are prone to overfitting, especially when the tree is deep or when there is noise in the data. <mark style="background: #BBFABBA6;">Overfitting occurs when the model is too complex and captures noise in the data rather than the underlying patterns.</mark> Additionally, decision trees are sensitive to the order of the features and may produce different trees depending on the order in which the features are considered. Finally, decision trees can be biased towards features with a large number of categories or values, which can lead to an unbalanced tree.

The general algorithm for decision tree induction is as follows:

1.  Select the best attribute to split the dataset based on some <mark style="background: #FF5582A6;">measure of impurity</mark>, such as entropy or Gini index.
    
2.  Split the dataset into subsets based on the selected attribute.
    
3.  Recursively apply steps 1 and 2 to each subset until all instances in a subset belong to the same class or have the same attribute values.

![[structure-of-a-decision-tree.png]]


Decision trees can be used for both classification and regression problems. In classification problems, the class label is a categorical variable, and in regression problems, the target variable is a continuous variable.

Decision trees have several advantages, such as being easy to interpret and visualize, handling both continuous and categorical variables, and handling missing values. However, they also have some limitations, such as being prone to overfitting and being sensitive to the training data.

The following are some commonly used formulas in decision tree algorithms:

-   **Entropy**: a measure of impurity in a set of examples.
$$Entropy = -\sum_{i=1}^n p_i log_2(p_i)$$


-   **Information gain**: the expected reduction in entropy by splitting a set of examples based on an attribute.

$$\text{Information Gain} = H(D) - \sum_{i=1}^{n}\left(\frac{|D_i|}{|D|} \cdot H(D_i)\right)$$
- **Gini index**: a measure of impurity in a set of examples, similar to entropy.

$$ \text{Gini Index} = \sum_{i=1}^n p_i (1-p_i)$$


