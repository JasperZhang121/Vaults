
### Tokenise

```python
tokeniser = nltk.tokenize.TreebankWordTokenizer()
stopwords = frozenset(nltk.corpus.stopwords.words("english"))
trans_table = str.maketrans(dict.fromkeys(string.punctuation))

def tokenise_text(str_):
    # remove non-ASCII characters
    str_ = str_.encode(encoding='ascii', errors='ignore').decode()
    return [t for t in tokeniser.tokenize(str_.lower().translate(trans_table)) if t not in stopwords]
```

### Vectors & Fit Model

Here using TF vectors, which can be changed for fitting different demands.

```python
def fit_model(Xtr, Ytr, C):
    """Tokenizes the sentences, calculates TF vectors, and trains a logistic regression model.
    
    Args:
    - Xtr: A list of training documents provided as text
    - Ytr: A list of training class labels
    - C: The regularization parameter
    """

    # write model fitting code using CountVectorizer and LogisticRegression
    # CountVectorizer is used to convert the text into sparse TF vectors
    # See https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html
    # LogisticRegression will train the classifier using these vectors
    # See https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html
    # return the model and CountVectorizer
    # Note: we need to return the CountVectorizer because 
    # it stores a mapping from words -> ids which we need for testing
    
    count_vectorizer = CountVectorizer()
    X_train_tf = count_vectorizer.fit_transform(Xtr)
    model = LogisticRegression(C=C, max_iter=1000)
    model.fit(X_train_tf, Ytr)
    
    return model, count_vectorizer
```

### Evaluate

```python
def test_model(Xtst, Ytst, model, count_vectoriser):
    # test the logistic regression classifier and calculate the accuracy
    y_pred = model.predict(X_test)
    score = accuracy_score(y_test, y_pred)
    return score
```


