
A loss function is a mathematical function that measures the difference between the predicted output of a model and the actual output. It provides a quantitative measure of the error between the predicted values and the true values. The choice of loss function is an important decision that can have a significant impact on the accuracy and generalization ability of the model. Commonly used loss functions in computer vision include <mark style="background: #BBFABBA6;">mean squared error (MSE), cross-entropy loss, and hinge loss</mark>. 

Check:
- [[3.1 Cost function - Least Square]]
- [[3.2 Cost Function - Logistic Regression]]

Optimization is the process of minimizing the loss function by adjusting the parameters of the model. In other words, it involves finding the set of model parameters that result in the smallest possible loss value. Optimization algorithms such as <mark style="background: #ADCCFFA6;">stochastic gradient descent (SGD), Adam, and Adagrad</mark> are commonly used in computer vision to update the model parameters during training. These algorithms adjust the parameters based on the gradient of the loss function, with the goal of finding the optimal set of parameters that minimize the loss.

Check:
- [[2. Gradient descent]]