
### Convolutional neural networks (CNNs) 

are a type of neural network that are especially well-suited for image recognition and other problems where the input has a spatial structure. They use specific architectures and connection patterns to learn features at different scales and locations in the input.

- Convolution operation:

In a convolutional layer, the convolution operation is used to <mark style="background: #FFB8EBA6;">apply a set of filters to the input image</mark>. This operation can be represented mathematically as follows:

$$\begin{equation} S(i,j) = (I * K)(i,j) = \sum_{m}\sum_{n}I(m,n)K(i-m,j-n) \end{equation}$$

where $S(i,j)$ is the output of the convolution operation at position $(i,j)$, I is the input image, K is the filter/kernel, and * denotes the convolution operation.

-   Pooling operation:

The pooling operation is used to downsample the feature maps obtained from the convolutional layer. The most common pooling operation is <mark style="background: #BBFABBA6;">max pooling</mark>, which takes the maximum value in each pooling region. Mathematically, max pooling can be defined as follows:

$$\begin{equation} M(i,j) = \max_{(m,n)\in R_{i,j}}S(m,n) \end{equation}$$

where M(i,j) is the output of the pooling operation at position (i,j), R_{i,j} is the pooling region centered at (i,j), and S is the input feature map.

-   Rectified Linear Unit (ReLU) activation:

ReLU is a popular activation function used in CNNs. It is defined as follows:

$$\begin{equation} f(x) = \max(0,x) \end{equation}$$

where x is the input to the activation function.

-   Softmax activation:

Softmax is commonly used as the activation function for the output layer in classification problems. It maps the output of the last hidden layer to a probability distribution over the classes. Mathematically, softmax can be defined as follows:

$$\begin{equation} P(y=j|x) = \frac{\exp(z_j)}{\sum_{k=1}^{K}\exp(z_k)} \end{equation}$$

where P(y=j|x) is the probability of the j-th class given the input x, z_j is the j-th element of the output vector z of the last hidden layer, and K is the number of classes.

---

### Example

```TensorFlow
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense

# Initialize the model
model = Sequential()

# Add a convolutional layer with 32 filters, a 3x3 kernel size, and ReLU activation
model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))

# Add a max pooling layer with a pool size of 2x2
model.add(MaxPooling2D(pool_size=(2, 2)))

# Add another convolutional layer with 64 filters, a 3x3 kernel size, and ReLU activation
model.add(Conv2D(64, (3, 3), activation='relu'))

# Add another max pooling layer with a pool size of 2x2
model.add(MaxPooling2D(pool_size=(2, 2)))

# Flatten the output of the previous layer
model.add(Flatten())

# Add a fully connected layer with 128 neurons and ReLU activation
model.add(Dense(128, activation='relu'))

# Add an output layer with a single neuron and sigmoid activation
model.add(Dense(1, activation='sigmoid'))

# Compile the model with binary crossentropy loss and Adam optimizer
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model on training data and validate on validation data
model.fit(x_train, y_train, batch_size=32, epochs=10, validation_data=(x_val, y_val))
```

This code defines a CNN with two convolutional layers, two max pooling layers, and two fully connected layers. The output layer has a single neuron with sigmoid activation, making it suitable for binary classification tasks. The model is compiled with binary crossentropy loss and Adam optimizer and trained on training data with batch size of 32 and for 10 epochs, while validating on validation data.


