**Introduction:**
Gated Recurrent Units (GRUs) are a type of recurrent neural network (RNN) architecture introduced by Kyunghyun Cho et al. in 2014. They were developed to <mark style="background: #BBFABBA6;">combat the vanishing gradient problem of traditional RNNs</mark>, making them more efficient at capturing long-term dependencies in sequences.

![[GRU.png]]

**Structure & Mechanism:**

1. **Gates**:
   GRUs have two gates:
   * **Update Gate (z)**: Determines <mark style="background: #FFB8EBA6;">how much of the previous hidden state to keep and how much of the new candidate state to consider</mark>. It acts like a mixture of the forget and input gates of the LSTM.
   * **Reset Gate (r)**: Decides how much of the past hidden state to forget, influencing the candidate hidden state.

2. **Hidden State**:
   Unlike LSTMs, which have a separate cell state and hidden state, GRUs only have a hidden state.

3. **Computations**:
   Using matrix notation for weight matrices and bias vectors:

   $z = \sigma(W_z . [h_{prev}, x_t] + b_z)$
  $r = \sigma(W_r . [h_{prev}, x_t] + b_r)$
  $h_{candidate} = tanh(W_h . [r * h_{prev}, x_t] + b_h)$
  $h_t = (1 - z) * h_{prev} + z * h_{candidate}$

   Here:
   * $h_{prev}$ is the hidden state from the previous time step.
   * $x_t$ is the current input.
   * $\sigma$ is the sigmoid function.
   * $*$ represents element-wise multiplication.

**Advantages:**

1. **Simpler Architecture**: With only two gates, GRUs have fewer parameters than LSTMs, making them computationally more efficient and quicker to train in some cases.
  
2. **Memory Management**: GRUs can capture long-term dependencies in data, similar to LSTMs, albeit using a different mechanism.

3. **Flexibility**: The gating mechanism allows GRUs to ignore parts of the hidden state when deemed unnecessary, providing adaptive memory tracking.

**Limitations:**

1. **No Cell State**: GRUs lack a separate cell state that LSTMs have. While this makes them simpler, some argue that LSTMs have a slight edge in capturing very long-term dependencies because of the cell state's explicit design.

2. **Performance**: In practice, the choice between GRUs and LSTMs often comes down to the specific dataset and problem. No one-size-fits-all answer dictates which is universally better.

**Applications:**

1. **Sequence-to-Sequence Models**: Common in tasks like machine translation.
  
2. **Time Series Analysis**: Predicting stock prices, weather patterns, etc.
  
3. **Speech Recognition**: Translating audio signals into text.
  
4. **Natural Language Processing**: Sentiment analysis, named entity recognition, and more.

Gated Recurrent Units (GRUs) represent a significant advancement in the RNN landscape, providing an efficient mechanism to capture sequential dependencies. While LSTMs remain a dominant force in many applications, GRUs provide an alternative that, in the right contexts, can offer competitive or even superior performance.