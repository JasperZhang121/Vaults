LSTMs are a type of recurrent neural network (RNN) [[1. Recurrent Neural Networks (RNNs)]] that are designed to address the problem of vanishing gradients in standard RNNs. The key innovation of LSTMs is the addition of a "memory cell" that allows the network to selectively remember or forget information over time.

### Architecture

The basic LSTM cell has<mark style="background: #FFF3A3A6;"> three "gates"</mark> that control the flow of information: the input gate, the <mark style="background: #FF5582A6;">forget gate,</mark> and the output gate. These gates are controlled by <mark style="background: #FFB8EBA6;">sigmoid activation functions</mark> and determine how much information is passed through the cell at each time step.

The input gate determines which information from the input should be stored in the cell, while the forget gate determines which information from the previous state should be forgotten. The output gate then determines which information from the current state should be passed on to the output.

In addition to the three gates, LSTMs also have a "cell state" that allows the network to store and update information over time. The cell state is modified by the input and forget gates, which selectively add or remove information, and by the output gate, which determines which information is passed on to the output.

![[LSTM.png]]

Given:
- $x_t$: Input at time step $t$
- $h_{t-1}$: Hidden state from the previous time step
- $c_{t-1}$: Memory cell state from the previous time step
- $\sigma$: Sigmoid activation function
- $\tanh$ Hyperbolic tangent activation function
- Weights and biases associated with each gate

The LSTM computations are:

1. **Forget Gate**:
$f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)$
The forget gate determines how much of the past cell state information will be retained.

2. **Input Gate**:
$i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)$
$\tilde{c}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)$
The input gate determines how much of the new information will be stored in the cell state.

3. **Cell State Update**:
$c_t = f_t \times c_{t-1} + i_t \times \tilde{c}_t$
The cell state gets updated by forgetting certain information (using the forget gate's output) and then adding new information (using the input gate's output).

4. **Output Gate**:
$o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)$
$h_t = o_t \times \tanh(c_t)$
The output gate determines the next hidden state, which is also the output for this time step.

In these formulas:
- $W$ represents weight matrices (e.g., $W_f$  is the weight matrix for the forget gate).
- $b$ represents bias vectors (e.g., $b_f$ is the bias vector for the forget gate).
- $\times$  denotes element-wise multiplication.
- $[h_{t-1}, x_t]$ indicates the concatenation of $h_{t-1}$ and $x_t$.

### Training

Like other neural networks, LSTMs are trained using backpropagation and gradient descent. During training, the weights of the network are adjusted to minimize a loss function that measures the discrepancy between the predicted output and the true output.

### Applications

LSTMs have been used in a variety of applications, including language modeling, speech recognition, machine translation, and video analysis. They are particularly well-suited for tasks that involve modeling long-term dependencies, such as predicting the next word in a sentence or generating captions for a video.

### Limitations

Despite their strengths, LSTMs can be difficult to train and require large amounts of data to achieve good performance. They can also be prone to overfitting, especially when dealing with noisy or sparse data. In addition, LSTMs can be computationally expensive, especially when dealing with long sequences.
