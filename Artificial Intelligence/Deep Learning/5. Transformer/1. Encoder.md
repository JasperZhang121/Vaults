The Encoder in a Transformer model is designed to process the input sequence and output a sequence of continuous representations. It's composed of a stack of identical layers, each with two sub-layers: a multi-head self-attention mechanism, and a position-wise fully connected feed-forward network.

![[attention_research_1.webp]]


1. **Components of the Encoder:**
    
    A. **Multi-head Self-Attention Mechanism:** The purpose of this component is to allow <mark style="background: #ABF7F7A6;">each word (or more generally, each token) in the input sequence to interact with every other token</mark>. This is accomplished by creating multiple 'heads', each of which performs an attention operation on the input sequence. The results from each head are then concatenated and linearly transformed to result in the final output of the self-attention mechanism.
    
    B. **Position-wise Fully Connected Feed-Forward Network:** This is essentially a two-layer neural network that's applied to each position separately and identically. It's used to transform the output of the self-attention mechanism. It contains two linear transformations, with a ReLU activation function in between.
    
    C. **Add & Norm (Residual Connection and Layer Normalization):** After each of the above two layers, the transformer adds a residual connection and then applies layer normalization. This is crucial for the successful training of deep networks, as it helps to stabilize the network and reduce training times.
    
2. **Working Together of the Components:**
    
    In the multi-head self-attention mechanism, each token in the input sequence is mapped to a query, a key, and a value vector by applying a linear transformation to the input vector. The attention scores (how much focus to put on other parts of the input sentence) are then calculated by taking the dot product of the query vector with the key vector of all tokens in the sentence, followed by a softmax operation.
    
    The output of the attention mechanism for each token is then obtained by taking a weighted sum of all value vectors, where the weights are the attention scores. This is done separately for each attention head, and the results are concatenated and linearly transformed to get the final output of the self-attention layer.
    
    The output of the self-attention layer is then fed into the position-wise feed-forward network. This is a simple neural network that's applied independently to each position. It doesn't have any recurrent or convolutional connections and is composed of two linear layers with a ReLU activation in between.
    
    Finally, the Add & Norm (residual connections and layer normalization) help to stabilize the outputs and aid in training the network.
    
3. **Full Process of the Encoder Running:**
    
    The process begins with the tokenized input sequence being converted into vectors, usually through an embedding layer. Each token in the input sequence is then processed by the multi-head self-attention mechanism, which computes an output vector for each token that's a weighted sum of all tokens' value vectors in the sequence.
    
    The weights in this sum (the attention scores) are computed using the query and key vectors of the tokens, and they determine how much each token in the sequence should contribute to the output of each other token.
    
    The output of the self-attention mechanism for each token is then passed through a position-wise feed-forward network. This network applies a simple transformation to each position independently and identically.
    
    After both the self-attention and position-wise feed-forward operations, there's a residual connection and layer normalization to aid in training the network and stabilize the output.
    
    The final output of the encoder is a sequence of vectors that can be used as input to the decoder. These vectors are a representation of the input sequence that takes into account both the individual meanings of the tokens and their context within the sequence.
    
    Note that each layer in the encoder processes the input independently, resulting in multiple layers of encoding. The outputs from one layer are used as inputs to the next, until the final layer's outputs are produced. This is why the encoder is said to be composed of a 'stack' of layers.
    