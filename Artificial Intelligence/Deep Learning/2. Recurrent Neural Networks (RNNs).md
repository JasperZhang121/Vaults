RNNs are a type of neural network that can model <mark style="background: #BBFABBA6;">sequential data</mark>, such as <mark style="background: #ADCCFFA6;">time series or natural language</mark>. They are capable of processing input of variable length and can maintain a state or memory across time steps.

-   The basic structure of an RNN:

At each time step, an RNN takes an input vector $x_t$ and a hidden state vector $h_{t-1}$ from the previous time step, and produces an output vector $y_t$ and a new hidden state vector $h_t$. This can be represented mathematically as follows:

$$\begin{equation} h_t = f(W_{hh}h_{t-1} + W_{xh}x_t + b_h) \end{equation}$$

$$\begin{equation} y_t = g(W_{hy}h_t + b_y) \end{equation}$$

where $W_{hh}$, $W_{xh}$, and $W_{hy}$ are weight matrices, $b_h$ and $b_y$ are bias vectors, $f$ is a non-linear activation function applied element-wise to the input, and $g$ is an activation function applied to the output.

-   Backpropagation through time (BPTT):

Training an RNN involves computing gradients through time using the BPTT algorithm. The gradients are used to update the parameters of the RNN, such as the weight matrices and bias vectors.

-   Long Short-Term Memory (LSTM):

LSTM is a variant of RNNs that can handle long-term dependencies better than the basic RNN. It has an additional memory cell that can store information for a longer period of time and separate input, output, and forget gates that control the flow of information.

-   Gated Recurrent Unit (GRU):

GRU is another variant of RNNs that is similar to LSTM but with fewer parameters. It has two gates: an update gate that controls how much of the previous memory to keep and a reset gate that controls how much of the new input to incorporate.

```python
from keras.models import Sequential
from keras.layers import LSTM, Dense

# Initialize the model
model = Sequential()

# Add an LSTM layer with 64 units and input shape (timesteps, input_dim)
model.add(LSTM(64, input_shape=(timesteps, input_dim)))

# Add a dense layer with 10 units and softmax activation
model.add(Dense(10, activation='softmax'))

# Compile the model with categorical crossentropy loss and Adam optimizer
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model on training data and validate on validation data
model.fit(x_train, y_train, batch_size=32, epochs=10, validation_data=(x_val, y_val))
```