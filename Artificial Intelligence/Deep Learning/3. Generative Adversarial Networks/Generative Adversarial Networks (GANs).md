
Generative Adversarial Networks (GANs) are a type of neural network that can l<mark style="background: #FFB8EBA6;">earn to generate new data that is similar to a training dataset</mark>. GANs consist of two neural networks: <mark style="background: #FF5582A6;">a generator and a discriminator</mark>. The generator learns to create new data that looks like it came from the training dataset, while the discriminator learns to distinguish between real data from the training dataset and fake data generated by the generator.

### Architecture

The generator network takes random noise as input and produces a new data sample that is meant to resemble the training data. The discriminator network takes in either a real sample from the training data or a fake sample from the generator and tries to determine whether it is real or fake.

During training, the generator is given feedback from the discriminator on how well it is doing at creating realistic samples. The generator tries to improve its output to fool the discriminator into thinking that its output is real, while the discriminator tries to improve its ability to distinguish between real and fake data.

### Training

GANs are trained using <mark style="background: #FF5582A6;">a minimax game</mark> between the generator and discriminator. The generator tries to <mark style="background: #FFB8EBA6;">minimize the probability</mark> that the discriminator can correctly identify its output as fake, while the discriminator tries to<mark style="background: #BBFABBA6;"> maximize this probability</mark>. This results in a game of cat-and-mouse, where the generator learns to create better and more realistic samples, while the discriminator learns to become better at identifying fake samples.

### Applications

GANs have been used in a variety of applications, including image generation, video generation, and text generation. One popular use of GANs is to create realistic images of faces, which can be used in a variety of contexts, such as video games or virtual reality.

### Limitations

GANs can be difficult to train and can suffer from stability issues, such as mode collapse, where the generator learns to produce only a few types of output. In addition, GANs can be computationally expensive to train and require large amounts of data to achieve good performance. Finally, GANs can be prone to generating samples that contain biases or other undesirable features, which can be difficult to control.


### Process

1.  The discriminator network is trained to maximize the probability of assigning the correct label (real or fake) to each sample:
    
    $max_D E[log(D(x))] + E[log(1 - D(G(z)))]$
    
    where x is a real sample, z is a random noise vector, G(z) is the fake sample generated by the generator network, and D(x) and D(G(z)) are the outputs of the discriminator network for the real and fake samples, respectively.
    
2.  The generator network is trained to minimize the probability of the discriminator network correctly assigning the fake label to the generated samples:
    
    $min_G E[log(1 - D(G(z)))]$
    
    where G(z) is the fake sample generated by the generator network, and D(G(z)) is the output of the discriminator network for the fake sample.
    

The overall objective of the GAN is to find a <mark style="background: #FFB86CA6;">Nash equilibrium</mark> between the generator and discriminator networks, where the generator produces samples that are indistinguishable from real samples, and the discriminator cannot accurately distinguish between real and fake samples.


### Cost function:

$min_G max_D V(D, G) = E[x ~ p_data(x)][log D(x)] + E[z ~ p_z(z)][log (1 - D(G(z)))]$

where:
- G(z) is the output of the generator network given input z.
- D(x) is the output of the discriminator network given input x.
- $p_data(x)$ is the true data distribution.
- $p_z(z)$ is the noise distribution that the generator samples from.
- $E[.]$ denotes the expected value.

The goal of the generator is to minimize this cost function by generating samples that fool the discriminator into thinking they are real, while the goal of the discriminator is to maximize this cost function by correctly distinguishing between real and generated samples.
