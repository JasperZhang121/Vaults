
Neural networks are a fundamental component of deep learning. They are composed of interconnected nodes, or neurons, that process and transmit information. Neural networks are used for a variety of tasks such as image and speech recognition, natural language processing, and more.

## Structure of a Neuron

A typical artificial neuron consists of:
- **Input Weights**: Each input is multiplied by a weight, which can be adjusted during training.
- **Summation Function**: The weighted inputs are summed up.
- **Activation Function**: The summation is passed through an activation function to introduce non-linearity.

## Layers in a Neural Network

1. **Input Layer**: Receives raw input data.
2. **Hidden Layers**: Comprise neurons that process data. Deeper networks have more hidden layers.
3. **Output Layer**: Produces the final result, often representing class probabilities in classification tasks.

## Activation Functions

- **Sigmoid**: S-shaped curve, good for binary classification outputs.
- **ReLU (Rectified Linear Unit)**: Replaces negative values with zero, commonly used in hidden layers.
- **Softmax**: Converts raw scores into class probabilities, commonly used in the output layer for multiclass classification.

## Loss Functions

- **Mean Squared Error (MSE)**: Used for regression tasks.
- **Cross-Entropy Loss**: Used for classification tasks, especially with softmax activation in the output layer.

## Training a Neural Network

1. **Forward Pass**: Input data is passed through the network to get predictions.
2. **Loss Calculation**: The difference between predictions and actual targets is calculated using a loss function.
3. **Backpropagation**: Gradients of the loss with respect to weights are computed layer by layer, and weights are updated using optimization algorithms like gradient descent.

## Optimization Algorithms

- **Gradient Descent**: Adjusts weights based on calculated gradients.
- **Stochastic Gradient Descent (SGD)**: Optimizes using a small batch of data at a time.
- **Adam**: Adaptive optimization algorithm combining features of SGD and RMSProp.

## Overfitting and Regularization

- **Overfitting**: Model performs well on training data but poorly on unseen data.
- **Regularization**: Techniques like L1 and L2 regularization prevent overfitting by adding penalty terms to the loss function based on weights.

## Hyperparameters

- **Learning Rate**: Determines step size during weight updates.
- **Number of Hidden Layers**: Affects model complexity and training time.
- **Number of Neurons in Hidden Layers**: Impacts model capacity and complexity.
 

