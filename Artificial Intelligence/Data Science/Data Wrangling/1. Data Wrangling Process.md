1. **Data Extraction:**
    
    - Identify and gather data from various sources such as databases, files, web services, APIs, or through web scraping.
    - Understand the structure and format of the extracted data: CSV, Excel, SQL, JSON, XML, etc. This also includes understanding the semantics of the data fields.
2. **Data Quality Assessment:**
    
    - Examine the data quality by checking for accuracy, completeness, consistency, reliability, and relevance. This step helps to ascertain whether the data is suitable for the intended analysis or not.
    - Look for issues like missing values, duplicate records, inconsistent formats, or incorrect data. Each of these issues may require different handling strategies.
3. **Data Profiling:**
    
    - Exploration: This involves a deep dive into the data to understand its characteristics and patterns. You can use techniques like data querying and data mining to explore the data.
    - Summarisation: This involves presenting the data in a condensed form like statistical summaries or data distribution to get an overview of the data.
    - Visualisation: Use graphical representations such as plots, histograms, or heat maps to understand and communicate the data distribution and patterns.
4. **Data Cleaning:**
    
    - Imputation: Handle missing data by substituting them with statistical estimates or other logical values.
    - Transformation: Convert the data into suitable formats for analysis. This could include encoding, normalisation, or other mathematical transformations.
    - Aggregation: Combine data in a way that summarises the information or provides a different perspective.
    - Standardisation: Ensure that data follows a common format or scale, making it easier for further analysis.
5. **Data Integration:**
    
    - Schema Mapping: Align the schema of different datasets to ensure consistency. This could include renaming columns, changing data types, etc.
    - Data Matching: Identify and match records that refer to the same entity across different datasets. This is also known as record linkage or deduplication.
    - Data Fusion: Merge data from multiple sources into a single, consistent dataset. This often requires handling conflicts between different data sources.
6. **Data Export:**
    
    - Determine the appropriate format for data export based on the subsequent usage, such as CSV, JSON, XML, SQL, etc. Different formats are suitable for different kinds of applications.
    - Validate the exported data to ensure it matches with the final version of the cleaned and processed data. This ensures that no errors were introduced during the export process.
    - Automate the data export process if it needs to be done on a regular basis. Automation can save a lot of time and effort in repeated tasks.

### Outcome

1. **Understanding:**
    
    - After going through the data wrangling process, you should have a comprehensive understanding of your dataset. This includes knowledge about the data structure, the meaning of different features, their distributions, relationships between features, potential issues, and how to handle them. This understanding is crucial for further data analysis or modelling work.
2. **Cleaned, Standardised, Consistent, and Integrated Data:**
    
    - The wrangled data is now cleaned, which means it is free of inaccuracies, inconsistencies, and missing values.
    - It's standardised, meaning that the data follows a common format or scale, enabling easier comparison and analysis.
    - The data is consistent, ensuring that similar entities have the same representation across the dataset.
    - If data from multiple sources was used, the result is an integrated dataset where data from different sources has been harmoniously combined.
3. **Documentation of Data:**
    
    - As part of the data wrangling process, you should also have created detailed documentation about the data. This includes descriptions of the source of data, meaning of different features, decisions made during cleaning and transformation, known issues and their resolutions, and other details about the data.
    - This documentation is extremely valuable for anyone who might use the dataset in the future. It provides context for the data, ensures reproducibility of the wrangling process, and saves a lot of time and effort in understanding the data.