1. **Data Extraction:**
    
    - Identify and gather data from various sources such as databases, files, web services, APIs, or through web scraping.
    - Understand the structure and format of the extracted data: CSV, Excel, SQL, JSON, XML, etc. This also includes understanding the semantics of the data fields.
2. **Data Quality Assessment:**
    
    - Examine the data quality by checking for <mark style="background: #FFB8EBA6;">completeness, consistency, uniqueness, validity, accuracy, and timeliness</mark>. This step helps to ascertain whether the data is suitable for the intended analysis or not.
    - Look for issues like missing values, duplicate records, inconsistent formats, or incorrect data. Each of these issues may require different handling strategies.
3. **Data Profiling:**
    - <mark style="background: #FFB86CA6;">Examining, exploring, visualising and computing statistics</mark>
    - Exploration: This involves a <mark style="background: #BBFABBA6;">deep dive into the data to understand its characteristics and patterns</mark>. You can use techniques like data querying and data mining to explore the data.
    - Summarisation: This involves presenting the data in a <mark style="background: #CACFD9A6;">condensed form like statistical summaries or data distribution to get an overview of the data</mark>.
    - Visualisation: Use graphical representations such as plots, histograms, or heat maps to understand and communicate the data distribution and patterns.
4. **Data Cleaning:**
    
    - Imputation: Handle <mark style="background: #ABF7F7A6;">missing data by substituting them with statistical estimates or other logical values</mark>.
    - Transformation: Convert the data into suitable formats for analysis. This could include <mark style="background: #FFB86CA6;">encoding, normalisation, or other mathematical transformations</mark>.
    - Aggregation: Combine data in a way that <mark style="background: #BBFABBA6;">summarises the information</mark> or provides a different perspective.
    - Standardisation: Ensure that data follows a common format or scale, making it easier for further analysis.
5. **Data Integration:**
    
    - Schema Mapping: Align the schema of different datasets to ensure consistency. This could include <mark style="background: #D2B3FFA6;">renaming columns, changing data types</mark>, etc.
    - Data Matching: Identify and match records that refer to the same entity across different datasets. This is also known as <mark style="background: #ADCCFFA6;">record linkage or deduplication</mark>.
    - Data Fusion: Merge data from multiple sources into a single, consistent dataset. This often requires <mark style="background: #FF5582A6;">handling conflicts between different data sources</mark>.
6. **Data Export:**
    
    - Determine the appropriate format for data export based on the subsequent usage, such as CSV, JSON, XML, SQL, etc. Different formats are suitable for different kinds of applications.
    - Validate the exported data to ensure it matches with the final version of the cleaned and processed data. This ensures that no errors were introduced during the export process.
    - Automate the data export process if it needs to be done on a regular basis. Automation can save a lot of time and effort in repeated tasks.

### Outcome

1. **Understanding:**
    
    - After going through the data wrangling process, you should have a comprehensive understanding of your dataset. This includes knowledge about the <mark style="background: #FFB8EBA6;">data structure, the meaning of different features, their distributions, relationships between features, potential issues, and how to handle them</mark>. This understanding is crucial for further data analysis or modelling work.
2. **Cleaned, Standardised, Consistent, and Integrated Data:**
    
    - The wrangled data is now cleaned, which means it is <mark style="background: #FFB86CA6;">free of inaccuracies, inconsistencies, and missing values</mark>.
    - It's standardised, meaning that the data <mark style="background: #BBFABBA6;">follows a common format or scale</mark>, enabling easier comparison and analysis.
    - The data is consistent, ensuring that <mark style="background: #ADCCFFA6;">similar entities have the same representation across the dataset</mark>.
    - If data from multiple sources was used, the result is an integrated dataset where data from different sources has been harmoniously combined.
3. **Documentation of Data:**
    
    - As part of the data wrangling process, you should also have created <mark style="background: #FFB86CA6;">detailed documentation about the data</mark>. This includes descriptions of the <mark style="background: #FF5582A6;">source of data, meaning of different features, decisions made during cleaning and transformation, known issues and their resolutions, and other details about the data</mark>.
    - This documentation is extremely valuable for anyone who might use the dataset in the future. It provides context for the data, ensures reproducibility of the wrangling process, and saves a lot of time and effort in understanding the data.