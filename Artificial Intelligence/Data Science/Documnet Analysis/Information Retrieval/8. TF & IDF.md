**Term Frequency (TF) and Inverse Document Frequency (IDF)** are two important concepts in information retrieval and text mining. They play a crucial role in <mark style="background: #FFF3A3A6;">ranking and scoring documents for relevance</mark> in search engines and other natural language processing tasks.

1. **Term Frequency (TF):**
    
    - Term Frequency measures the frequency of a term (word) in a document. It indicates how many times a specific term occurs in a document relative to the total number of terms in that document.
    - TF provides information about the local importance of a term within a specific document.
    - TF is computed using the following formula:
```js
TF(term, document) = (Number of occurrences of the term in the document) / (Total number of terms in the document)
```
  

2. **Inverse Document Frequency (IDF):**
    
    - Inverse Document Frequency measures the <mark style="background: #FFB8EBA6;">importance of a term in the entire document collection</mark>. It is designed to downweight terms that occur frequently across many documents, as they are likely to be less informative for distinguishing between documents.
    - The logarithm is used to <mark style="background: #D2B3FFA6;">dampen the effect of very high document frequencies</mark> and prevent overly dominant terms from affecting the score.
    - IDF is computed using the following formula:
```js
IDF(term) = log((Total number of documents) / (Number of documents containing the term))
```


**TF-IDF:**

- The combination of Term Frequency (TF) and Inverse Document Frequency (IDF) is known as TF-IDF. It is a popular weighting scheme used to represent the importance of a term in a document relative to the entire document collection.
- <mark style="background: #ABF7F7A6;">TF-IDF gives higher scores to terms that appear frequently in a particular document (high TF) and are rare across the entire collection (high IDF)</mark>.
- The TF-IDF score for a term in a document is calculated as follows:

**Use Cases:**

- TF-IDF is widely used in information retrieval systems for document ranking, where documents containing more relevant and rare terms receive higher scores.
- It is used in text classification, sentiment analysis, and clustering tasks to identify important features (terms) in a document or set of documents.


---

Sublinear TF scaling
A technique used to modify the calculation of the Term Frequency (TF) component in the TF-IDF (Term Frequency-Inverse Document Frequency) weighting scheme. Instead of using the raw term frequency directly, sublinear scaling applies a function to dampen the effect of excessively frequent terms and give less weight to those terms that appear multiple times within a document.

**Motivation:** The standard TF calculation, where TF(term, document) is equal to the number of occurrences of the term in the document, may lead to imbalanced representations, particularly when dealing with long documents or specific domains. In such cases, common words (stop words) or frequent terms can dominate the importance of the document, making it challenging to accurately represent the document's content and distinguish it from others.

**Sublinear TF Scaling:** Sublinear TF scaling is introduced to address this issue by taking the logarithm of the raw term frequency. The sublinear scaling function applies a logarithmic transformation to compress the term frequency values, reducing the impact of highly frequent terms while preserving the order of importance among terms.

The most common sublinear scaling function is the logarithmic scaling:

```js
Sublinear_TF(term, document) = 1 + log(TF(term, document))
```

---

Maximum Term Frequency (TF) normalization is a technique used to normalize the raw Term Frequency values in a document before calculating the TF component of the TF-IDF (Term Frequency-Inverse Document Frequency) weighting scheme. The goal of maximum TF normalization is to prevent the bias towards longer documents that may have higher raw term frequencies due to their increased word count.

**Motivation:** In some cases, long documents tend to have higher raw term frequencies because they contain more words. This can lead to an unintended emphasis on longer documents in the ranking process when calculating TF-IDF scores. However, longer documents are not necessarily more relevant than shorter ones. Maximum TF normalization addresses this issue by scaling down the raw term frequencies.

**Maximum TF Normalization:** The maximum TF normalization is performed by <mark style="background: #BBFABBA6;">dividing the raw term frequency (TF) for each term in the document by the maximum raw term frequency in that document</mark>. The resulting normalized term frequency (TF_norm) value is between 0 and 1.

The formula for Maximum TF Normalization is:

```c
TF_norm(term, document) = alpha + (1-alpha) * TF(term, document) / max(TF(all_terms, document))

0<alpha<1, usually set to 0.4
```


