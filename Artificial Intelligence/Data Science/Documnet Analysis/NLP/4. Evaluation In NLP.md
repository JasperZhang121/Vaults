### General evaluation techniques
Intrinsic evaluation  
Directly test a task correctness using a gold standard (also called ground truth), e.g.
- Evaluate a POS-Tagger  
- Calculate the match between predicted and gold standard POS-tags

Extrinsic evaluation  
Test whether the output is useful for downstream tasks, e.g.
- Evaluate a summarisation technique in an IR setting
- Does using summaries instead of complete documents help a particul ar retrieval task?

NLP methods are usually intrinsically evaluated against a gold standard

#### Classification metrics
- Accuracy
	- The number of correct predictions divide by the total number of instance
	- Not suitable for class-imbalance datasets
- Precision, Recall, F-measure
	- check [[Evaluation of Classifiers]]
- Evaluating multi-class classification
	- Macro F-measure:$$\text{Macro } F1 = \frac{F1_1 + F1_2 + \dots + F1_k}{k}$$ Where $F1_1, F1_2, \dots, F1_k$  are the F1-scores for each of the $k$ classes.
	- Micro F-measure:  Counting the total true positives, false negatives and false positives globally, then calculate the F-measure
- Threshold free metric: ROC-AUC![[ROC-AUC.png]]
	- AUC: Area Under The Curve  
	- ROC: Receiver Operating Characteristics
	- For binary classification, it allows to tradeoff between Precision and Recall
		- FPR=FP/(FP+TN)
		- TPR=TP/(TP+FN) 
		- AUROC of 0.5 (area under the red dashed line) corresponds to a coin flip, i.e. a random model
		- AUROC less than 0.7 is usually sub- optimal performance 
		- AUROC of 0.70 to 0.80 is often good performance 
		- AUROC greater than 0.8 is often excellent performance 
		- AUROC of 1.0 (area under the purple line) corresponds to a perfect classifier

### Term overlap metrics

output is a text sequence:
- Machine translation  
- Text generation  
- Text simplification  
- Image Caption Generation

Challenges:
- Almost infinite possible correct outputs
- Unlikely that the model will give the same result as a human labeller
- Getting people to manually evaluate every output is costly


BLEU (bilingual evaluation understudy)
An algorithm developed to evaluate the quality of text that has been machine-translated from one language to another. Created at IBM Research in 2002, BLEU provides a numerical score that measures how many phrases in the machine-translated text overlap with phrases in a human-translated reference text. BLEU is often used to assess the performance of machine translation systems.

- A modified form of precision to compare a candidate translation against multiple reference translations.
- Perfect match = 1.0, Perfect mismatch = 0.0  
- Precision-based metrics are biased in favour of short translations → brevity penalty (BP)

**BLEU Score Formula:**
$$\text{BLEU} = BP \times \exp\left(\sum_{n=1}^{N} w_n \log p_n\right)$$
Where:
- $p_n$ is the precision of matching n-grams.
- $w_n$ is the weight given to each n-gram precision (typically,  $w_n = \frac{1}{N}$) for a BLEU-4 calculation).
- $BP$ is the brevity penalty.

pn​ is calculated as the ratio of the number of n-grams in the machine translation output that match with the reference translation, to the total number of n-grams in the machine translation output.

**Example:**

Given the provided precision values and the brevity penalty, the BLEU score can be calculated as follows:

Given:
$p_1 = \frac{4}{6}$
$p_2 = \frac{3}{5}$
$p_3 = \frac{2}{4}$
$p_4 = \frac{1}{3}$
$BP = 1$

The formula for BLEU is:

$BLEU = BP \times \exp\left( \frac{1}{4} \sum_{n=1}^{4} \ln(p_n) \right)$

Plugging in the given values:
$BLEU = 1 \times \exp\left( \frac{1}{4} \left( \ln\left(\frac{4}{6}\right) + \ln\left(\frac{3}{5}\right) + \ln\left(\frac{2}{4}\right) + \ln\left(\frac{1}{3}\right) \right) \right)$

$BLEU \approx 1 \times \exp\left( \frac{1}{4} \left( -0.4055 - 0.5108 - 0.6931 - 1.0986 \right) \right)$

$BLEU \approx 1 \times \exp\left( \frac{1}{4} \times (-2.708) \right)$

$BLEU \approx 1 \times \exp(-0.677)$

$BLEU \approx 1 \times 0.508$

$BLEU \approx 0.508$

#### ROUGE (Recall-Oriented Understudy for Gisting Evaluation)
- Calculate the recall between human and automatic outputs in terms of n-grams (n-gram overlap)
- ROUGE-N: Overlap of n-grams between the system and a gold standard reference

ROUGE is used instead of BLEU in <mark style="background: #ABF7F7A6;">summarisation and text simplification tasks because getting high precision is easy</mark> (e.g. in summarisation just copy the first sentence verbatim)

### Classifier comparison

Comparison between algorithms, e.g.
- Logistic regressions vs. MLP  
- L2 regularisation vs. L1 regularisation

Comparison between feature sets, e.g.
- Bag-of-words vs. Word embeddings  
- Word embeddings vs. Character embeddings

#### Baselines
- How can we know if our result is good?
- Evaluate against a set of baselines:
	- trivial models (e.g. always predict most common class, random guessing) 
	- simple models (e.g. logistic regression)  
	- well known methods for your task  
	- the current state of the art method

#### Ablation studies
- How do we prove that all parts of the proposed model are necessary?
- Ablation testing involves systematically removing (ablating) various aspects of a model, such as feature groups, to see if the ablated classifier is as good as the full model

#### Different datasets
- How do we prove that our model is generally applicable to a problem not just a single dataset?
- Test on multiple different datasets  
- Make sure to include any standard benchmark datasets