### Multilingual NLP
- There is a need for the development of Multilingual applications, for example, commercial applications including: forum moderation and product recommendation systems  
- It's possible to build multilingual NLP resources with monolingual or parallel corpora.... → lots of data
- Solve NLP problems which require us to understand multiple languages
- Streamline our NLP tools to work seamlessly across many languages
- Solve NLP problems in languages without a lot of data

**Examples of Multilingual NLP problems:**

Machine translation
• Translate a sentence from one language into another.  
• Typically modelled as a sequence-to-sequence problem. 
• Often we use <mark style="background: #BBFABBA6;">parallel corpora</mark> to train the model.

Intra-word code switching
- Where the <mark style="background: #ABF7F7A6;">author switches languages part way through a word</mark>.
- For example, in oetverkocht ‘sold out’, the particle oet ‘out’ is used that is associated with Limburgish whereas verkocht ‘sold’ is associated with Dutch. (Nguyen & Cornips, 2016)
- Can be formulated as a sequence labelling problem at the character level, e.g. Tags with BIO encoding

**Multilingual resources**

Universal Dependencies: POS-Tags, morphological features, syntactic dependencies, and treebanks across 100+ languages

• Facilitates multilingual parser development  
• Cross-lingual learning  
• Parsing research from a language typology perspective


**Multilingual word embeddings**
- A word embedding space that is shared across multiple languages. Similar words in different languages are close in the embedding space.
- More recent approaches work by aligning mono-lingual embeddings, because it is easier to find large amounts of data.

Constructing multilingual word embeddings:

Supervised: using a <mark style="background: #FFB8EBA6;">small bilingual dictionary</mark> to learn a mapping from the source to the target space with (iterative) Procrustes alignment

Input: d
• Word vectors from language A (denoted xi ∈ Rd)
• Word vectors from language B (denoted yi ∈ R )
• A small set of words in language A that are translations of words in language B Represented as word pairs, denoted: (xi, yi)i∈{1,...,n}
Output:  
• A mapping from vectors in language A to vectors in language B: yi ≈ Wxi

We use the word pairs, denoted as $(x_i, y_i)$ for $i \in \{1, \ldots, n\}$, as supervised examples to optimize a loss function. Our objective is to achieve an approximation:

$$ y_i \approx Wx_i $$

where we aim to minimize the average loss across all examples:

$$ \text{Minimize:} \quad \frac{1}{n} \sum_{i=1}^{n} l(Wx_i, y_i) $$

In this context, $W$ is chosen under the constraint to be an orthogonal mapping, i.e.,

$$ W^\top W = I $$

This constraint preserves similarities, for instance:

$$ y_i^\top y_j \approx (Wx_i)^\top (Wx_j) = x_i^\top W^\top Wx_j = x_i^\top x_j $$

One choice for the loss function $l$ is the Euclidean distance:

$$ l(Wx_i, y_i) = \| Wx_i - y_i \|^2 $$

More recent techniques propose advanced loss functions that provide better performance, such as Cross-domain Similarity Local Scaling (CSLS).

<mark style="background: #FFB8EBA6;">MUSE is a Python library</mark> for multilingual word embeddings, whose goal is to provide the community with:
- state-of-the-art multilingual word embeddings (fastText embeddings aligned in a common space)
- large-scale high-quality bilingual dictionaries for training and evaluation

Unsupervised: without any parallel data, learn a mapping from the source to the target space using adversarial training and (iterative) Procrustes refinement


### Low Resource (Language) NLP  
• Challenges  
• Cross-lingual language model (XLM)

Domains where there is a small amount of data
- Applies in domains which <mark style="background: #FFB86CA6;">don't have substantial training resources: biomedical, legal, etc</mark>.
- Languages lacking large monolingual or parallel corpora and/or manually crafted linguistic resources sufficient for building statistical NLP applications
- State-of-the-art NLP models require large amounts of <mark style="background: #FFB8EBA6;">training data and complex language-specific engineering</mark>
- Language-specific engineering is expensive, requires linguistically trained speakers of the language

![[multi-language-nlp.png]]

**Cross-Lingual Transfer Learning**
Transfer of annotations
- Such as POS tags, syntactic or semantic features via cross-lingual bridges (e.g. word or phrase alignments)
Transfer of models
- Training a model in a resource-rich language and applying it in a resource-poor language in zero-shot or one-shot learning
Transfer other parameters, e.g. features

**Joint Multilingual or “Polyglot” Learning**
Resource-rich and resource-poor learning using a language - <mark style="background: #ADCCFFA6;">universal representation</mark>
- Convert data in all languages to a shared representation (e.g. multilingual word vectors)
- Train a single model on a mix of datasets in all languages, to enable parameter sharing where possible

**Cross-lingual language model (XLM)**
- **TLM (Translation Language Modeling)**: The model is trained on a <mark style="background: #BBFABBA6;">concatenated bilingual corpus</mark>. A sentence in one language is concatenated with its translation, and then standard masked language modeling (like BERT's MLM) is performed.
- **MLM (Masked Language Modeling)**: This is similar to BERT’s training method where random words in the sentence are masked, and the model predicts them.

**Zero-shot cross-lingual classification**
- Fine tune XLM on an English language classification task (e.g. NLI with only English text)
- Evaluate on a different language (e.g. French, Spanish ....)


