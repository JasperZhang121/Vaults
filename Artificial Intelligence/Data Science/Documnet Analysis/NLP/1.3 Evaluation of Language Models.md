### **3. Evaluation of Language Models:**

- **Intrinsic Evaluation:**
  - **Perplexity:** This is the primary metric used for evaluating language models. It measures how well a probability model predicts a sample. A <mark style="background: #FFB8EBA6;">lower perplexity indicates a better</mark> model that can more accurately predict the sample.
  - **Likelihood:** Using the log-likelihood of a held-out dataset, which is unseen data, to evaluate how well the model predicts this data.
  - **Cross-Entropy:** It is closely related to perplexity and measures the average number of bits needed to encode the information provided by the model.

- **Extrinsic Evaluation:**
  - **Task-Based Metrics:** Here, the language model is <mark style="background: #FFF3A3A6;">integrated into an end application</mark> (e.g., machine translation, speech recognition), and the performance improvement in that task is measured.
  - **Human Judgment:** Sometimes, human evaluators are used to assess the quality of the text generated by the model based on criteria like fluency, coherence, and relevance.

**Perplexity** is a measurement used to quantify how well a probability model predicts a sample. In the context of language models, perplexity is a way to capture the uncertainty a model has in predicting (or "understanding") a sequence of text. Here's a detailed breakdown of the concept:

- **Perplexity** of a language model on a set of test data is defined as the inverse probability of the test set, normalized by the number of words.

For a test set $W = w_1, w_2, ..., w_N$, the perplexity $PP$ is defined as:

$PP(W) = P(w_1, w_2, ..., w_N)^{-\frac{1}{N}}$

Where:
- $P(w_1, w_2, ..., w_N)$ is the probability of the test set according to the model.
- $N$ is the total number of words in the test set.

If the language model assigns equal probability to all words in the test set, the perplexity equals the size of the vocabulary. A lower perplexity score indicates that the model predicts the test data with higher likelihood.

- **Lower Perplexity**: A model with lower perplexity is considered better because it means that the model is less perplexed by the test data. It suggests the model has a higher predictive accuracy for the test set.

- **Prediction Quality**: Perplexity can be seen as a measurement of how surprised the model is when it encounters the actual outcome; less surprise (i.e., lower perplexity) implies better performance.

Perplexity is most informative when the test and training data have similar statistical properties. If the test data differ significantly from the training data, perplexity may not be a good measure of the model's quality.

- **Pre-processing and Vocabulary**: The way text data is cleaned and prepared for the model can impact perplexity. Moreover, the set of words included in the model (the vocabulary) can affect the score. <mark style="background: #ABF7F7A6;">When comparing language models, it's important that they are evaluated using the same vocabulary</mark>.

- **NLP Task Performance**: A crucial point is that improving perplexity does not necessarily mean that an NLP task (like translation, summarization, etc.) will perform better. It simply means that the language model is better at predicting text data.

Let's say we have two language models, Model A and Model B. We test both on the same set of sentences, and Model A has a perplexity of 80, while Model B has a perplexity of 100. <mark style="background: #BBFABBA6;">We would generally prefer Model A for this particular test set since it has a lower perplexity</mark>, indicating that it predicts the words in the test sentences with higher likelihood than Model B. However, this does not mean that Model A will outperform Model B in tasks like speech recognition or machine translation, where other factors come into play.

- **Evaluating Over Time:** Language models may also be evaluated on their ability to retain performance over time, handling changes in language use (dynamic corpora).

### **4. Limitations and Challenges in Evaluation:**
- **Out-of-Domain Generalization:** A model that performs well on one dataset may not perform well on another, especially if there's a domain shift.
- **The Sensitivity of Perplexity:** Perplexity is sensitive to the size of the test set and the smoothness of the model. It does not always correlate perfectly with human judgment.
- **Model Biases:** Evaluation metrics do not typically account for biases in the model's output, which may propagate or amplify societal biases.
