### **Language Models:**

**1. Introduction to Language Models:**
- **Definition:** A language model is a <mark style="background: #FFF3A3A6;">probabilistic framework</mark> that can predict the next word in a sentence given the previous words (n-grams), or can assign probabilities to sequences of words.
- **Use Cases:** They are used in various applications such as speech recognition, machine translation, text generation, and auto-complete features.

#### **Chain rule of probability:**
$P(x1, x2, …, xl ) = P(x1) P(x2 | x1) P(x3 | x1, x2) ⋯ P(xl| x1, x2, …, xl−1)$

**Example:**
	P(John Smith’s hotel room bugged) = P(John) P(Smith’s | John) P(hotel | John Smith’s) … P(bugged | John Smith’s hotel room)

#### **Estimate the Probabilities:**
Maximum likelihood estimation (MLE): 
P(bugged | John Smith’s hotel room) = count(John Smith’s hotel room bugged)/count(John Smith’s hotel room)

**Markov Assumption:**
Simplification:
P(bugged| John Smith's hotel room) ≈ P(bugged|room) 
or
P(bugged| John Smith's hotel room) ≈ P(bugged|hotel room)

#### **Unigram Model-Zero-order Markov assumption:**
$P(x_1, x_2,…, x_l) = P(x_1) \times \prod_{i=1}^{l} P(x_i)$
#### **Bigram Model - First-order Markov assumption:**
$P(x_1, x_2,…, x_l) = P(x_1) \times \prod_{i=2}^{l} P(x_i | x_{i-1})$

P("I want to eat Chinese food")=P("I")×P("want"∣"I")×P("to"∣"want")×P("eat"∣"to")×P("Chinese"∣"eat")×P("food"∣"Chinese")

**Compute Bigram Probabilities:**
Maximum likelihood estimation:
$P(x_i | x_{i-1}) = \frac{\text{count}(x_{i-1}, x_i)}{\sum_x \text{count}(x_{i-1}, x)} = \frac{\text{count}(x_{i-1}, x_i)}{\text{count}(x_{i-1})}$

**Log probabilities (logarithm helps with numerical stability):**
```
logP("<s> I want to eat Chinese food </s>")
=logP("I"∣<s>")+logP("want"∣"I")+logP("to"∣"want")+logP("eat"∣"to")+logP("Chinese"∣"eat")+logP("food"∣"Chinese")+logP("</s>"∣"food")
```

#### **Trigram Models**
$P(w_1, w_2, ..., w_n) = P(w_1) \times P(w_2 | w_1) \times \prod_{i=3}^{n} P(w_i | w_{i-2}, w_{i-1})$

### **2. Smoothing in Language Models:**

#### Overfitting
Overfitting occurs when a statistical model learns the training data too well, including the noise and details that do not generalize to other data sets. This results in poor performance when the model is used to predict new, unseen data, which is the case with most real-world applications where the test corpus differs from the training corpus.

When a <mark style="background: #CACFD9A6;">language model has never encountered the phrase "Chinese food"</mark> in the training corpus, it will assign a probability of zero to this bigram <mark style="background: #D2B3FFA6;">(count(Chinese food) = 0)</mark>. This becomes problematic when trying to calculate the probability of a sentence containing "Chinese food" since the product of any number and zero is zero, resulting in the entire sentence being assigned a zero probability, which isn't helpful.

- **Purpose:** Smoothing is a technique used to handle the problem of <mark style="background: #FFF3A3A6;">zero probabilities in language models</mark> for unseen words or sequences (n-grams) in the training corpus.

- **Techniques:**
  - **Additive (Laplace) Smoothing:** It involves <mark style="background: #CACFD9A6;">adding a small constant (typically 1) to all n-gram counts</mark> to adjust the probabilities, ensuring that no n-gram has a zero probability.
  - **Good-Turing Discounting:** It estimates the probability of <mark style="background: #D2B3FFA6;">unseen n-grams based on the frequency</mark> of n-grams that appear once in the training data.
  - **Backoff and Interpolation:** These methods dynamically adjust the use of lower-order n-gram statistics when higher-order n-grams have zero occurrences.
$$hat{P}(x_i | x_{i-1}) = \lambda P(x_i | x_{i-1}) + (1 - \lambda)P(x_i)$$
$$\hat{P}(x_i | x_{i-2}, x_{i-1}) = \lambda_3 P(x_i | x_{i-2}, x_{i-1}) + \lambda_2 P(x_i | x_{i-1}) + \lambda_1 P(x_i)$$

  - **Kneser-Ney Smoothing:** An advanced method that takes into account the frequency of the n-gram's context rather than the n-gram itself, providing a more nuanced approach to probability distribution.

#### Held-out Data Maximization
- **Held-out data** is a part of the dataset that the model never sees during training. It is used to tune the hyperparameters of the model, such as \(\lambda\) in n-gram models, so that the model generalizes well to new, unseen data.

#### Expectation Maximization (EM)
- **Expectation Maximization (EM)** is an iterative method for finding maximum likelihood estimates of parameters in statistical models, where the model depends on unobserved latent variables. EM alternates between performing an expectation (E) step, which creates a function for the expectation of the log-likelihood evaluated using the current estimate for the parameters, and a maximization (M) step, which computes parameters maximizing the expected log-likelihood found on the E step.

#### Collins et al.'s Approach
- The approach referred to in the text you provided suggests a method for determining the weights (\(\lambda\)) for n-grams of different lengths. This method is:
  - **Data-driven**: \(\lambda\) values are larger for n-gram counts that are more frequently observed in the training data.
  - **N-gram specific**: Each n-gram order (unigram, bigram, trigram, etc.) has a different \(\lambda\) value, allowing the model to weigh different n-gram orders differently based on their predictive power.
  - **Simplified parameter estimation**: Only one parameter (presumably a global smoothing parameter) needs to be estimated, which simplifies the optimization process.

$$\lambda_3 = \frac{\text{count}(x_{i-2}, x_{i-1})}{\text{count}(x_{i-2}, x_{i-1}) + \gamma}$$
$$\lambda_2 = (1 - \lambda_3) \frac{\text{count}(x_{i-1})}{\text{count}(x_{i-1}) + \gamma}$$
$$\lambda_1 = 1 - \lambda_2 - \lambda_3$$
#### How it Works:
1. **Initialization**: Begin with an initial guess for the \(\lambda\) values for each n-gram order.
2. **Expectation (E-step)**: Calculate the expected counts of each n-gram in the held-out data using the current \(\lambda\) values.
3. **Maximization (M-step)**: Adjust the \(\lambda\) values to maximize the likelihood of the held-out data given the expected counts from the E-step.
4. **Iteration**: Repeat the E-step and M-step until the \(\lambda\) values converge or the improvement falls below a certain threshold.
5. **Finalization**: Once the \(\lambda\) values have been optimized, they are used in the language model to predict the likelihood of new sentences.

#### Absolute Discounting
Absolute Discounting aims to address the issue of infrequent sequences in language models and to allocate some probability mass to unseen n-grams. Here's an expanded explanation based on the points you've mentioned:

#### Absolute Discounting Process:

- **Target of Discounting**: Absolute discounting specifically targets sequences that occur <mark style="background: #D2B3FFA6;">infrequently (i.e., low-frequency n-grams)</mark>. The intuition is that counts from infrequent events are often overestimated in their true probability of occurrence.

- **Reserving Probability Mass**: By discounting each observed count by a small constant \( d \), probability mass is freed up. This mass can then be <mark style="background: #ADCCFFA6;">redistributed to n-grams that haven't been seen in the training data</mark>, ensuring that the model can handle previously unseen sequences.

- **Amount of Discount**: Determining how much to discount, represented by \( d \), is critical. This value is often estimated by using a <mark style="background: #ABF7F7A6;">held-out dataset separate from the training corpus</mark>. The held-out corpus is not used for training the model but for tuning parameters like $d$ to ensure the model generalizes well to new data. Typically, $d$  is a small positive number such as 0.75, but the exact value that works best can depend on the specific dataset and model.

- **Ensuring a True Probability Distribution**: For a model to be useful, the probabilities for all possible next words given a context must sum to 1, forming a true probability distribution. With absolute discounting, <mark style="background: #ADCCFFA6;">after discounting the counts and redistributing probability mass to unseen n-grams</mark>, it's essential to ensure that the sum of probabilities over all possible next words still equals 1.

  To make sure this is the case, a normalization factor is used. The redistributed mass is assigned to unseen n-grams in proportion to some base measure, often the unigram distribution. After redistribution, the probabilities should indeed sum to 1, thus maintaining a true probability distribution.

#### Practical Considerations:

When implementing absolute discounting, one must:

- Properly estimate the discount value \( d \) using a validation set.
- Ensure that the redistribution of probability mass is done in a way that respects the base distribution (like a unigram model).
- Check the resulting probabilities after discounting and redistribution to confirm that they form a proper probability distribution.

Absolute discounting is one of several techniques (others include Good-Turing discounting and Kneser-Ney smoothing) that aim to improve the performance of n-gram models, especially in the presence of sparse data. The right choice of discounting method can significantly affect the quality of language model predictions.
$$PAbsDiscount(xi | xi−1) = \frac{\max(\text{count}(xi−1, xi) − d, 0)}{\text{count}(xi−1)} + \lambda(xi−1) P(xi)$$
$$\lambda(xi−1) = \frac{d}{\text{count}(xi−1)} \cdot \left|\{x : \text{count}(xi−1, x) > 0\}\right|$$
### Kneser-Ney

The continuation probability `Pcontinuation(x_i)` is defined as the proportion of unique preceding words (contexts) that the word `x_i` follows. Mathematically, it is represented as follows:

$$
P_{\text{continuation}}(x_i) \propto |\{x : \text{count}(x, x_i) > 0\}|
$$

This implies that `Pcontinuation(x_i)` is directly proportional to the number of different words `x` that precede `x_i`.

For example, consider the word "Francisco" and compare it with "glasses":

$$
|\{x_{i−1} : \text{count}(x_{i−1}, \text{Francisco}) > 0\}| \ll |\{x_{i−1} : \text{count}(x_{i−1}, \text{glasses}) > 0\}|
$$

The notation `|\{x_{i−1} : \text{count}(x_{i−1}, \text{Francisco}) > 0\}|` represents the count of unique contexts that the word "Francisco" appears in, which is significantly less than that for the word "glasses."

The exact calculation of `Pcontinuation(x_i)` is given by the formula:

$$
P_{\text{continuation}}(x_i) = \frac{|\{x : \text{count}(x, x_i) > 0\}|}{\sum_{x'}|\{x : \text{count}(x, x') > 0\}|}
$$

Where the denominator sums over all `x'` the number of unique words `x` that precede each `x'`.


The Kneser-Ney smoothing probability `PKN(xi | xi−1)` is given by the formula:
$$
P_{\text{KN}}(x_i | x_{i−1}) = \frac{\max(\text{count}(x_{i−1}, x_i) - d, 0)}{\text{count}(x_{i−1})} + \lambda(x_{i−1}) P_{\text{continuation}}(x_i)
$$
where:

- `count(x_{i−1}, x_i)` is the count of the bigram `(x_{i−1}, x_i)`.
- `d` is the discount parameter.
- `λ(x_{i−1})` is a normalizing constant for the context `x_{i−1}`.
- `P_{\text{continuation}}(x_i)` is the continuation probability of the word `x_i`.

#### **Stupid Backoff (Brants et al. 2007):**
A <mark style="background: #ADCCFFA6;">simple and computationally efficient approach</mark> for handling unseen n-grams in large language models. It was proposed by Brants et al. in 2007 and is particularly suited for situations where the dataset is so large that even n-grams with low counts are considered to have enough statistical evidence.

1. **Key Principle:** Stupid Backoff does not try to create a <mark style="background: #D2B3FFA6;">true probability distribution</mark>; instead, it offers <mark style="background: #ADCCFFA6;">a scoring function that can be used to rank possible continuations</mark> of a given text sequence.

2. **Backoff Mechanism:**
   - When an n-gram has been observed in the training data, its score is proportional to its frequency.
   - If an n-gram has not been observed (i.e., has a count of 0), Stupid Backoff falls back to a shorter n-gram (n-1 gram) by dropping the first word of the n-gram.
   - This process is repeated recursively: if the (n-1)-gram hasn't been seen, it will backoff to an (n-2)-gram, and so on, until it reaches unigrams.

3. **Fixed Weight \( \lambda \):**
   - When backing off to a shorter n-gram, a fixed weight \( \lambda \) is applied, which is less than 1, to the frequency of the shorter n-gram.
   - This weight discounts the score of the shorter n-gram, giving preference to longer, observed n-grams.
   - The value of \( \lambda \) is chosen empirically, and the paper suggested a value of 0.4 based on their experiments.

4. **Score Calculation:**
   - If the n-gram has been seen, its score is:
     $$ S(w_i| w_{i-n+1}^{i-1}) = \frac{\text{count}(w_{i-n+1}^i)}{\text{count}(w_{i-n+1}^{i-1})} $$
   - If the n-gram has not been seen, the score is:
     $$ \lambda S(w_i| w_{i-n+2}^{i-1}) $$
   - Here, \( S(x) \) is the score of a unigram \( x \):
     $$ S(x) = \frac{\text{count}(x)}{N} $$
   - Where \( N \) is the size of the training corpus in words.

5. **Not a True Probability Distribution:**
   - Because Stupid Backoff does not ensure that the probabilities of all possible continuations of a given context sum to one, it does not produce a true probability distribution.
   - This means that it cannot be used where probabilistic interpretations are necessary, like in probabilistic parsing or hidden Markov models.

Despite its simplicity and the fact that it doesn't provide a probability distribution, Stupid Backoff has been shown to work well in practice, especially for very large language models where the sheer volume of data makes complex smoothing methods less necessary. It's primarily used for tasks where the goal is to rank candidate continuations, such as in web search or machine translation.

### **3. Evaluation of Language Models:**

- **Intrinsic Evaluation:**
  - **Perplexity:** This is the primary metric used for evaluating language models. It measures how well a probability model predicts a sample. A lower perplexity indicates a better model that can more accurately predict the sample.
  - **Likelihood:** Using the log-likelihood of a held-out dataset, which is unseen data, to evaluate how well the model predicts this data.
  - **Cross-Entropy:** It is closely related to perplexity and measures the average number of bits needed to encode the information provided by the model.

- **Extrinsic Evaluation:**
  - **Task-Based Metrics:** Here, the language model is integrated into an end application (e.g., machine translation, speech recognition), and the performance improvement in that task is measured.
  - **Human Judgment:** Sometimes, human evaluators are used to assess the quality of the text generated by the model based on criteria like fluency, coherence, and relevance.

**Perplexity** is a measurement used to quantify how well a probability model predicts a sample. In the context of language models, perplexity is a way to capture the uncertainty a model has in predicting (or "understanding") a sequence of text. Here's a detailed breakdown of the concept:

- **Perplexity** of a language model on a set of test data is defined as the inverse probability of the test set, normalized by the number of words.

For a test set $W = w_1, w_2, ..., w_N$, the perplexity $PP$ is defined as:

$PP(W) = P(w_1, w_2, ..., w_N)^{-\frac{1}{N}}$

Where:
- $P(w_1, w_2, ..., w_N)$ is the probability of the test set according to the model.
- $N$ is the total number of words in the test set.

If the language model assigns equal probability to all words in the test set, the perplexity equals the size of the vocabulary. A lower perplexity score indicates that the model predicts the test data with higher likelihood.

- **Lower Perplexity**: A model with lower perplexity is considered better because it means that the model is less perplexed by the test data. It suggests the model has a higher predictive accuracy for the test set.

- **Prediction Quality**: Perplexity can be seen as a measurement of how surprised the model is when it encounters the actual outcome; less surprise (i.e., lower perplexity) implies better performance.

Perplexity is most informative when the test and training data have similar statistical properties. If the test data differ significantly from the training data, perplexity may not be a good measure of the model's quality.

- **Pre-processing and Vocabulary**: The way text data is cleaned and prepared for the model can impact perplexity. Moreover, the set of words included in the model (the vocabulary) can affect the score. When comparing language models, it's important that they are evaluated using the same vocabulary.

- **NLP Task Performance**: A crucial point is that improving perplexity does not necessarily mean that an NLP task (like translation, summarization, etc.) will perform better. It simply means that the language model is better at predicting text data.

Let's say we have two language models, Model A and Model B. We test both on the same set of sentences, and Model A has a perplexity of 80, while Model B has a perplexity of 100. <mark style="background: #BBFABBA6;">We would generally prefer Model A for this particular test set since it has a lower perplexity</mark>, indicating that it predicts the words in the test sentences with higher likelihood than Model B. However, this does not mean that Model A will outperform Model B in tasks like speech recognition or machine translation, where other factors come into play.

- **Evaluating Over Time:** Language models may also be evaluated on their ability to retain performance over time, handling changes in language use (dynamic corpora).

### **4. Limitations and Challenges in Evaluation:**
- **Out-of-Domain Generalization:** A model that performs well on one dataset may not perform well on another, especially if there's a domain shift.
- **The Sensitivity of Perplexity:** Perplexity is sensitive to the size of the test set and the smoothness of the model. It does not always correlate perfectly with human judgment.
- **Model Biases:** Evaluation metrics do not typically account for biases in the model's output, which may propagate or amplify societal biases.
