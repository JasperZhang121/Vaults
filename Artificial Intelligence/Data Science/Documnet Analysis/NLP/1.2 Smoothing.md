### **2. Smoothing in Language Models:**

#### Overfitting
Overfitting occurs when a statistical model learns the training data too well, including the noise and details that do not generalize to other data sets. This results in poor performance when the model is used to predict new, unseen data, which is the case with most real-world applications where the test corpus differs from the training corpus.

When a <mark style="background: #CACFD9A6;">language model has never encountered the phrase "Chinese food"</mark> in the training corpus, it will assign a probability of zero to this bigram <mark style="background: #D2B3FFA6;">(count(Chinese food) = 0)</mark>. This becomes problematic when trying to calculate the probability of a sentence containing "Chinese food" since the product of any number and zero is zero, resulting in the entire sentence being assigned a zero probability, which isn't helpful.

- **Purpose:** Smoothing is a technique used to handle the problem of <mark style="background: #FFF3A3A6;">zero probabilities in language models</mark> for unseen words or sequences (n-grams) in the training corpus.

- **Techniques:**
  - **Additive (Laplace) Smoothing:** It involves <mark style="background: #CACFD9A6;">adding a small constant (typically 1) to all n-gram counts</mark> to adjust the probabilities, ensuring that no n-gram has a zero probability.
  - **Good-Turing Discounting:** It estimates the probability of <mark style="background: #D2B3FFA6;">unseen n-grams based on the frequency</mark> of n-grams that appear once in the training data.
  - **Backoff and Interpolation:** These methods dynamically adjust the use of lower-order n-gram statistics when higher-order n-grams have zero occurrences.
$$hat{P}(x_i | x_{i-1}) = \lambda P(x_i | x_{i-1}) + (1 - \lambda)P(x_i)$$
$$\hat{P}(x_i | x_{i-2}, x_{i-1}) = \lambda_3 P(x_i | x_{i-2}, x_{i-1}) + \lambda_2 P(x_i | x_{i-1}) + \lambda_1 P(x_i)$$

  - **Kneser-Ney Smoothing:** An advanced method that takes into account the frequency of the n-gram's context rather than the n-gram itself, providing a more nuanced approach to probability distribution.

Let's illustrate how backoff and interpolation methods would work with a simple example. We will consider a scenario where we're trying to determine the probability of the word "dog" following the bigram "the brown".

#### Without Backoff or Interpolation:

Let's assume we have the following counts from our training corpus:

- $C("the\ brown\ dog") = 0$ (the trigram "the brown dog" was never observed)
- $C("the\ brown") = 2$
- $C("dog") = 10$
- $C(\text{total bigrams}) = 1000$
- $C(\text{total words}) = 5000$

Using just the maximum likelihood estimate, we would get:

- $P("dog" | "the\ brown") = \frac{C("the\ brown\ dog")}{C("the\ brown")} = \frac{0}{2} = 0$

This is problematic because it assigns a probability of zero to the trigram "the brown dog," implying that it's impossible for "dog" to follow "the brown," which is not a realistic representation of language.

#### With Backoff:

We can use the bigram "brown dog" or even the unigram "dog" to estimate this probability instead:

- $C("brown\ dog") = 3$
- $C("brown") = 15$

So, we can back off to:

- $P_{\text{backoff}}("dog" | "brown") = \frac{C("brown\ dog")}{C("brown")} = \frac{3}{15}$

This non-zero probability is more realistic than assuming it's impossible for "dog" to follow "brown".

### With Interpolation:

We can calculate a weighted combination of the unigram, bigram, and trigram probabilities. Assume that we determined our lambda values (which should sum to 1) from a validation set are as follows:

- $\lambda_1 = 0.2$ (for unigram)
- $\lambda_2 = 0.3$ (for bigram)
- $\lambda_3 = 0.5$ (for trigram, but we know this is zero in our case)

We calculate the probabilities as follows:

- $P("dog") = \frac{C("dog")}{C(\text{total words})} = \frac{10}{5000}$

Now, using interpolation, we get:

- $\hat{P}("dog" | "the\ brown") = \lambda_3 \cdot 0 + \lambda_2 \cdot P_{\text{backoff}}("dog" | "brown") + \lambda_1 \cdot P("dog")$
- $\hat{P}("dog" | "the\ brown") = 0.5 \cdot 0 + 0.3 \cdot \frac{3}{15} + 0.2 \cdot \frac{10}{5000}$
- $\hat{P}("dog" | "the\ brown") = 0.3 \cdot 0.2 + 0.2 \cdot 0.002$
- $\hat{P}("dog" | "the\ brown") = 0.06 + 0.0004$
- $\hat{P}("dog" | "the\ brown") = 0.0604$

By using interpolation, we've assigned a small but non-zero probability to the trigram "the brown dog," which reflects the reality that while "dog" may not have been observed following "the brown" in our training data, it is not impossible for it to occur.

---
#### Held-out Data Maximization
- **Held-out data** is a part of the dataset that the model never sees during training. It is used to tune the hyperparameters of the model, such as \(\lambda\) in n-gram models, so that the model generalizes well to new, unseen data.

#### Expectation Maximization (EM)
- **Expectation Maximization (EM)** is an iterative method for finding maximum likelihood estimates of parameters in statistical models, where the model depends on unobserved latent variables. EM alternates between performing an expectation (E) step, which creates a function for the expectation of the log-likelihood evaluated using the current estimate for the parameters, and a maximization (M) step, which computes parameters maximizing the expected log-likelihood found on the E step.
---
#### Collins et al.'s Approach
  - **Data-driven**: $\lambda$ values are larger for n-gram counts that are more frequently observed in the training data.
  - **N-gram specific**: Each n-gram order (unigram, bigram, trigram, etc.) has a different $\lambda$ value, allowing the model to weigh different n-gram orders differently based on their predictive power.
  - **Simplified parameter estimation**: Only one parameter (presumably a global smoothing parameter) needs to be estimated, which simplifies the optimization process.

$$\lambda_3 = \frac{\text{count}(x_{i-2}, x_{i-1})}{\text{count}(x_{i-2}, x_{i-1}) + \gamma}$$
$$\lambda_2 = (1 - \lambda_3) \frac{\text{count}(x_{i-1})}{\text{count}(x_{i-1}) + \gamma}$$
$$\lambda_1 = 1 - \lambda_2 - \lambda_3$$
#### How it Works:
1. **Initialization**: Begin with an initial guess for the $\lambda$ values for each n-gram order.
2. **Expectation (E-step)**: Calculate the expected counts of each n-gram in the held-out data using the current $\lambda$ values.
3. **Maximization (M-step)**: Adjust the \(\lambda\) values to maximize the likelihood of the held-out data given the expected counts from the E-step.
4. **Iteration**: Repeat the E-step and M-step until the \(\lambda\) values converge or the improvement falls below a certain threshold.
5. **Finalization**: Once the $\lambda$ values have been optimized, they are used in the language model to predict the likelihood of new sentences.

#### Absolute Discounting

Process:
- **Target of Discounting**: Absolute discounting specifically targets sequences that occur <mark style="background: #D2B3FFA6;">infrequently (i.e., low-frequency n-grams)</mark>. The intuition is that counts from infrequent events are often overestimated in their true probability of occurrence.

- **Reserving Probability Mass**: By discounting each observed count by a small constant $d$, probability mass is freed up. This mass can then be <mark style="background: #ADCCFFA6;">redistributed to n-grams that haven't been seen in the training data</mark>, ensuring that the model can handle previously unseen sequences.

- **Amount of Discount**: Determining how much to discount, represented by$d$, is critical. This value is often estimated by using a <mark style="background: #ABF7F7A6;">held-out dataset separate from the training corpus</mark>. The held-out corpus is not used for training the model but for tuning parameters like $d$ to ensure the model generalizes well to new data. Typically, $d$  is a small positive number such as 0.75, but the exact value that works best can depend on the specific dataset and model.

- **Ensuring a True Probability Distribution**: For a model to be useful, the probabilities for all possible next words given a context must sum to 1, forming a true probability distribution. With absolute discounting, <mark style="background: #ADCCFFA6;">after discounting the counts and redistributing probability mass to unseen n-grams</mark>, it's essential to ensure that the sum of probabilities over all possible next words still equals 1.

  To make sure this is the case, a normalization factor is used. The redistributed mass is assigned to unseen n-grams in proportion to some base measure, often the unigram distribution. After redistribution, the probabilities should indeed sum to 1, thus maintaining a true probability distribution.

#### Practical Considerations:

When implementing absolute discounting, one must:

- Properly estimate the discount value $d$ using a validation set.
- Ensure that the redistribution of probability mass is done in a way that respects the base distribution (like a unigram model).
- Check the resulting probabilities after discounting and redistribution to confirm that they form a proper probability distribution.

Absolute discounting is one of several techniques (others include Good-Turing discounting and Kneser-Ney smoothing) that aim to improve the performance of n-gram models, especially in the presence of sparse data. The right choice of discounting method can significantly affect the quality of language model predictions.
$$PAbsDiscount(xi | xi−1) = \frac{\max(\text{count}(xi−1, xi) − d, 0)}{\text{count}(xi−1)} + \lambda(xi−1) P(xi)$$
$$\lambda(xi−1) = \frac{d}{\text{count}(xi−1)} \cdot \left|\{x : \text{count}(xi−1, x) > 0\}\right|$$
### Kneser-Ney

The continuation probability `Pcontinuation(x_i)` is defined as the proportion of unique preceding words (contexts) that the word `x_i` follows. Mathematically, it is represented as follows:

$$
P_{\text{continuation}}(x_i) \propto |\{x : \text{count}(x, x_i) > 0\}|
$$

This implies that `Pcontinuation(x_i)` is directly proportional to the number of different words `x` that precede `x_i`.

For example, consider the word "Francisco" and compare it with "glasses":

$$
|\{x_{i−1} : \text{count}(x_{i−1}, \text{Francisco}) > 0\}| \ll |\{x_{i−1} : \text{count}(x_{i−1}, \text{glasses}) > 0\}|
$$

The notation $|\{x_{i−1} : \text{count}(x_{i−1}, \text{Francisco}) > 0\}|$ represents the count of unique contexts that the word "Francisco" appears in, which is significantly less than that for the word "glasses."

The exact calculation of `Pcontinuation(x_i)` is given by the formula:

$$
P_{\text{continuation}}(x_i) = \frac{|\{x : \text{count}(x, x_i) > 0\}|}{\sum_{x'}|\{x : \text{count}(x, x') > 0\}|}
$$

Where the denominator sums over all `x'` the number of unique words `x` that precede each `x'`.

The Kneser-Ney smoothing probability `PKN(xi | xi−1)` is given by the formula:
$$
P_{\text{KN}}(x_i | x_{i−1}) = \frac{\max(\text{count}(x_{i−1}, x_i) - d, 0)}{\text{count}(x_{i−1})} + \lambda(x_{i−1}) P_{\text{continuation}}(x_i)
$$
where:
- $count(x_{i−1}, x_i)$ is the count of the bigram $(x_{i−1}, x_i)$.
- $d$ is the discount parameter.
- $λ(x_{i−1})$ is a normalizing constant for the context $x_{i−1}$.
- $P_{\text{continuation}}(x_i)$ is the continuation probability of the word $x_i$.

#### **Stupid Backoff (Brants et al. 2007):**
A <mark style="background: #ADCCFFA6;">simple and computationally efficient approach</mark> for handling unseen n-grams in large language models. It was proposed by Brants et al. in 2007 and is particularly suited for situations where the dataset is so large that even n-grams with low counts are considered to have enough statistical evidence.

1. **Key Principle:** Stupid Backoff does not try to create a <mark style="background: #D2B3FFA6;">true probability distribution</mark>; instead, it offers <mark style="background: #ADCCFFA6;">a scoring function that can be used to rank possible continuations</mark> of a given text sequence.

2. **Backoff Mechanism:**
   - When an n-gram has been observed in the training data, its score is proportional to its frequency.
   - If an n-gram has not been observed (i.e., has a count of 0), Stupid Backoff falls back to a shorter n-gram (n-1 gram) by dropping the first word of the n-gram.
   - This process is repeated recursively: if the (n-1)-gram hasn't been seen, it will backoff to an (n-2)-gram, and so on, until it reaches unigrams.

3. **Fixed Weight \( \lambda \):**
   - When backing off to a shorter n-gram, a fixed weight \( \lambda \) is applied, which is less than 1, to the frequency of the shorter n-gram.
   - This weight discounts the score of the shorter n-gram, giving preference to longer, observed n-grams.
   - The value of \( \lambda \) is chosen empirically, and the paper suggested a value of 0.4 based on their experiments.

4. **Score Calculation:**
   - If the n-gram has been seen, its score is:
     $$ S(w_i| w_{i-n+1}^{i-1}) = \frac{\text{count}(w_{i-n+1}^i)}{\text{count}(w_{i-n+1}^{i-1})} $$
   - If the n-gram has not been seen, the score is:
     $$ \lambda S(w_i| w_{i-n+2}^{i-1}) $$
   - Here, \( S(x) \) is the score of a unigram \( x \):
     $$ S(x) = \frac{\text{count}(x)}{N} $$
   - Where \( N \) is the size of the training corpus in words.

5. **Not a True Probability Distribution:**
   - Because Stupid Backoff does not ensure that the probabilities of all possible continuations of a given context sum to one, it does not produce a true probability distribution.
   - This means that it cannot be used where probabilistic interpretations are necessary, like in probabilistic parsing or hidden Markov models.

Despite its simplicity and the fact that it doesn't provide a probability distribution, Stupid Backoff has been shown to work well in practice, especially for very large language models where the sheer volume of data makes complex smoothing methods less necessary. It's primarily used for tasks where the goal is to rank candidate continuations, such as in web search or machine translation.
