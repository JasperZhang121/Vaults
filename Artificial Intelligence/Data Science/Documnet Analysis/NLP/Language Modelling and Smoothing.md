### **Language Models:**

**1. Introduction to Language Models:**
- **Definition:** A language model is a <mark style="background: #FFF3A3A6;">probabilistic framework</mark> that can predict the next word in a sentence given the previous words (n-grams), or can assign probabilities to sequences of words.
- **Use Cases:** They are used in various applications such as speech recognition, machine translation, text generation, and auto-complete features.

**Chain rule of probability:**
$P(x1, x2, …, xl ) = P(x1) P(x2 | x1) P(x3 | x1, x2) ⋯ P(xl| x1, x2, …, xl−1)$

**Example:**
	P(John Smith’s hotel room bugged) = P(John) P(Smith’s | John) P(hotel | John Smith’s) … P(bugged | John Smith’s hotel room)

**Estimate the Probabilities:**
Maximum likelihood estimation (MLE): 
P(bugged | John Smith’s hotel room) = count(John Smith’s hotel room bugged)/count(John Smith’s hotel room)

**Markov Assumption:**
Simplification:
P(bugged| John Smith's hotel room) ≈ P(bugged|room) 
or
P(bugged| John Smith's hotel room) ≈ P(bugged|hotel room)

**Bigram Model - First-order Markov assumption:**
$P(x_1, x_2,…, x_l) = P(x_1) \times \prod_{i=2}^{l} P(x_i | x_{i-1})$
P("I want to eat Chinese food")=P("I")×P("want"∣"I")×P("to"∣"want")×P("eat"∣"to")×P("Chinese"∣"eat")×P("food"∣"Chinese")

**Unigram Model-Zero-order Markov assumption:**
$P(x_1, x_2,…, x_l) = P(x_1) \times \prod_{i=1}^{l} P(x_i)$

**Compute Bigram Probabilities:**
Maximum likelihood estimation:
$P(x_i | x_{i-1}) = \frac{\text{count}(x_{i-1}, x_i)}{\sum_x \text{count}(x_{i-1}, x)} = \frac{\text{count}(x_{i-1}, x_i)}{\text{count}(x_{i-1})}$

### **2. Smoothing in Language Models:**

- **Purpose:** Smoothing is a technique used to handle the problem of zero probabilities in language models for unseen words or sequences (n-grams) in the training corpus.

- **Techniques:**
  - **Additive (Laplace) Smoothing:** It involves adding a small constant (typically 1) to all n-gram counts to adjust the probabilities, ensuring that no n-gram has a zero probability.
  - **Good-Turing Discounting:** It estimates the probability of unseen n-grams based on the frequency of n-grams that appear once in the training data.
  - **Backoff and Interpolation:** These methods dynamically adjust the use of lower-order n-gram statistics when higher-order n-grams have zero occurrences.
  - **Kneser-Ney Smoothing:** An advanced method that takes into account the frequency of the n-gram's context rather than the n-gram itself, providing a more nuanced approach to probability distribution.

### **3. Evaluation of Language Models:**

- **Intrinsic Evaluation:**
  - **Perplexity:** This is the primary metric used for evaluating language models. It measures how well a probability model predicts a sample. A lower perplexity indicates a better model that can more accurately predict the sample.
  - **Likelihood:** Using the log-likelihood of a held-out dataset, which is unseen data, to evaluate how well the model predicts this data.
  - **Cross-Entropy:** It is closely related to perplexity and measures the average number of bits needed to encode the information provided by the model.

- **Extrinsic Evaluation:**
  - **Task-Based Metrics:** Here, the language model is integrated into an end application (e.g., machine translation, speech recognition), and the performance improvement in that task is measured.
  - **Human Judgment:** Sometimes, human evaluators are used to assess the quality of the text generated by the model based on criteria like fluency, coherence, and relevance.

- **Evaluating Over Time:** Language models may also be evaluated on their ability to retain performance over time, handling changes in language use (dynamic corpora).

### **4. Limitations and Challenges in Evaluation:**
- **Out-of-Domain Generalization:** A model that performs well on one dataset may not perform well on another, especially if there's a domain shift.
- **The Sensitivity of Perplexity:** Perplexity is sensitive to the size of the test set and the smoothness of the model. It does not always correlate perfectly with human judgment.
- **Model Biases:** Evaluation metrics do not typically account for biases in the model's output, which may propagate or amplify societal biases.
