Syntactic parsing is the computational process of <mark style="background: #ABF7F7A6;">analyzing a string of symbols in natural language (sentences) according to the rules of formal grammar</mark>. The aim is to understand the <mark style="background: #D2B3FFA6;">structure of sentences and the relationships between words</mark>. This is crucial for numerous Natural Language Processing (NLP) tasks such as machine translation, speech recognition, and information extraction.
### Context-Free Grammars (CFG)

- **Definition**: A CFG is a type of formal grammar that is composed of a finite set of production rules. It is called "context-free" because the rules can be <mark style="background: #ADCCFFA6;">applied regardless of the context of the nonterminals</mark>.

- **Components**:
  - **Terminals**: The basic symbols from which strings are formed (usually the words in a language).
  - **Nonterminals**: Symbols that denote sets of strings (syntactic categories like NP for noun phrase, VP for verb phrase).
  - **Start symbol**: A special nonterminal which denotes the category of the entire sentence (often S).
  - **Production rules**: Rules that describe how terminals and nonterminals can be combined.

- **Example Rule**:
  - S → NP VP
  - NP → Det N | N

![[context-free-grammer.png]]
- **Chomsky Hierarchy**: CFGs are part of the Chomsky hierarchy, which categorizes grammatical formalisms according to their expressive power. CFGs are more powerful than regular grammars but less powerful than context-sensitive grammars.


#### Probabilistic Context-Free Grammar (PCFG) 

An extension of a regular context-free grammar. While a context-free grammar (CFG) provides rules for constructing valid strings in a language, a PCFG assigns <mark style="background: #FFF3A3A6;">probabilities to each of these rules</mark>. The main motivation for using PCFGs is to address the ambiguity in natural languages: a single sentence can often have multiple valid parses, and PCFGs provide a mechanism to rank these parses based on their likelihood.

A PCFG consists of:
- A set of non-terminals (e.g., NP, VP, S, etc.).
- A set of terminals (words in the language).
- A set of production rules (e.g., S → NP VP).
- Probabilities associated with each production rule.

The probabilities of all rules that have the same left-hand side non-terminal should sum to 1.

The main advantage of using a PCFG over a regular CFG is its ability to <mark style="background: #BBFABBA6;">select the most probable parse tree</mark> for a given sentence, out of multiple possible trees. This is especially useful in natural language processing tasks like parsing where ambiguity is prevalent.

Let's consider a simplified grammar for the English language:
```
S → NP VP [0.9]
S → VP [0.1]
NP → "she" [0.5]
NP → "they" [0.5]
VP → "run" [0.7]
VP → "walk" [0.3]
```
In the above grammar:
- "S → NP VP" has a probability of 0.9.
- "S → VP" has a probability of 0.1.

If we want to parse the sentence "she run", there can be multiple valid parse trees based on the CFG rules. However, using the PCFG, we can assign a probability to each tree and <mark style="background: #FFF3A3A6;">select the most likely one</mark>.

PCFGs are commonly used in:
- Syntactic parsing of natural language sentences.
- Machine translation, to find the <mark style="background: #FFF3A3A6;">most probable translation</mark> given source and target language grammars.
- Speech recognition, to assign probabilities to different interpretations of a spoken sentence.

While PCFGs can be powerful tools, they also come with challenges:
- Estimating probabilities: Obtaining accurate probabilities for each rule can be challenging and requires a sufficiently large and representative training corpus.
- Overfitting: Like other probabilistic models, PCFGs can overfit to the training data, leading to poor generalization on unseen data.
- Computational complexity: Parsing with PCFGs can be computationally intensive, especially for long sentences.

When learning a PCFG from a treebank, the goal is to estimate the probabilities for each rule in the grammar. The **Maximum-Likelihood Estimation (MLE)** provides a straightforward method to determine these probabilities.

## Maximum-Likelihood Estimation for PCFG:

Given a rule \( \alpha \rightarrow \beta \):

- **Count(α → β)** is the number of times the rule \( \alpha \rightarrow \beta \) appears in the treebank.
  
- **Count(α)** is the total number of times the non-terminal \( \alpha \) appears as the left-hand side of any rule in the treebank.

The MLE estimate of the rule's probability, denoted $q^*(\alpha \rightarrow \beta)$, is calculated as:
$$q^*(\alpha \rightarrow \beta) = \frac{Count (\alpha \rightarrow \beta)}{Count (\alpha)}$$
This provides an estimate of how often, when we see the non-terminal $\alpha$, it produces the specific right-hand side $\beta$.

## An Example:

Let's say we want to estimate the probability of the rule NP → Det Noun using the Penn Treebank:

1. Suppose, after parsing the entire treebank, we find that **Count(NP → Det Noun) = 10,000**, meaning the rule NP → Det Noun appeared 10,000 times.

2. We also find that **Count(NP) = 20,000**, meaning the non-terminal NP appeared as the left-hand side of any rule 20,000 times in total.

3. Plugging into our formula:
$$q^*(NP \rightarrow Det Noun) = \frac{10,000}{20,000} = 0.5$$

Thus, based on the treebank data, there's a 50% chance that a noun phrase (NP) will expand to a determiner followed by a noun.

### Constituency Parsing

- **Purpose**: To build <mark style="background: #BBFABBA6;">a parse tree that represents the syntactic structure</mark> of a sentence according to a CFG.

- **Parse Tree**:
  - A hierarchical tree where each node represents a grammar rule application.
  - Leaves are the terminals (words of the sentence).
  - Internal nodes are nonterminals (syntactic categories).

- **Parsing Techniques**:
  - **Top-Down Parsing**: Starts at the start symbol and works down the tree, attempting to match the input sentence.
  - **Bottom-Up Parsing**: Starts with the input sentence and works up to the start symbol.
  - **Earley Parser**: A dynamic programming approach that efficiently parses sentences in a CFG.
  - **Chart Parsing**: Uses a data structure called a "chart" to store intermediate parsing results and avoid redundant parsing steps.

- **Evaluation Metrics**:
  - **Precision and Recall**: Measures of how many of the parse tree's constituents are correct according to a gold standard.
  - **F-Score**: The harmonic mean of precision and recall.

### Dependency Grammars

- **Definition**: A dependency grammar is a class of grammars where the structure of a sentence is described in terms of a binary asymmetric relation called "dependency" between a head (a governor) and its dependents (modifiers).

- **Structure**:
  - **Head**: The central word of a phrase which <mark style="background: #ABF7F7A6;">determines the syntactic type of that phrase</mark>.
  - **Dependent**: A word that <mark style="background: #FFF3A3A6;">modifies the head or adds some kind of information to it</mark>.
  - The head-dependent relations form a directed graph called a dependency tree, which is generally acyclic and connected.

- **Principles**:
  - Each word in a sentence except the root has exactly one head.
  - Dependency links do not cross each other, making the structure a tree.

In general, determining the head in a sentence or phrase depends on the syntactic rules of the language and the nature of the relationship between words. Here are some general guidelines for determining heads in English:

1. **In verb phrases (VP)**: The main verb is usually the head. E.g., in "She has been reading", "reading" is the head.
    
2. **In noun phrases (NP)**: The main noun is the head. E.g., in "the big red ball", "ball" is the head.
    
3. **In prepositional phrases (PP)**: The preposition is usually the head. E.g., in "on the table", "on" is the head.
    
4. **In adjective phrases (ADJP) and adverb phrases (ADVP)**: The main adjective or adverb is typically the head. E.g., in "very quickly", "quickly" is the head.
    
5. **In coordinate structures**: The conjunction is not the head; instead, the elements being coordinated are co-heads. E.g., in "John and Mary", neither "John" nor "Mary" is dependent on the other, and "and" is not the head.

### Dependency Parsing

- **Purpose**: To analyze the syntactic structure of a sentence by identifying dependency relations between "head" words and words which modify those heads.

- **Dependency Tree**:
  - A directed graph with nodes for each word in the sentence.
  - Edges indicate dependencies (the arrows point from heads to dependents).

- **Parsing Techniques**:
  - **Transition-Based Parsing**: Constructs a dependency tree using a series of actions (shift, reduce, left-arc, right-arc) guided by a stack and a buffer.
  - **Graph-Based Parsing**: Formulates parsing as a problem of finding the most probable dependency tree based on the entire sentence.
  - **Hybrid Approaches**: Combine aspects of both transition-based and graph-based methods.

- **Evaluation Metrics**:
  - **Unlabeled Attachment Score (UAS)**: Proportion of words in a sentence that have the correct head.
  - **Labeled Attachment Score (LAS)**: Proportion of words with both the correct head and the correct label for the dependency relation.

- **Projectivity**: 
In the context of dependency trees, "projectivity" refers to the property where, if a word A governs another word B (i.e., A is the head of B), then there are no words outside of the span between A and B in the sentence which are related (dependent) to any word inside this span. In simpler terms, if you draw the tree on top of the sentence, you don't have to draw any lines crossing over other lines.


Well-Formedness A dependency tree is well-formed iff
- Single head: Each word has only one head 
- Acyclic: The graph should be acyclic i.e. has no cycles 
- Connected: There is a path between any pair of nodes 
- Projective: if an edge from word A to word B implies that there exists a directed path in the graph from A to every word between A and B in the sentence

Sure! Let's dive a bit deeper into these two parsing algorithms:

### Transition-based Parsing

#### Overview:
Transition-based parsing, also known as shift-reduce parsing, operates incrementally. At each step, the parser decides between shifting the next word from the buffer onto the stack or reducing words already on the stack, forming a dependency.
#### Components:
- **Stack**: This is used for building the parse tree. Words or tokens are shifted onto the stack and later reduced to form dependencies.
  
- **Buffer**: Contains the tokens of the sentence that have yet to be parsed.
  
- **Parser**: Determines the action (i.e., shift, reduce, or other transitions) to take based on the current configuration of the stack and buffer.

#### Notable Features:
- Works best for capturing <mark style="background: #ABF7F7A6;">local dependencies because of its incremental nature</mark>.
- Is typically faster than graph-based methods as it processes the sentence linearly.
- Examples of this approach include the Nivre's arc-eager algorithm and the methods by Covington and Yamada & Matsumoto.

#### Example
**Initial State:**

- Stack: [ROOT]
- Buffer: [She, reads, books]

**Step 1:** Shift "She" onto the stack.

- Stack: [ROOT, She]
- Buffer: [reads, books]

**Step 2:** Shift "reads" onto the stack.

- Stack: [ROOT, She, reads]
- Buffer: [books]

**Step 3:** Create a dependency between "reads" (head) and "She" (dependent).

- Stack: [ROOT, reads]
- Buffer: [books]

**Step 4:** Shift "books" onto the stack.

- Stack: [ROOT, reads, books]
- Buffer: []

**Step 5:** Create a dependency between "reads" (head) and "books" (dependent).

- Stack: [ROOT, reads]
- Buffer: []

**Final Tree:** ROOT -> reads -> She, books

### Graph-based Parsing

#### Overview:
Graph-based parsing methods consider all possible trees for a given sentence and then search for the optimal tree, often defined as the tree with the highest total score based on a model.

#### Process:
- For a given sentence, construct a fully connected graph where the nodes are words, and potential edges represent possible dependencies.
  
- Each potential dependency is assigned a score based on a trained model.
  
- The goal is then to find the maximum spanning tree (MST) in this graph, which gives the optimal set of dependencies for the sentence.

#### Notable Features:
- Tends to be <mark style="background: #ADCCFFA6;">more accurate for longer sentences</mark>, where heads and dependents might be distant from each other.
- While they often yield better accuracy, especially for long sentences, they can be computationally more intensive than transition-based parsers.
- Examples include Eisner's algorithm, a variant of the CKY parsing algorithm adapted for dependency parsing, and the maximum spanning tree (MST) approach introduced by McDonald.

#### Example

For graph-based parsing, we'll <mark style="background: #ABF7F7A6;">create a fully connected graph</mark> where nodes are words and potential edges represent possible dependencies.

**Nodes:** ROOT, She, reads, books

**Potential Edges with hypothetical scores:**

- ROOT -> She (0.2)
- ROOT -> reads (0.8)
- ROOT -> books (0.1)
- She -> reads (0.7)
- She -> books (0.1)
- reads -> She (0.5)
- reads -> books (0.9)
- books -> She (0.1)
- books -> reads (0.3)

Now, the goal is to find a Maximum Spanning Tree (MST) from these potential edges. For simplicity's sake, let's consider the highest scoring edges:

- ROOT -> reads (0.8)
- reads -> She (0.5)
- reads -> books (0.9)

**Final Tree:** ROOT -> reads -> She, books

### Dependency Structures vs. Phrase Structures

Dependency structures explicitly represent 
- Head-dependent relations (directed arcs)
- Functional categories (arc labels)
- Predicate-argument structure 

Dependency structure independent of word order 
- Suitable for free word order languages, such as Indian languages • Phrase structures explicitly represent 
- Phrases (non-terminal nodes) 
- Structural categories (non-terminal labels) 
- Fragments are directly interpretable

### Dependency Corpora
- CoNLL dependencies https://aclanthology.org/D07-1096/
- Stanford typed dependencies http://nlp.stanford.edu/software/dependencies_manual.pdf
- Universal dependencies https://universaldependencies.org/u/overview/syntax.html

---

Constituency tree (phrase structure tree) https://parser.kitaev.io/

Dependency tree https://explosion.ai/demos/displacy