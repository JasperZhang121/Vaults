### **Language Models:**

**1. Introduction to Language Models:**
- **Definition:** A language model is a <mark style="background: #FFF3A3A6;">probabilistic framework</mark> that can predict the next word in a sentence given the previous words (n-grams), or can assign probabilities to sequences of words.
- **Use Cases:** They are used in various applications such as speech recognition, machine translation, text generation, and auto-complete features.

#### **Chain rule of probability:**
$P(x1, x2, …, xl ) = P(x1) P(x2 | x1) P(x3 | x1, x2) ⋯ P(xl| x1, x2, …, xl−1)$

**Example:**
	P(John Smith’s hotel room bugged) = P(John) P(Smith’s | John) P(hotel | John Smith’s) … P(bugged | John Smith’s hotel room)

#### **Estimate the Probabilities:**
Maximum likelihood estimation (MLE): 
P(bugged | John Smith’s hotel room) = count(John Smith’s hotel room bugged)/count(John Smith’s hotel room)

**Markov Assumption:**
Simplification:
P(bugged| John Smith's hotel room) ≈ P(bugged|room) 
or
P(bugged| John Smith's hotel room) ≈ P(bugged|hotel room)

#### **Unigram Model-Zero-order Markov assumption:**
$P(x_1, x_2,…, x_l) = P(x_1) \times \prod_{i=1}^{l} P(x_i)$
#### **Bigram Model - First-order Markov assumption:**
$P(x_1, x_2,…, x_l) = P(x_1) \times \prod_{i=2}^{l} P(x_i | x_{i-1})$

P("I want to eat Chinese food")=P("I")×P("want"∣"I")×P("to"∣"want")×P("eat"∣"to")×P("Chinese"∣"eat")×P("food"∣"Chinese")

**Compute Bigram Probabilities:**
Maximum likelihood estimation:
$P(x_i | x_{i-1}) = \frac{\text{count}(x_{i-1}, x_i)}{\sum_x \text{count}(x_{i-1}, x)} = \frac{\text{count}(x_{i-1}, x_i)}{\text{count}(x_{i-1})}$

**Log probabilities (logarithm helps with numerical stability):**
```
logP("<s> I want to eat Chinese food </s>")
=logP("I"∣<s>")+logP("want"∣"I")+logP("to"∣"want")+logP("eat"∣"to")+logP("Chinese"∣"eat")+logP("food"∣"Chinese")+logP("</s>"∣"food")
```
#### **Trigram Models**
$P(w_1, w_2, ..., w_n) = P(w_1) \times P(w_2 | w_1) \times \prod_{i=3}^{n} P(w_i | w_{i-2}, w_{i-1})$
