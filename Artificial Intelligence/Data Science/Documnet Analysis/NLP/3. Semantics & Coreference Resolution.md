
- Semantics deals with the <mark style="background: #FFB86CA6;">study of meaning in language</mark>. It aims to understand how words, phrases, and sentences convey meaning, both in isolation and when combined in context.
- Modelling semantics is the holy grail of NLP and a central question in Artificial Intelligence
	- Building a robot that can follow natural language instructions to execute tasks
	- Answering questions, such as where is the nearest coffee shop? 
	- Translating a sentence from one language into another, while preserving the underlying meaning 
	- Fact-checking an article by searching the web for contradictory evidence 
	- Logic-checking an argument by identifying contradictions, ambiguity, and unsupported assertions
- Semantic theories explain how to linguistically represent meaning: 
	- Logical semantics 
	- Lexical semantics

### **Logical Semantics:**
- Focuses on representing the meaning of sentences in a formal system, often using symbolic logic.
- Constructs like propositions, truth values (true or false), and logical operators (AND, OR, NOT) are used.
- Examines how the truth values of larger expressions depend on the truth values of their parts.

### **Predicate-Argument Semantics:**
- Concerns itself with predicates and their arguments.
  - Predicates: Verbs or adjectives that indicate properties or actions.
  - Arguments: Entities (often nouns) that participate in the actions or have the properties described by the predicates.
- For instance, in "John reads a book", "reads" is the predicate, and "John" and "a book" are its arguments.

check:
- [[1. Propositional Logic]]
- [[2. First Order Logic]]
### **Lexical Semantics:**
- Explores the meaning of words and the relationships between them.
- Studies phenomena like synonymy (words with similar meanings, e.g., "happy" and "joyful"), antonymy (opposite meanings, e.g., "happy" and "sad"), and hyponymy (hierarchical relationships, e.g., "sparrow" is a hyponym of "bird").
- Also includes the study of word senses and ambiguity.

Homonymy: coincidentally share an orthographic form. e.g. bank
- The bank took my deposit. (financial institution) 
- The bank was grassy. (sloping mound) 

Polysemy: two senses are semantically related. e.g. solution 
- Work out the solution in your head. 
- Heat the solution to 75° Celsius.  

Homophone: same pronunciation, but different spellings.  e.g., wood / would, to / two 

Homograph: same orthographic form, but different pronunciation (this is a problem in speech synthesis). e.g. bass
- I like to play the bass (a musical instrument – bass guitar)
- Fresh bass is tasty (a fish)

Synonyms are two word lemmas that are identical or nearly identical in meaning.
- Example: couch / sofa
- Example: car / automobile

Antonyms are two word lemmas that have opposite meanings.
- Example: long / short
- Example: big / little
- Example: rise / fall
- Example: in / out

A hyponym is a word whose meaning is more specific than another word.
- Example: car (hyponym of vehicle)
- Example: dog (hyponym of animal)
- Example: mango (hyponym of fruit)

A hypernym is a word that is more general than another word, essentially its class.
- Example: vehicle (hypernym of car)
- Example: animal (hypernym of dog)
- Example: cheese (hypernym of brie)

Distributional Hypothesis
- Distributional semantics are computed from context statistics (e.g. Brown clusters)
- Distributed semantics represent meaning by numerical vectors, rather than symbolic structures (e.g. word2vec, which is also distributional)|

Brown clustering
- Brown clustering induces <mark style="background: #FFB86CA6;">hierarchical representations</mark> using cluster mergers that optimise an objective defined on word occurrence counts.  (no distributed vectors used)
- Some Learning algorithms like CRF and Perceptron perform better with discrete feature vectors.
- Discrete representations can be distilled from word vectors by clustering.

Embeddings
- Word embedding: Ensure words with similar context occupy close spatial positions in multidimensional space, as represented by the vector representations
- Character embedding: same idea, but encode at the character level to overcome unseen words (out-of-vocabulary)
- Sentence embedding: learn representations for sentences

### **Coreference Resolution:**
- Focuses on identifying which nouns in <mark style="background: #FFB86CA6;">a text refer to the same entities</mark> in the real world.
- An example is resolving "he" to "John" in the sentences: "John went to the park. He sat on a bench."
- Essential for understanding context and maintaining coherence in discourse.


#### Referring Expression

"Tim Cook has jetted in for talks with officials as [he] seeks to clear up a pile of problems."

Pronouns
- Search for candidate antecedents: any noun phrase is the proceeding text is a candidate
- Match against hard agreement constraints
	- he: singular, masculine, animate, third person  
	- Tim Cook: singular, masculine, animate, third person
	- officials: plural, animate, third person 
	- talks: plural, inanimate, third person

Select using heuristics
- Recency – close in the sentence
- Subject over objects – nouns in the subject position (in a dependency parse) are more likely to be coreferent


"Apple Inc Chief Executive [Tim Cook] has jetted into China ... [Cook] is on his first business trip to the country..."

Proper Nouns
- Proper nouns often corefers with another proper nouns
- Strategies (easier to solve):
	- Match the syntactic head words of the reference with the referent
	- In machine learning, a solution is to include a range of matching features: exact match, head match, and string inclusion.
	- Gazetteers of acronyms (Australian National University / ANU), and other aliases (Queensland Technology University / Queensland Tech)


"The firm [Apple Inc]; the firm’s biggest growth market [China]; and the country [China]"

Nominals
- Nominals are noun phrases that are not pronouns nor proper nouns.
- Hard to solve, as it requires world knowledge:
	- Apple Inc. is a firm 
	- China is a growth market

#### Algorithms for Coreference Resolution

Identifying spans of text mentioning entities:
- Get noun phrases from a sentence structure (e.g. using constituency parsing),
- and filter using simple rules (e.g. remove numeric entities, remove nested noun phrases).

Clustering those mentions:
- Mentioned-based models: supervised learning or ranking 
- Entity-based models: clustering

Clustering Mentions: Classification (mentioned-based models):
Mention-pair models
- Binary label $y_{ij}$ is assigned to each pair of mentions (i, j), i < j If i and j corefer, then $y_{ij}$ = 1, otherwise $y_{ij}$ = 0
- Use any <mark style="background: #FFF3A3A6;">off-the-shelf classifier to solve this binary classification problem</mark>. 
- Also need to use heuristics to construct coherent groups for each entity.

Clustering Mentions: Ranking (Mention-based models):
In this model, instead of looking at pairs of mentions and deciding if they co-refer or not (as in the mention-pair model), we instead look at each mention and decide which previous mention (if any) it refers to. The major difference is that we're now ranking possible antecedents for a given mention rather than just classifying pairs as co-referent or not.

Example:

1. **For Each Mention `i`**:
    - Consider all mentions before `i` as possible antecedents, along with the possibility that `i` doesn't refer to any of them (`ε`).
2. **Score Each Possible Antecedent**:
    - Use a scoring function `ψM(a, i)` to score the suitability of each potential antecedent `a` for mention `i`.
3. **Choose Best Antecedent**:
    - The antecedent `â` for mention `i` is chosen as the one that maximizes the score: `â=argmax ψ (a,i) for a∈{ε,1,2,...,i−1}`.
        - If `â` is `ε`, then mention `i` doesn't refer to any previously mentioned entity and is potentially a new entity.
4. **Training**:
    - The ranking model can be trained using standard objectives used in discriminative classification. For instance, if we have ground truth antecedents for our mentions, we can calculate the loss between our predicted antecedents and the true ones. This loss can then be minimized using methods like hinge loss or negative log-likelihood.

Clustering Mentions: Entity-based models
- These models aim to predict whether a new mention belongs to an existing entity (cluster of mentions) or if it should start a new entity.
- Entity-based model require a scoring function at the entity level
- To implement this in practice requires some form of search to group mentions so that they can be scored:
	- Brute force  
	- Incrementally build up clusters  
	- Actions learnt by reinforcement learning

"Jane is a software engineer. She works at a big tech company. Jane loves coding and often spends her weekends working on personal projects."

Step-by-step Coreference Resolution:

1. **Initialization:** Start with no entities.
    
2. **First Mention - "Jane"**
    
    - No existing entities, so create a new entity `E1 = {"Jane"}`.
3. **Second Mention - "She"**
    
    - Compare "She" to all existing entities.
    - "She" is likely a pronoun referring to a female entity. The entity `E1` contains "Jane", which matches in gender and number.
    - Assign "She" to `E1`. Now, `E1 = {"Jane", "She"}`.
4. **Third Mention - "Jane"**
    
    - Compare "Jane" to all existing entities.
    - It directly matches with the mention in `E1`.
    - Assign "Jane" to `E1`. Entity remains `E1 = {"Jane", "She", "Jane"}`.
5. **End of Text**:
    
    - Final Entities: `E1 = {"Jane", "She", "Jane"}`.

From the text, the entity-based model successfully recognized that "Jane" and "She" refer to the same person and grouped them under a single entity `E1`.

This is a straightforward example. In more complex texts, mentions might not be this clear, and the model would need to rely heavily on the accumulated features of entities, context, and other linguistic cues to make accurate clustering decisions.

#### Mention embedding
A concept in natural language processing (NLP) where mentions (words or phrases referring to entities) in the text are represented as continuous vector spaces. These embeddings capture the semantics and context of the mentions and can be used in a variety of NLP tasks, including coreference resolution, entity linking, and more.

One approach is inspired in embedding multi-word expressions. Run a bidirectional LSTM over the entire text, obtaining hidden states from the left-to-right and right-to-left passes

- Each candidate mention span (s, t) is then represented by the vertical concatenation of four vectors:  u(s,t) = [u(s,t) ; u(s,t) ; u(s,t) ; φ(s,t)]

first last head  
where u(s,t) = hs+1 is the embedding of the first word in the span,

(s,t) first  
ulast = ht is the embedding of the last word,

u(s,t) is the embedding of the head word, head

and φ(s,t) is a vector of surface features, such as the length of the span


