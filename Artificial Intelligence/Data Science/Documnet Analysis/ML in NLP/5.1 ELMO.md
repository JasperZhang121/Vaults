### Embeddings from Language Models

**1. Introduction:**
- **Definition:** ELMo (Embeddings from Language Models) represents contextualized word embeddings generated from a deep bidirectional language model.
- **Developers:** Developed by researchers at the Allen Institute for Artificial Intelligence in 2018.

**2. Key Features:**

- **Contextualized Embeddings:** Unlike traditional word embeddings (e.g., Word2Vec, GloVe) that assign a static embedding to each word, <mark style="background: #ABF7F7A6;">ELMo provides embeddings based on the word's context in a sentence</mark>. This means that the word "bat" will have different embeddings in "baseball bat" and "night bat".
  
- **Deep and Bidirectional:** ELMo utilizes a bidirectional LSTM (BiLSTM) to consider both the left and the right context in all layers, resulting in a rich representation of words.

**3. How ELMo Works:**

1. **Pre-training on Language Model Task:** A <mark style="background: #CACFD9A6;">deep bidirectional language model</mark> is trained on a large corpus. This captures both semantic and syntactic information.
  
2. **Extraction of Embeddings:** For any input sentence, embeddings are extracted from all layers of the BiLSTM. These embeddings are a function of the entire input sentence.
   
3. **Weighted Sum:** The embeddings from different layers are combined into a single word representation using a weighted sum. These weights are learned for the specific downstream task.

**4. Advantages:**

- **State-of-the-Art Performance:** When introduced, ELMo achieved state-of-the-art results on several NLP benchmarks across tasks like question answering, sentiment analysis, and named entity recognition.
  
- **Transfer Learning:** ELMo embeddings, being pretrained on a large corpus, can be effectively transferred to various NLP tasks, reducing the need for extensive task-specific data.
  
- **Handles Polysemy:** ELMo can generate different embeddings for a word based on its context, addressing the challenge of polysemous words.

**5. Usage:**

- **Task-specific Models:** ELMo embeddings can be easily incorporated into existing models for a variety of NLP tasks. These embeddings can serve as additional input features to enhance model performance.
  
- **Fine-tuning:** Although ELMo is primarily used as a feature-based approach, the embeddings can also be fine-tuned for specific tasks, if necessary.

**6. Evolution in NLP Landscape:**

- ELMo represented a shift in the NLP community towards the use of deeply contextualized word embeddings. After its introduction, similar models like OpenAI's GPT and Google's BERT emerged, further pushing the boundaries of what's possible in NLP.
