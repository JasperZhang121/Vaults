Neural Network Language Models, often abbreviated as NNLMs, represent a shift from traditional statistical methods of language modeling like n-gram models. Instead of counting word frequencies, NNLMs learn continuous representations of words and leverage these representations to predict the likelihood of word sequences.

**Functions of a Neural Network Language Model:**

1. **Text Generation**: One of the most popular applications of NNLMs is in <mark style="background: #ABF7F7A6;">generating text</mark>. Given a seed phrase or starting sequence, the model can produce a continuation of that sequence, attempting to mimic the style and coherence of human language.

2. **Likelihood Estimation**: NNLMs can estimate the likelihood or probability of a given text sequence. This is useful in various applications, such as machine translation or speech recognition, where determining the most likely sequence can enhance the accuracy of the system.

**Auto-regressive Modeling:**

The term "auto-regressive" in the context of language models refers to the process of <mark style="background: #ABF7F7A6;">using previous tokens (words or subwords) in a sequence to predict the next token</mark>.

**Steps in Auto-regressive Neural Network Language Modeling:**

1. **Token Representation**: Each word/token is typically represented as a high-dimensional vector. Initially, these vectors might be simple embeddings like word2vec or GloVe, but as the model is trained, these vectors (or embeddings) are fine-tuned to capture semantic and syntactic information.

2. **Sequence Input**: Given a sequence of words (e.g., "I have a"), these words are converted into their corresponding vectors and fed into the neural network.

3. **Model Architecture**: The architecture of the neural network can vary. Recurrent Neural Networks (RNNs), Long Short-Term Memory networks (LSTMs), and Transformer-based models like GPT and BERT are common choices. These architectures are designed to capture the sequential nature of language, remembering context from previous tokens to aid in prediction.

4. **Prediction**: <mark style="background: #BBFABBA6;">The model's goal is to predict the next word in the sequence</mark>. For the example "I have a", the model might predict "dream" as the next word. The model outputs a probability distribution over the entire vocabulary, and the word with the highest probability is chosen as the prediction.

5. **Training**: During training, the model is provided with a large corpus of text. It uses the actual next word in the sequence to adjust its weights (via backpropagation) and get better at its predictions. The aim is to minimize the difference between its predicted probabilities and the actual outcomes (known as the loss).

6. **Inference**: During inference (or when the model is being used post-training), the predicted word can be fed back into the model as part of the input to predict the subsequent word, allowing for continued text generation.

Neural Network Language Models, especially those based on the auto-regressive approach, have revolutionized various NLP tasks. Their ability to generate coherent text and predict the likelihood of sequences makes them integral to modern NLP systems, from chatbots to translation tools. As technology evolves, we can anticipate even more sophisticated and efficient models that push the boundaries of what machines can understand and produce in terms of human language.