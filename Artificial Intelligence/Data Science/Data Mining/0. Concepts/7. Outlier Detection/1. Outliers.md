**Outlier:**

-   A data object that **deviates significantly** from the normal objects as if it were <mark style="background: #ABF7F7A6;">generated by a different mechanism</mark>.
-   Example Unusual credit card purchase, Sports stars, Tax frauds
-   Outliers are essentially the same as _anomalies_ (or, at least, there is no widely accepted distinction).
-   Outliers are <mark style="background: #ADCCFFA6;">different from noise data</mark>: Noise is random error or variance in a measured variable
-   <mark style="background: #FFB8EBA6;">Noise should be removed before outlier detection</mark>.
-   Outliers are _interesting_:  They violate the mechanism that generates the normal data
-   Outlier detection vs. _novelty_ detection: early stage is an outlier; but later could be _novelty_ and then merged into the model

**Applications:**
- Credit card fraud detection - fraudulent transactions  
- Telecom fraud detection -stolen phones  
- Customer segmentation
- Medical research
- Students at risk of failing -> novelty detection?  
- National security
- Network intrusion detection

**Types: Global, Contextual or Collective**
-   A data set may have multiple <mark style="background: #ADCCFFA6;">types of outlier</mark>.
-   One object may belong to more than one type of outlier.

**Global outlier** (or point anomaly)
-   Object is **O**g if it <mark style="background: #D2B3FFA6;">significantly deviates from the rest of the data set</mark>
-   e.g. Intrusion detection in computer networks
-   Issue: Find an appropriate measurement of deviation

**Contextual outlier**
-   Object is **Oc** if it <mark style="background: #ABF7F7A6;">deviates significantly based on a selected context</mark>
-   e.g.  0 o C in Canberra  outlier? (depends if it is summer or winter)
-   Attributes of data objects should be divided into two groups:  
    -   _Contextual_ attributes: <mark style="background: #D2B3FFA6;">defines the context</mark>, e.g., time & location
    -   _Behavioural_ attributes:  <mark style="background: #FFB86CA6;">characteristics of the object</mark>, used in outlier determination, e.g., temperature
-   Can be viewed as a generalization of _local outliers_—whose density significantly deviates from its local area whre the local area is defined by contextual attributes
-   Issue: How to define or formulate meaningful context?


**Challenges in Outlier Detection**

-   Modeling normal objects and outliers properly
    -   <mark style="background: #FFB8EBA6;">Hard to enumerate all possible normal behaviours</mark> in an application
    -   The border between normal and outlier objects is often a <mark style="background: #ABF7F7A6;">grey area</mark>
    -   Can assign a data object into class "normal" vs "outlier" , or assign an "outlier-ness" measure
-   Application-specific outlier detection
    -   Choice of <mark style="background: #FFB86CA6;">distance</mark> measure among objects and the model of relationship among objects are often <mark style="background: #ADCCFFA6;">application-dependent</mark>
    -   e.g., clinical data: a small deviation could be an outlier; while in marketing analysis, larger fluctuations are expected
    -   Applications associate <mark style="background: #FFF3A3A6;">different costs with detecting or missing an outlier</mark>.
    -   Difficult to develop <mark style="background: #BBFABBA6;">generic, application independent outlier detection methods</mark>
-   Handling noise in outlier detection
    -   Noise may distort the normal objects and blur the distinction between normal objects and outliers.  It may help outliers to hide and reduce the effectiveness of outlier detection. It is best practice to remove noise first. Sometimes noise can <mark style="background: #ADCCFFA6;">be removed systematically using knowledge of the data-collection process (such as typos, sensor drift, or instrumentation errors)</mark>.    
-   Interpretability
    -   Understand why these are outliers: Justification of the detection
    -   Specify the degree of an outlier: the unlikelihood of the object being generated by a normal mechanism

One of the really challenging problems in outlier detection is <mark style="background: #ADCCFFA6;">turning the human perception of "I know one when I see it" into an objective algorithm that knows one when it sees it</mark>. Unlike other mining problems, there are very few accepted objective quality measures of outlier-detection. Generally, the best we can do is to assume that an outlier is a good one if it ranks most highly compared to other potential outliers in the same data set, according to the measure we use to define outliers. If, subjectively, we don't like what we see, we try a different method or a different measure or perhaps decide we have no outliers.

### Statistical Approaches

Statistical methods (also known as _model-based methods_) assume that the normal data follow some statistical model (a <mark style="background: #FFB86CA6;">stochastic</mark> model). The data not following the model  (i.e. in <mark style="background: #ADCCFFA6;">low probability regions</mark> of the model) are deemed to be outliers.

First use Gaussian  (also called normal) distribution to model the normal data:
- For each object in region R, estimate $g_{D}$, the probability that  y  fits the Gaussian distribution
- If  $g_{D}$  is very low, y  is unlikely  to be generated by the Gaussian model, thus an outlier

**Effectiveness**
- The effectiveness of these  statistical methods highly depends on whether the <mark style="background: #D2B3FFA6;">assumption of a statistical model holds in the real data</mark>. Analysts may need to understand something about the underlying data-generation process, perhaps a physical process,  to select a suitable statistical model to use. The fit of the data to the model must be validated (and this is ironic because we are setting out to find what does _not_ fit).  

- There are many alternatives to statistical models available for this method, e.g., parametric vs. non-parametric.

**Parametric methods**
- Assumes that the non-outlying  data is generated by a parametric distribution with parameter $\theta$. e.g. Gaussian as above.
- The probability density function of the parametric distribution f(x,$\theta$) gives the probability that object x  is generated by the distribution. The smaller this value, the more likely x is an outlier.

Some methods based on fitting  one or more Gaussian distributions, are covered here.

**Non-parametric methods**
- Non-parametric methods do not assume an a-priori statistical model and determine the model from the input data.

- They are not completely parameter-free but consider the number and nature of the parameters are flexible and not fixed in advance.

**Histogram non-parametric method**  

- The parameters to be <mark style="background: #D2B3FFA6;">determined are bin width and bin boundaries</mark>. A simple threshold can then be applied to determine outliers as those objects falling in a low-frequency bin.   Three problems occur:

	-   The method <mark style="background: #FFF3A3A6;">is very sensitive to the arbitrary bin boundaries</mark>. An outlier can fall outside a low-frequency class due only  to the choice of starting point for binning along the x axis (and vice-versa).  
	-   Too small bin size → normal objects in empty/rare bin, ie, false positive
	-   Too big bin size → outliers in some frequent bins, ie, false negative

**Kernel Density estimation non-parametric method**
- Kernel density estimation fits a _smoothing_ function to estimate a probability density distribution, to <mark style="background: #ABF7F7A6;">achieve something like a smoothed histogram</mark>. If the density estimated by the smoothed distribution is low in some region, that is, below some threshold, then the objects in the region are considered outliers.
- The determination of data points to become outliers is dependent on the parameters to the fitting process (parameters similar to bin size and bin boundaries for histograms), but offers a smoother boundary behaviour than the discrete histogram.