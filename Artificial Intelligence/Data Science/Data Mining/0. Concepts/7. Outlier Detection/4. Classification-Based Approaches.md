
If we have a labelled training set of outliers and non-outliers, then classification methods can be used. However, the training set would typically be heavily biased in favour of non-outlying data, so classification has to be sensitive to this asymmetry of classes.

### **One-class model: A classifier is built to describe only the normal class**

- Learn the decision boundary of the normal class using classification methods such as a <mark style="background: #ADCCFFA6;">one-class variant of SVM</mark>. Using the diagram above, only those points inside the curve would be used for training, or else we assume all the training data we have is "normal". 
- At run-time, any points that do not belong to the normal class (i.e. not within the decision boundary) are declared to be outliers.

- Advantage: Can easily detect new outliers that were not close to any outlier objects in the training set

- Extension:  Can also have normal objects may belong to multiple classes, and can be more selective if labels available for training


### **Strengths and Weaknesses of Classification-based approaches**

- Strength: human knowledge can be incorporated by selection of training data

- Strength: Outlier detection is fast

- Bottleneck: Quality heavily depends on the availability and quality of the training set, but often difficult to obtain representative and high-quality training data





