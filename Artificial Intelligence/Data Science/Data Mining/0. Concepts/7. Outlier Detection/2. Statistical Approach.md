### Statistical Approaches

Statistical methods (also known as _model-based methods_) assume that the normal data follow some statistical model (a <mark style="background: #FFB86CA6;">stochastic</mark> model). The data not following the model  (i.e. in <mark style="background: #ADCCFFA6;">low probability regions</mark> of the model) are deemed to be outliers.

First use Gaussian  (also called normal) distribution to model the normal data:
- For each object in region R, estimate $g_{D}$, the probability that  y  fits the Gaussian distribution
- If  $g_{D}$  is very low, y  is unlikely  to be generated by the Gaussian model, thus an outlier

**Effectiveness**
- The effectiveness of these  statistical methods highly depends on whether the <mark style="background: #D2B3FFA6;">assumption of a statistical model holds in the real data</mark>. Analysts may need to understand something about the underlying data-generation process, perhaps a physical process,  to select a suitable statistical model to use. The fit of the data to the model must be validated (and this is ironic because we are setting out to find what does _not_ fit).  

- There are many alternatives to statistical models available for this method, e.g., parametric vs. non-parametric.

**Parametric methods**
- Assumes that the non-outlying  data is generated by a parametric distribution with parameter $\theta$. e.g. Gaussian as above.
- The probability density function of the parametric distribution f(x,$\theta$) gives the probability that object x  is generated by the distribution. The smaller this value, the more likely x is an outlier.

Some methods based on fitting  one or more Gaussian distributions, are covered here.

**Non-parametric methods**
- Non-parametric methods do not assume an a-priori statistical model and determine the model from the input data.

- They are not completely parameter-free but consider the number and nature of the parameters are flexible and not fixed in advance.

**Histogram non-parametric method**  

- The parameters to be <mark style="background: #D2B3FFA6;">determined are bin width and bin boundaries</mark>. A simple threshold can then be applied to determine outliers as those objects falling in a low-frequency bin.   Three problems occur:

	-   The method <mark style="background: #FFF3A3A6;">is very sensitive to the arbitrary bin boundaries</mark>. An outlier can fall outside a low-frequency class due only  to the choice of starting point for binning along the x axis (and vice-versa).  
	-   Too small bin size → normal objects in empty/rare bin, ie, false positive
	-   Too big bin size → outliers in some frequent bins, ie, false negative

**Kernel Density estimation non-parametric method**
- Kernel density estimation fits a _smoothing_ function to estimate a probability density distribution, to <mark style="background: #ABF7F7A6;">achieve something like a smoothed histogram</mark>. If the density estimated by the smoothed distribution is low in some region, that is, below some threshold, then the objects in the region are considered outliers.
- The determination of data points to become outliers is dependent on the parameters to the fitting process (parameters similar to bin size and bin boundaries for histograms), but offers a smoother boundary behaviour than the discrete histogram.

----

### Parametric Methods: Univariate Outliers from a Normal Distribution

**General approach to determine outliers based on only one variable**

For outliers based on **only one numeric variable** (attribute):

1.  Assume data is generated by an underlying _Gaussian/normal_ distribution (or choose some other distribution).
2.  Set parameters of the distribution from input data, using e.g. maximum likelihood
3.  Identify data points of low probability according to the fitted distribution as outliers

##### Methods

**1. Using IQR**
Earlier we identified outliers for [[2.2 Basic Statistical Descriptions of a Single Data Variable]], defined as values below Q1 − 1.5 IQR or above Q3 + 1.5 IQR. The same method for determination of outliers can be used more generally here.


**2. Using maximum likelihood: Outliers are > 3 stddevs from the mean**

Example:

Consider the following data for temperature in Canberra at noon over 11 days:

{24.0, 28.9, 28.9, 28.9, 29.0, 29.1, 29.1, 29.2, 29.2, 29.3, 29.4}.

1. We will assume a normal distribution that is defined by

$${\displaystyle f(x\;|\;\mu ,\sigma ^{2})={\frac {1}{\sqrt {2\pi \sigma ^{2}}}}\;e^{-{\frac {(x-\mu )^{2}}{2\sigma ^{2}}}}}$$

with parameters: mean \mu and  variance  \sigma^2.

2. We will set these parameters to fit the distribution to our data by maximising likelihood.

Likelihood is defined as
$$p(x_1, ..., x_N | \mu, \sigma^2) = \prod_{n=1}^{N} \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x_n - \mu)^2}{2\sigma^2}\right)$$

This can be maximised by taking derivatives of the log of the likelihood to get estimates for the maximum likelihood mean and maximum likelihood variance respectively.

Using the temperatures in Canberra with  n = 11, we get   $\hat{\mu} = 28.636$  and    $\hat{\sigma}= \sqrt{2.175} = 1.474$

3. Now we can determine outliers as points of low probability by a heuristic such as:

_An <mark style="background: #FFF3A3A6;">outlier is any value outside 3 estimated standard deviations</mark> from the estimated mean._

So outliers fall outside $28.64 \pm (3 \times 1.474 ) = [24.21, 33.06]$
In our temperature data, 24.0 is the only value that falls outside this range, and therefore 24.0 is an outlier.

**3. Using Grubb's test**

Grubb's test, also called the maximum normed residual test is an alternative  heuristic to identify  data points of low probability according to the fitted normal distribution as outliers. Strictly, the statistical assurance comes when both the data follows a normal distribution and at most one outlying value exists. In this formulation, we permit the outlying values to be at either the minimum or the maximum  ends of the data, and so we use the two-sided t-test, and both the minimum and maximum should be tested. In practice, all values will be tested (or more efficiently, all values starting from each end until a non-outlier from each end is found).

Consider a dataset of N  data points, with standard deviation s.

You need to choose an \alpha to apply this test, and by that determine the significance you want. This is not unlike having to choose the number of standard deviations in the maximising likelihood method above. A value of \alpha = 0.05 is typical.

Define the z-score for  a data point x as z(x)=\frac{\mid x-\bar{x}\mid}{s}, that is, the point's distance from the sample mean as a proportion of the standard deviation.

Then, according to Grubb's test  x is an outlier if

z(x) \geq \frac{N-1}{\sqrt{N}} \sqrt{\frac{t_{\alpha/(2N),N-2}^2}{N - 2 + t_{\alpha/(2N),N-2}^2}}
where t_{\alpha/(2N),N-2}  is the critical value taken by a t-distribution with degrees of freedom N-2  at a significance level of \alpha/(2N).