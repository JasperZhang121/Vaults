Statistical methods (also known as _model-based methods_) assume that the normal data follow some statistical model (a <mark style="background: #FFB86CA6;">stochastic</mark> model). The data not following the model  (i.e. in <mark style="background: #ADCCFFA6;">low probability regions</mark> of the model) are deemed to be outliers.

#### Use Gaussian distribution to model the normal data:
- For each object in region R, estimate $g_{D}$, the probability that  y  fits the Gaussian distribution
- If  $g_{D}$  is very low, y  is unlikely  to be generated by the Gaussian model, thus an outlier

### **Effectiveness**

- The effectiveness of these statistical methods highly depends on whether the <mark style="background: #D2B3FFA6;">assumption of a statistical model holds in the real data</mark>. Analysts may need to understand something about the underlying data-generation process, perhaps a physical process,  to select a suitable statistical model to use. The fit of the data to the model must be validated (and this is ironic because we are setting out to find what does _not_ fit).  

- There are many alternatives to statistical models available for this method, e.g., parametric vs. non-parametric.

### **Parametric methods**

- Assumes that the non-outlying data is generated by a parametric distribution with parameter $\theta$. e.g. Gaussian as above.
- The probability density function of the parametric distribution f(x,$\theta$) gives the probability that object x  is generated by the distribution. The smaller this value, the more likely x is an outlier.

Some methods based on fitting one or more Gaussian distributions, are covered here.

### **Non-parametric methods**
- Non-parametric methods do not assume an a-priori statistical model and determine the model from the input data.
- They are not completely parameter-free but consider the number and nature of the parameters are flexible and not fixed in advance.

#### **Histogram non-parametric method**  

- The parameters to be <mark style="background: #D2B3FFA6;">determined are bin width and bin boundaries</mark>. A simple threshold can then be applied to determine outliers as those objects falling in a low-frequency bin.   Three problems occur:

	-   The method <mark style="background: #FFF3A3A6;">is very sensitive to the arbitrary bin boundaries</mark>. An outlier can fall outside a low-frequency class due only to the choice of starting point for binning along the x axis (and vice-versa).  
	-   Too small bin size → normal objects in empty/rare bin, ie, false positive
	-   Too big bin size → outliers in some frequent bins, ie, false negative

#### **Kernel Density estimation non-parametric method**
- Kernel density estimation fits a _smoothing_ function to estimate a probability density distribution, to <mark style="background: #ABF7F7A6;">achieve something like a smoothed histogram</mark>. If the density estimated by the smoothed distribution is low in some region, that is, below some threshold, then the objects in the region are considered outliers.
- The determination of data points to become outliers is dependent on the parameters to the fitting process (parameters similar to bin size and bin boundaries for histograms), but offers a smoother boundary behaviour than the discrete histogram.

----

### Parametric Methods: Univariate Outliers from a Normal Distribution

**General approach to determine outliers based on only one variable**

#### Outliers with **only one numeric variable** (attribute):
1.  Assume data is generated by an underlying _Gaussian/normal_ distribution (or choose some other distribution)
2.  Set parameters of the distribution from input data, using e.g. maximum likelihood
3.  Identify data points of low probability according to the fitted distribution as outliers

#### Methods

##### **1. Using IQR**
Earlier we identified outliers for [[2.2 Basic Statistical Descriptions of a Single Data Variable]], defined as values below Q1 − 1.5 IQR or above Q3 + 1.5 IQR. The same method for determination of outliers can be used more generally here.

##### **2. Using maximum likelihood: 
Outliers are > 3 stddevs from the mean**

Example:

Consider the following data for temperature in Canberra at noon over 11 days:

{24.0, 28.9, 28.9, 28.9, 29.0, 29.1, 29.1, 29.2, 29.2, 29.3, 29.4}.

1. We will assume a normal distribution that is defined by

$${\displaystyle f(x\;|\;\mu ,\sigma ^{2})={\frac {1}{\sqrt {2\pi \sigma ^{2}}}}\;e^{-{\frac {(x-\mu )^{2}}{2\sigma ^{2}}}}}$$

with parameters: mean \mu and  variance  \sigma^2.

2. We will set these parameters to fit the distribution to our data by maximising likelihood.

Likelihood is defined as
$$p(x_1, ..., x_N | \mu, \sigma^2) = \prod_{n=1}^{N} \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x_n - \mu)^2}{2\sigma^2}\right)$$

This can be maximised by taking derivatives of the log of the likelihood to get estimates for the maximum likelihood mean and maximum likelihood variance respectively.

Using the temperatures in Canberra with  n = 11, we get   $\hat{\mu} = 28.636$  and    $\hat{\sigma}= \sqrt{2.175} = 1.474$

3. Now we can determine outliers as points of low probability by a heuristic such as:

_An <mark style="background: #FFF3A3A6;">outlier is any value outside 3 estimated standard deviations</mark> from the estimated mean._

So outliers fall outside $28.64 \pm (3 \times 1.474 ) = [24.21, 33.06]$
In our temperature data, 24.0 is the only value that falls outside this range, and therefore 24.0 is an outlier.

#### **3. Using Grubb's test**

Grubb's test, also called the maximum normed residual test is an <mark style="background: #BBFABBA6;">alternative heuristic</mark> to identify data points of low probability according to the fitted normal distribution as outliers. Strictly, the statistical assurance comes when both the data follows a normal distribution and <mark style="background: #ABF7F7A6;">at most one outlying value</mark> exists. In this formulation, we permit the outlying values to be at either the minimum or the maximum ends of the data, and so we use the two-sided t-test, and both the minimum and maximum should be tested. In practice, all values will be tested (or more efficiently, all values starting from each end until a non-outlier from each end is found).

Consider a dataset of N data points, with standard deviation s.

You need to choose an \alpha to apply this test, and by that determine the significance you want. This is not unlike having to choose the number of standard deviations in the maximising likelihood method above. A value of \alpha = 0.05 is typical.

Define the z-score for a data point x as $z(x)=\frac{\mid x-\bar{x}\mid}{s}$, that is, the point's distance from the sample mean as a proportion of the standard deviation.

Then, according to Grubb's test  x is an outlier if

$z(x) \geq \frac{N-1}{\sqrt{N}} \sqrt{\frac{t_{\alpha/(2N),N-2}^2}{N - 2 + t_{\alpha/(2N),N-2}^2}}$
where $t_{\alpha/(2N),N-2}$  is the critical value taken by a t-distribution with degrees of freedom N-2  at a significance level of $\alpha/(2N)$. [Check the T-distribution table](https://en.wikipedia.org/wiki/Student%27s_t-distribution#Table_of_selected_values)

Example
Using the Canberra temperature data above, we will test if the minimum value, 24.0, is an outlier by Grubb's test.

Choose $\alpha = 0.05$. From above, we have that $s = \hat{\sigma}= \sqrt{2.175} = 1.474$. Then   $\frac{\alpha}{ 2N } =0.00227$  and  $z(24.0) = \frac{\mid 24.0 - 28.636 \mid}{1.474} = \frac{4.636}{1.474}= 3.145$.  Then  lookup $t_{0.00227, 9}$ using $co-ords (0.99773, 9)$ to get 3.250. Here, we approximate by taking the lower value of the nearby columns in the table to be slightly more permissive in assigning data values to be outliers. Now we have that 24.0 is an outlier if $3.145 \geq \frac{10}{3.317} \sqrt(\frac{3.250^2}{9+3.250^2}) = 3.015 * 0.735 = 2.215$. So 24.0 is an outlier.

---
### Parametric Methods: Multivariate Outliers

**Multivariate data** refers to a data set involving two or more attributes or variables (the usual case!). This is addressed by transforming the multivariate outlier detection task into a univariate outlier detection problem.

#### **Method 1. Compute Mahalaobis distance**

Let ō be the mean vector for a multivariate data set. Let S be the covariance matrix. Mahalaobis distance for an object o to ō is defined as 
 $${\it MDist}(o,\bar{o}) = (o - \bar{o})^T S^{-1}(o-\bar{o})$$
where ${^T}$ and ${^-1}$ are the operators for  matrix transpose and inverse respectively.

Using this transformation, we now have a univariate data set $\{\it{MDist}(o,\bar{o}) \mid o\in D\}$
Then use the Grubb's test on this univariate data set  to identify  outliers.

#### **Method 2. Use χ2 –statistic**

This method assumes a normal distribution.
For each n dimensional object o with dimension values $o_{i}$, i = 1,...n, calculate
$$\chi^{2} = \sum_{i=1}^{n} \frac{(o_{i} - E_{i})^2}{E_{i}}$$
where $E_i$ is the mean of the i-dimension among all objects.
If this $χ2 –statistic$ is large for o, then o is an outlier.

----

### Parametric Methods: Mixture of Parametric Distributions

Assuming that data is generated by a normal distribution could often be **overly simplified**. -> Assume data is generated by multiple distributions, for example two as here.  

For any object o in the data set, the probability that o is generated by the mixture of the two distributions is given by

![[parametric_mixture_distributions.png]]

where $f_{\theta1}$  and $f_{\theta2}$ are the probability density functions of $\theta_1$ and $\theta_2$
Then we can use  an expectation maximisation  algorithm for probablistic model-based clustering  to learn the parameters μ1, σ1, μ2, σ2 from data.  Each cluster is then  represented by one of the  two normal distributions.
An object o  is an outlier if it does not belong to any learned cluster, that is, the probability that it was generated by the combination of the two distributions is below some threshold.
We  are not covering the expectation maximisation algorithm in the course but it is explained  in the text. 

#### **Weakness**
If we have  one or two distributions and  _o_ in the diagram  is a genuine outlier then the lower local density of  the collection of objects means they, including _o_,  are  likely to be interpreted as non-outliers by the normal distribution-fitting.

