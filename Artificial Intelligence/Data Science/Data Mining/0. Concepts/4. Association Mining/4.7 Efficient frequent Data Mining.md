An important optimization in frequent itemset mining algorithms based on the <mark style="background: #FF5582A6;">fact that if an itemset is frequent, then all of its non-empty subsets are also frequent</mark>. This allows for significant memory-saving in algorithms like Apriori.

Example:
If a frequent itemset A contains 100 items, then all non-empty subsets of A, which includes more than 1.27 x 10^30 subsets, are also frequent itemsets. This is due to the fact that if an itemset is frequent, then all of its non-empty subsets are frequent as well.

In data warehouse cuboids, a similar problem of combinatorial explosion is solved by looking at closed cells.

Since every non-empty subset of a frequent itemset is also frequent, we don't need to explicitly store and count the support of every single subset. Instead, we only need to store and count the support of the individual items that make up the frequent itemsets. This significantly reduces the memory requirements and computation time for frequent itemset mining.