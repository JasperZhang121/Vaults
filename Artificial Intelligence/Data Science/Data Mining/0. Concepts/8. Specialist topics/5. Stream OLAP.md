Multidimensional OLAP analysis is still needed in stream data analysis.
Due to the limited memory, disk space, and processing power, it is impossible to store the detailed level of data and compute a fully materialised cube.

### Three key techniques for Stream OLAP

-   Tilted time frame
-   Critical layer storing
-   Partial materialisation

#### **Tilted Time Frame: Time Dimension with Compressed Time Scale**

-   Key idea: The most <mark style="background: #FFB86CA6;">recent time is registered at the finest granularity</mark>; the most distant time is registered at a coarser granularity.
-   The level of coarseness desirable depends on applications.
-   Two examples to design a tilted time frame:

#### Natural tilted time frame model

-   Time frame is structured in <mark style="background: #ABF7F7A6;">multiple granularities based on the natural time scale</mark> (See example in the figure (a) below).
-   Heading from now back in time, we store the most recent 4 quarter-hours, followed by the last 24 hours, then 31 days, and then 12 months = 71 units of time describing a year up to the current time point.  
-   Compute frequent item sets
    -   in the last hour with the precision of a quarter of an hour or
    -   in the last day with the precision of an hour ...etc

#### Logarithmic tilted time frame model (figure (b) below)

-   Time frame is structured in multiple granularities according to a logarithmic scale.
-   Suppose the most recent slot holds the current quarter-hour
-   The remaining slots are for the previous quarter-hour, the half hour before that (2 quarters), the hour before that (4 quarters) , 8 quarters, and so on, with the slot time-size growing at an exponential rate backwards in time.

---
### **Critical Layers**

-   Even with the tilted time frame model, it can still be too costly to dynamically compute and store a materialized cube.
-   Compute and store only some **mission-critical cuboids** of the full data cube
-   <mark style="background: #FFB8EBA6;">Dynamically and incrementally compute and store two critical cuboids (or layers)</mark>
    -   The first layer, called the **minimal interest layer**, is the minimally interesting layer that an analyst would like to study.
    -   The second layer, called the **observation layer**, is the layer at which an analyst (or an automated system) would like to continuously study the data.
    -   These layers are determined based on their conceptual and computational importance in stream data analysis
-   Dimensions at the **raw data layer** includes _individual user, street address_, and _second_.
-   At the **minimal interest** layer, the three dimensions are _user group, street block_, and _minute_, respectively.
    -   Any cuboids that are lower than the minimal interest layer are beyond user interest.
    -   We only need to compute and store the (three-dimensional) aggregate cells for the (user group, street block, minute).
-   At the **observation layer**, the three dimensions are _∗ (meaning all user), city_, and _quarter_, respectively. 
-   The cuboids at the observation layer should be computed dynamically, taking the tilted time frame model into account as well.
-   This is the layer that an analyst takes as an observation to make decisions.

---

### **Partial Materialisation**

“What if a user needs a layer that would be between the two critical layers?”

Materialising a cube at only two critical layers leaves much room for how to compute the cuboids in between. These cuboids can be precomputed fully, partially, or not at all (i.e., leave everything to be computed on the fly).

#### **Popular path cubing:** 
-   rolls up the cuboids from the minimal interest layer to the observation layer by following one popular drilling path
-   materialises only the layers along the path, and leaves other layers to be computed only when needed.
