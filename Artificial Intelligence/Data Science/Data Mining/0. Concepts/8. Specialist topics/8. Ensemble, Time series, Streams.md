>Sampling is called _with replacement_ when a tuple selected at random from the population is not returned to the population, and then, the selected  tuple cannot be selected again in a further sampling process.

`False`

> True statements

In Bagging, each tuple in training dataset $D_i$ is sampled uniformly from the original dataset D with replacement.

```
a. Bagging (Bootstrap Aggregating) involves sampling tuples from the original dataset D with replacement, which means each tuple has an equal chance of being selected for the training dataset D_i.

b. Bagging does not assign different weights to each training tuple. Instead, each tuple has an equal chance of being selected during the sampling process.

d. In AdaBoost, the tuples are sampled with a modified distribution. Initially, all tuples have equal weights, but the weights are adjusted based on the performance of the classifiers. Tuples that are misclassified in previous iterations are given higher weights to focus on them in subsequent iterations.
```


> The following table shows the weights of five items in  dataset D after i-1 iterations of <mark style="background: #FFB8EBA6;">AdaBoost</mark>.

|       | $x_1$ | $x_2$ | $x_3$ | $x_4$ | $x_5$ |
|-------|-----|-----|-----|-----|-----|
| $weight (w_j)$ | 0.2 | 0.1 | 0.15 | 0.15 | 0.4 |

To train  the i-th classifier, we sampled 5 tuples with replacement from the dataset D and obtained the following training dataset $D_i$:

$D_i = (x_1, x_1, x_2, x_2, x_5)$

We trained some classification model $M_i$ using $D_i$ and the classifier correctly classified $x_1$ and $x_2$ and misclassified $x_5$.

>What will be the weight of $x_5$ after adjusting weights and normalisation based on the classification result?
>
>(Note that $err(M_i)$ = 0.4 in this case). Write your answer rounded to two decimal places.

$$Error(M_i)/ (1-Error(M_i) )= 0.4/0.6 = 2/3$$

Weights of x_1 and x_2 before normalisation are:

$w_1 = 0.2 \times (2/3) = 0.4/3$
$w_2 = 0.1 \times (2/3)= 0.2/3$

So to normalise the weights in D we need to divide the all new weights by (0.4/3 + 0.2/3 + 0.15+0.15+0.4) = 0.2+0.15+0,15+0.4 = 0.9

For $x_5$ in particular,the new weight $w_5 = 0.4/0.9 = 0.44$

```
The correct answer is: 0.44
```

> Characteristics of data stream.

- Random access to a data tuple is <mark style="background: #FFB86CA6;">expensive</mark>. Linear or sublinear computational techniques are required to <mark style="background: #FFB8EBA6;">analyse the stream</mark>.
- In general, it is not feasible to store every data tuple. To overcome the storage problem, we often <mark style="background: #BBFABBA6;">store summaries of data</mark> seen so far.
- Some level of <mark style="background: #FFF3A3A6;">approximation is acceptable</mark> in data stream analysis.

> Correct statement which describes a given method for a data stream.

- Histogram → can be used to approximate the <mark style="background: #ABF7F7A6;">frequency distribution</mark> of attribute values
- Random sampling → refers to the <mark style="background: #D2B3FFA6;">process of probabilistic choice</mark> of a data object to be processed or not
- Sliding window → makes a decision on some recent data

>Multidimensional <mark style="background: #ABF7F7A6;">OLAP analysis</mark> is still needed in stream data analysis. However, due to the limited memory, disk space, and processing power, it is <mark style="background: #FFB8EBA6;">impossible to store the detailed level of data and compute a fully materialised cube</mark>. Several methods have been proposed to obtain an efficient OLAP with data stream.

[tilted time frame] method compress the time dimension of the data using different granularity. This approach focuses on fact that 1) the most recent time is registered at the finest granularity, and 2) most distance time is registered at a coarser granularity. For example, [natural tilted time frame model] structures time frames in multiple granularities based on the natural time scale, whereas [logarithmic tilted time frame model] structures time frames in multiple granularities according to a logarithmic scale.

The above approach can only be applied to a time dimension of the data, and materialisation of all possible cubes are still too costly. [critical layers] approach tries to mitigate by dynamically computing and storing two critical cuboids. The upper layer, called the [observation layer], is the layer at which an analyst (or an automated system) would like to continuously study the data. The bottom layer, called the [minimal interest layer], is the minimally interesting layer that an analyst would like to study.


> Lossy Counting Algorithm

Let's say we'd like to estimate frequencies of items in a stream using the lossy counting algorithm.

We set our margin of error, \epsilon to be 0.1 and started to count frequencies.

After a while, we decide to estimate the frequency of item e, so we search the corresponding item in our data structure D and find an entry of the form (e, f, \Delta) whose value is (e, 9, 3).

>What will be the maximum frequency of item e?

```
The maximum frequency of item e can be determined by considering the frequency count (f) in the current entry and the maximum possible frequency count (Delta) from the previous entry for item e.

In the given scenario, the entry for item e is (e, 9, 3). This means that the current frequency count for item e is 9, and the maximum possible frequency count from the previous entry is 3.

-> 9+3 = 12
```

>What will be the minimum frequency of item e?

```
We know we have observed f=9 that we have counted.
```

> Choose the correct description of time series characteristic

- Cyclic movements or cyclic variations → <mark style="background: #FFB8EBA6;">Long-term oscillations</mark> about a trend line, which may or may not be periodic, 
- Trend or long-term movements → a <mark style="background: #FF5582A6;">general direction</mark> in which a time-series is moving over a long interval of time
- Seasonal movements or seasonal variations → systematic or <mark style="background: #ABF7F7A6;">calendar related</mark> movements
- Irregular or random movements → sporadic motion of time series due to random or chance events, such as labor disputes or floods

> Given sequence 
> 2, 4, 6, 6, 4, 2, 0, 4
> What is the sequence of the <mark style="background: #ADCCFFA6;">moving average</mark> of order 4

```
4.5, 5, 4.5, 3, 2.5
```

> Given the same sequence, what is the sequence of the weighted moving average of order 2 with weights (2, 3)?

```
3.2 5.2 6 4.8 2.8 0.8 2.4
```



