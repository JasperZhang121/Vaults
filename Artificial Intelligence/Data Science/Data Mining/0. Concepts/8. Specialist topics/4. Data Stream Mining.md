### Data Stream Mining
- refers to the process of analyzing and extracting information from high-volume, fast-moving, and potentially infinite data streams in real-time. 
- Data streams are generated by various sources, including wireless sensor networks, social media, financial transactions, and other real-time data sources. 
- Data streams are characterized by their size, speed, and dynamic nature, which require specialized techniques to handle them effectively. 
- Some of the characteristics of data streams are that they are continuous, fast-changing, and require real-time processing.

### Processing of data streams
- Different from traditional data processing since <mark style="background: #FFB86CA6;">data streams are typically unbounded and cannot be stored in their entirety</mark>. 
- Random access to data in a data stream is expensive and impractical. Instead, a single-scan algorithm is used to process each record in the stream once. 
- Moreover, only the summary of the data seen thus far is stored, which requires the use of multi-level and multi-dimensional processing methods to analyze the data effectively.

### Examples of data streams
- telecommunication calling records, 
- credit card transaction flows, 
- network monitoring and traffic engineering, 
- financial market data, 
- industrial processes such as power supply and manufacturing. 
- Sensor data from video streams,
- RFID's, 
- security monitoring, 
- web logs, web page click streams, 
- the internet of things (IoT) are also sources of data streams.

The difference between data stream mining and traditional data mining is that <mark style="background: #ABF7F7A6;">data stream mining deals with dynamic and continuous data</mark>, while traditional data mining deals with static and stored data. Data stream mining requires specialized algorithms, techniques, and tools to handle the large volumes of data, speed, and dynamic nature of data streams.

![[stream_vs_traditional.png]]


<mark style="background: #FFB8EBA6;">Data streams are continuous and large in volume, often making it impractical to scan through the entire data stream more than once, or even look at every element of a stream.</mark> This has led to the need to relax the requirement for exact answers and settle for approximate answers using synopses, which provide summaries of the data based on synopsis data structures. Random sampling, sliding windows, histograms, multi-resolution methods, and reservoir sampling are common synopsis techniques. Random sampling involves probabilistically choosing a data item for processing, which can be challenging in a data stream due to the unknown dataset size. Reservoir sampling addresses this by keeping a reservoir of s samples seen so far, from which an <mark style="background: #FFF3A3A6;">unbiased random sample of size up to s can be extracted at any time</mark>. Sliding windows make a decision based on some recent data, where <mark style="background: #FF5582A6;">new data that arrives will expire after a window size w</mark>. Histograms partition the data by attribute values into a set of contiguous buckets and can be used to approximate the frequency distribution of element values in a data stream, making them efficient for answering queries about data frequency.

-   Classification problems on data streams can be solved using an ensemble method that allows for changes in the underlying data stream over time (called concept drift) by adding new models to the ensemble that are built from new data, and retiring old models that were built from older data.
    
-   Ensemble learning over stream data can effectively analyze each object only once (subject to the underlying mining algorithm), store older data in summary form while newer data may be stored in detail, and adapt the overall model over time so that concept drift over time is reflected in predictions.
    
-   The algorithm to build an ensemble classifier involves partitioning the labelled time-sequenced data stream into k chunks and using each chunk to train a conventional probabilistic classifier (e.g., Naive Bayes) to obtain k classifiers ($M_i$, $w_i$), where $w_i$ is the weight of $M_i$ (initially all equal).
    
-   The k classifiers can be used to classify new unlabelled data by voting for each class with their own predicted class probability multiplied by their own assigned class weight, and the highest summed-votes class is predicted.
    
-   After the arrival of a fresh chunk of labeled data of that fixed size, a new classifier M is trained. Also, use that fresh chunk of data as test data for all the previous k classifiers, and discard the classifier from the ensemble with the greatest error rate over that new chunk of data. M is then also included in the ensemble, leaving k classifiers in the ensemble.
    
-   The weights of the k ensemble classifiers are updated with higher weight assigned for higher accuracy over the new data chunk.
    
-   This approach is effective for handling concept drift over time and maintaining accuracy of the classification model in a changing data stream environment.