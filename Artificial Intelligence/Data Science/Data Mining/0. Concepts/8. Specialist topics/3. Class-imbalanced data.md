  
In class-imbalanced data problems, where one class has only a small number of examples in the training set, simply maximizing overall accuracy may not be the best strategy as it can prioritize correct negative examples at the expense of correct positive examples. Therefore, it is necessary to evaluate classification accuracy in such cases using measures such as sensitivity, specificity, or F-score.

To improve classification accuracy, <mark style="background: #BBFABBA6;">oversampling or undersampling</mark> can be used to change the data distribution in the training set. Oversampling involves randomly duplicating positive examples to balance the adjusted training set, while undersampling randomly deletes negative examples from the training set.

For probabilistic classifiers such as Naive Bayes and Neural networks, it is possible to <mark style="background: #ADCCFFA6;">lower the decision threshold for the positive class so that more examples are classified as positive</mark>, reducing the likelihood of false negatives, which can be harmful in class-imbalanced problems. ROC curves can be used to help determine an appropriate threshold.

Ensemble learning methods can also be used, where each model in the ensemble differs by employing some oversampling, undersampling, or threshold-moving technique.

In practice, threshold moving and ensemble methods have been shown to perform better empirically, but oversampling and undersampling are still commonly used by practitioners.