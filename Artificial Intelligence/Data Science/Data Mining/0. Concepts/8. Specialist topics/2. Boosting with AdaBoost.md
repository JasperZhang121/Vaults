Boosting is a machine learning ensemble method that aims to improve the accuracy of a weak model by <mark style="background: #D2B3FFA6;">iteratively building a strong model from a sequence of weak models</mark>. AdaBoost (Adaptive Boosting) is a popular boosting algorithm that was introduced by Freund and Schapire in 1997.

### AdaBoost algorithm

- Starts by fitting a weak learner to the training data, assigning equal weights to all training instances. 
- The weak learner is then evaluated, and the misclassified instances are assigned higher weights than the correctly classified ones. 
- In the next iteration, a new weak learner is trained on the same training set, but with <mark style="background: #FF5582A6;">updated weights that prioritize the misclassified instances</mark> from the previous iteration. 
- This process is repeated for a <mark style="background: #ABF7F7A6;">predefined number of iterations</mark>, with each weak learner added to the ensemble weighted according to its performance on the training set. 
- The final ensemble model is a weighted combination of all the weak models.

### Advantages over other ensemble methods, such as Bagging and Random Forests

- For example, it can handle <mark style="background: #BBFABBA6;">complex data distributions, reduce overfitting, and provide high accuracy</mark> even with a small number of weak learners. 
- Additionally, AdaBoost is not restricted to any particular type of weak learner, which allows it to be applied to a wide range of classification and regression problems.

### Limitations 

- Its sensitivity to noisy **data and outliers**, which can result in overfitting.
- Another limitation is its tendency to focus on a small number of dominant features in the data, which can lead to **bias** and reduced generalization performance.

A common example of AdaBoost application is in <mark style="background: #D2B3FFA6;">face detection systems</mark>. In this application, a set of weak classifiers are trained on various features, such as edge orientation or color, to detect the presence of a face. The weak classifiers are then combined using AdaBoost to create a strong classifier that can accurately detect faces in images. AdaBoost has also been used in various other applications, such as predicting customer churn in telecommunication companies, detecting credit card fraud, and predicting stock prices.