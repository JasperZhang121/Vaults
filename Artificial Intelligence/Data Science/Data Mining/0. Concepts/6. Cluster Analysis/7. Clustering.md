
> True statements

- Clustering algorithms are unsupervised learning methods.
- Clustering is the process of grouping a set of data objects into multiple groups or clusters. 
- A hierarchical clustering method can be classified as being either agglomerative or divisive.

---

|   | x1  | x2  |
|---|-----|-----|
| 1 | 1.0 | 1.0 |
| 2 | 1.5 | 2.0 |
| 3 | 3.0 | 4.0 |
| 4 | 5.0 | 7.0 |
| 5 | 3.5 | 5.0 |
| 6 | 4.5 | 5.0 |
| 7 | 3.5 | 4.5 |


> Our goal is to find 2 clusters using k-means algorithm.
> 
> What are the **cluster assignments**  after the first iteration given two initial cluster means (1.0, 1.0) and (5.0, 7.0)?
> 
> TIE BREAKING RULE: if there is a point that has the same distance to both cluster means, then we can assign that point to the cluster containing (1.0, 1.0).


```
(1, 2, 3), (4, 5, 6, 7)

Use the Euclidean to calculate the data distance to (1,1) and (5,7) respectivly, classify the data to closer cluster. If the distance are equal, assign it to the cluster that (1,1) in.
```

> What will be the clustering result after the second iteration?


```
calculate the new mean:

mean cluster1 (1,2,3) = [1.83, 2.33]
mean cluster2 (4,5,6,7) = [4.125, 5.375]
```


```
1. Calculate the distance between each data point and the mean values of both clusters.

For data point 1:

- Distance to mean cluster 1 (1.83, 2.33): sqrt((1.83 - 1.0)^2 + (2.33 - 1.0)^2) = 1.053
- Distance to mean cluster 2 (4.125, 5.375): sqrt((4.125 - 1.0)^2 + (5.375 - 1.0)^2) = 4.657

Assign data point 1 to cluster 1 as it has a smaller distance to the mean of cluster 1.


2. Repeat the process for the remaining data points:

For data point 2:

- Distance to mean cluster 1: sqrt((1.83 - 1.5)^2 + (2.33 - 2.0)^2) = 0.496
- Distance to mean cluster 2: sqrt((4.125 - 1.5)^2 + (5.375 - 2.0)^2) = 4.497

Assign data point 2 to cluster 1.

For data point 3:

- Distance to mean cluster 1: sqrt((1.83 - 3.0)^2 + (2.33 - 4.0)^2) = 2.080
- Distance to mean cluster 2: sqrt((4.125 - 3.0)^2 + (5.375 - 4.0)^2) = 2.173

Assign data point 3 to cluster 2.

Repeat the process for the remaining data points (4, 5, 6, 7) to assign them to the appropriate clusters based on the smaller distance to the respective mean values.
```


> True statements

- K-means is a non-deterministic algorithm.
- The k-medoids method is more robust than k-means in the presence of noise and outliers.

```
K-means is a non-deterministic algorithm because its outcome can vary depending on the initial selection of cluster centroids. The algorithm iteratively updates the cluster centroids by minimizing the sum of squared distances between data points and their assigned centroids. However, the algorithm can converge to different local optima depending on the initial placement of centroids.

In the context of algorithms, deterministic refers to the property of producing the same output or result when given the same input, regardless of how many times it is executed. A deterministic algorithm will always produce the same output for a given input, making its behavior predictable and consistent.
```

> The dendogram represents the process of divisive clustering over 5 objects.

![[dendro.png]]

> Which pair of clusters are produced in the first step of the algorithm?

```
{X1, X2, X3, X4}, {X5}
```

>What will the final clustering configuration be if we cut the tree at a dissmilarity (i.e. distance) threshold of 2.7?

```
(X1, X2, X3, X4), (X5)
```


>Assume that we obtain clusters as follows from some clustering algorithm (possibly AGNES). Here we have single-dimensional objects, each  represented by the value of its single attribute.  

C1 = (1, 3)
C2 = (5, 8, 12)
C3 = (18, 25)
C4 = (10, 15)
C5 = (2, 20)

Given these clusters, we'd like to further group the clusters using one step of AGNES (agglomerative hierarchical clustering) with Euclidean distance.

> Which two clusters will be merged if we use **single-link** distance to measure the distance between clusters?

```
- Distance between C1 and C2: The minimum distance between any point in C1 and any point in C2 is 2 (distance between 3 in C1 and 5 in C2).

- Distance between C1 and C3: The minimum distance between any point in C1 and any point in C3 is 15 (distance between 1 in C1 and 18 in C3).

- Distance between C1 and C4: The minimum distance between any point in C1 and any point in C4 is 7 (distance between 1 in C1 and 10 in C4).

- Distance between C1 and C5: The minimum distance between any point in C1 and any point in C5 is 1 (distance between 1 in C1 and 2 in C5).

- Distance between C2 and C3: The minimum distance between any point in C2 and any point in C3 is 6 (distance between 5 in C2 and 18 in C3).

- Distance between C2 and C4: The minimum distance between any point in C2 and any point in C4 is 2 (distance between 5 in C2 and 10 in C4).

- Distance between C2 and C5: The minimum distance between any point in C2 and any point in C5 is 3 (distance between 5 in C2 and 2 in C5).

- Distance between C3 and C4: The minimum distance between any point in C3 and any point in C4 is 3 (distance between 18 in C3 and 15 in C4).

- Distance between C3 and C5: The minimum distance between any point in C3 and any point in C5 is 16 (distance between 18 in C3 and 2 in C5).

- Distance between C4 and C5: The minimum distance between any point in C4 and any point in C5 is 8 (distance between 10 in C4 and 2 in C5).

From the above distances, the smallest distance is 1, which is between C1 and C5. Therefore, if we use single-link distance, clusters C1 and C5 will be merged in the next step of AGNES.
```

> Which two clusters will be merged if we use **complete link** distance to measure the distance between clusters?


```
C2, C4
```

```
The complete-link distance between two clusters is calculated as the maximum distance between any pair of points from the two clusters. In other words, for each pair of points, you find the distance between them, and then you select the maximum distance among all these pairs.
```

Consider DBSCAN, density-based clustering.  Let $\epsilon=0.5$ and MinPts = 3.  Given dataset $D = \{0.5, 0.9, 1.2, 1.6, 2.2, 2.4, 2.5\}$ consisting of 7  1-dimensional data points, which of the following statements are true?

```
0.5 is density-reachable from 1.2: 1.2 -> 0.9 -> 0.5
2.2 is directly density-reachable from 2.5: 2.5 -> 2.4 -> 2.2
```


> True statements

```
Extrinsic evaluation methods can be applied when the ground truth categories are available.
```

```
Extrinsic evaluation methods evaluate the performance of a clustering algorithm by comparing the clustering results with some external criteria or ground truth. These criteria can include known class labels or other external information that can be used to assess the quality of the clustering.

The optimal number of clusters is not necessarily the square root of n/2 for a dataset of n points. Determining the optimal number of clusters is a complex problem and depends on various factors, such as the data distribution and the specific clustering algorithm being used.

Hopkins Statistic measures the clustering tendency of a dataset by assessing the randomness or non-randomness of the data distribution. It does not directly measure how well the clusters are separated.
```

```
1. Extrinsic Evaluation: Extrinsic evaluation methods assess the quality of clustering results by comparing them to external criteria or a predefined ground truth. In this approach, the clustering results are evaluated based on their agreement with some external reference, such as known class labels or expert-generated clusters. Extrinsic evaluation measures include metrics like purity, entropy, F-measure, precision, recall, and Rand index. These measures require external information to assess the clustering quality.
    
2. Intrinsic Evaluation: Intrinsic evaluation methods evaluate the quality of clustering results based solely on the internal structure and characteristics of the clusters themselves, without reference to external criteria. In this approach, the evaluation measures are based on the inherent properties of the data and the clusters discovered by the algorithm. Intrinsic evaluation measures include metrics like silhouette coefficient, Dunn index, Davies-Bouldin index, and Calinski-Harabasz index. These measures provide insights into the internal compactness and separation of the clusters.
```
