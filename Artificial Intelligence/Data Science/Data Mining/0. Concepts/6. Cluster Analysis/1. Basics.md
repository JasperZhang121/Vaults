### Cluster

- A collection of data objects that are similar or related to each other within the same group, but dissimilar or unrelated to objects in other groups. 
- Cluster analysis, also known as <mark style="background: #FFB8EBA6;">clustering or data segmentation, aims to find similarities between data objects based on their characteristics and group them into clusters.</mark> 
- It is an unsupervised learning technique, meaning there are no predefined classes.

### Typical applications of clustering

1.  **Stand-alone tool**: Clustering can be used independently to <mark style="background: #FFF3A3A6;">gain insights into the distribution of data</mark>.
2.  **Preprocessing step**: Clustering is often <mark style="background: #ABF7F7A6;">used as a preprocessing tool for other algorithms, such as regression, principal components analysis, classification, and association analysis</mark>.

### Purposes:

1.  **Summarization**: It can be used to <mark style="background: #FFB8EBA6;">summarize data and reduce its complexity</mark>.
2.  **Compression**: In image processing, clustering techniques like vector quantization can be used for compression purposes.
3.  **Finding K-nearest Neighbors**: Clustering helps in localizing searches to a specific cluster or a small number of clusters.
4.  **Outlier detection**: Outliers, which are data points <mark style="background: #BBFABBA6;">significantly distant from any cluster</mark>, can be identified using clustering methods.

### Different domains and applications:

1.  Biology: Taxonomy of living things, such as grouping organisms into kingdom, phylum, class, etc.
2.  Information retrieval: Document clustering to organize and retrieve similar documents.
3.  Land use: Identification of areas with similar land use patterns in Earth observation databases.
4.  Marketing: Discovery of distinct customer groups to develop targeted marketing strategies.
5.  City planning: Grouping houses based on their type, value, and geographical location.
6.  Earthquake studies: Clustering observed earthquake epicenters along continental faults.
7.  Climate research: Understanding Earth's climate by identifying atmospheric and oceanic patterns.
8.  Economic science: Market research and identifying market segments.

### Quality of Clustering

A good clustering method is characterized by producing high-quality clusters. These clusters exhibit the following characteristics:

1.  High intra-class similarity: The <mark style="background: #ABF7F7A6;">objects within a cluster should be cohesive and similar to each other</mark>.
2.  Low inter-class similarity: The clusters should be distinct from each other, <mark style="background: #D2B3FFA6;">exhibiting dissimilarity between clusters</mark>.

#### The quality of a clustering method depends on several factors

1.  Similarity measure: The method should <mark style="background: #BBFABBA6;">employ an appropriate similarity measure or distance function</mark>, denoted as d(i, j), to quantify the similarity between data objects. Different types of variables (interval-scaled, boolean, categorical, ordinal ratio, and vector variables) may require different distance functions.
2.  Implementation: The effectiveness of the clustering method is influenced by its implementation, including the algorithms and techniques used.
3.  Ability to discover hidden patterns: A good clustering method should be able to <mark style="background: #D2B3FFA6;">uncover some or all of the hidden patterns</mark> within the data, providing valuable insights.

#### Measuring the Quality of Clustering:

- Similarity or dissimilarity metrics: 
	- Similarity between data objects is typically expressed using a distance function, such as d(i, j). The definitions of distance functions differ depending on the types of variables being considered. 
	- It is essential to assign appropriate weights to different variables based on the application and the semantic meaning of the data.

- Quality of clustering: 
	- In addition to the similarity metric, clustering methods often incorporate a separate "quality" function to evaluate the goodness of a cluster. This function assesses the overall quality of the clustering results. 
	- However, determining what constitutes "similar enough" or "good enough" clusters is often subjective, and there is no universally defined threshold.


### Algorithmic Considerations

#### Partitioning criteria:
-   Single level vs. hierarchical partitioning: Choosing between a single-level partitioning approach or a hierarchical partitioning approach, where multi-level hierarchical partitioning is often desirable.

#### Separation of clusters:
-   Exclusive vs. non-exclusive: Determining whether clusters should be exclusive (e.g., each data point belongs to only one cluster) or non-exclusive (e.g., a data point can belong to <mark style="background: #FFB86CA6;">multiple clusters</mark>).

#### Similarity measure:
-   Distance-based vs. connectivity-based: Selecting a similarity measure that is either distance-based (e.g., <mark style="background: #BBFABBA6;">Euclidean distance</mark>, road network distance, vector distance) or connectivity-based (e.g., <mark style="background: #ABF7F7A6;">density or contiguity measures</mark>).

#### Clustering space:
-   Full space vs. subspaces: Deciding whether to perform clustering in the <mark style="background: #CACFD9A6;">full feature space</mark> (typically used in low-dimensional data) or in subspaces (often necessary in high-dimensional clustering).

### Requirements and Challenges

#### Scalability:
-   <mark style="background: #ABF7F7A6;">Clustering all data</mark> vs. sampling: Addressing the scalability challenge by considering whether to perform clustering on the entire dataset or on a sample of the data.

#### Ability to deal with <mark style="background: #D2B3FFA6;">different types of attributes</mark>:
-   Numerical, binary, categorical, ordinal, linked, and mixed attributes: Developing clustering algorithms that can handle various types of attributes, including numerical, binary, categorical, ordinal, linked, and combinations of these.

#### Constraint-based clustering:
-   Incorporating user-defined constraints: Allowing users to <mark style="background: #ABF7F7A6;">provide constraints to guide the clustering process</mark> based on their <mark style="background: #ADCCFFA6;">domain knowledge and preferences</mark>.

#### Interpretability and usability:
-   Ensuring that the clustering results are interpretable and usable by users, facilitating meaningful insights and decision-making.

#### Other considerations:
-   Discovery of clusters with <mark style="background: #ADCCFFA6;">arbitrary shape</mark>: Enabling the identification of clusters that may have irregular or non-standard shapes.
-   Dealing with noisy data: Developing techniques to handle data that contains noise or outliers.
-   Incremental clustering and insensitivity to input order: Supporting incremental clustering algorithms that can <mark style="background: #BBFABBA6;">handle streaming or dynamically changing data</mark>, as well as being insensitive to the order of input data.
-   Handling high dimensionality: Addressing the challenges posed by <mark style="background: #CACFD9A6;">high-dimensional data</mark> in terms of computational efficiency and meaningful clustering results.

### Major Approaches

1.  Partitioning approach:
    
    -   Construct various partitions and evaluate them based on a criterion such as <mark style="background: #FFB86CA6;">minimizing the sum of square errors</mark>.
    -   Typical methods: k-means, k-medoids, CLARANS.
2.  Hierarchical approach:
    
    -   Create a <mark style="background: #FFB8EBA6;">hierarchical decomposition of the data</mark> using a criterion.
    -   Typical methods: Diana, Agnes, BIRCH, CAMELEON.
3.  Density-based approach:
    
    -   Based on connectivity and <mark style="background: #ABF7F7A6;">density</mark> functions.
    -   Typical methods: DBSCAN (Density-based spatial clustering of applications with noise), OPTICS, DenClue.
4.  Grid-based approach:
    
    -   Based on <mark style="background: #BBFABBA6;">a multiple-level granularity structure</mark>.
    -   Typical methods: STING, WaveCluster, CLIQUE.
5.  Model-based:
    
    -   <mark style="background: #FFB86CA6;">Hypothesize a model for each cluster</mark> and find the <mark style="background: #ADCCFFA6;">best fit of that model to the data</mark>.
    -   Typical methods: EM (Expectation-Maximization), SOM (Self-Organizing Maps), COBWEB.
6.  Frequent pattern-based:
    
    -   Based on the analysis of frequent patterns in the data.
    -   Typical methods: p-Cluster.
7.  User-guided or constraint-based:
    
    -   Clustering considering user-specified or application-specific constraints.
    -   Typical methods: COD (obstacles), constrained clustering.
8.  Link-based clustering:
    
    -   Objects are linked together in various ways, and massive links can be used to cluster objects.
    -   Typical methods: SimRank, LinkClus.

