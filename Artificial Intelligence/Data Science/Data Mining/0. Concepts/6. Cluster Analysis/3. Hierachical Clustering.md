
### Hierarchical clustering
A method of cluster analysis which seeks to <mark style="background: #D2B3FFA6;">build a hierarchy of clusters</mark>. Strategies for hierarchical clustering generally fall into two types:

- Agglomerative: This is a "<mark style="background: #FFF3A3A6;">bottom up</mark>" approach: each observation starts in its own cluster, and a pair of clusters is merged in each step of moving up the hierarchy.

- Divisive: This is a "<mark style="background: #ABF7F7A6;">top down</mark>" approach: all observations start in one cluster, and a cluster is split into two at each step of moving down the hierarchy.

In general, the merges and splits are determined in a <mark style="background: #BBFABBA6;">greedy manner</mark>. The results of hierarchical clustering are usually presented in a dendogram.

Here is an example of the agglomerative and divisive hierarchical clustering approaches on data objects {a,b,c,d,e}.

![[hierarchical_clustering.png]]

Initially, the agglomerative method places each object into a cluster of its own. The clusters are then merged step-by-step according to some criterion. The merging process is repeated until all the objects are eventually merged to one cluster.

The divisive method proceeds in the oppostive direction. All the objects are used to form one initial cluster. The cluster is split according to some principle. The splitting process repeats until each new cluster contains only a single object.

#### AGNES (AGglomerative NESting)
- Uses the single-link method for determining the distance (dissimilarity)  between clusters. Other methods can instead be applied, see Distance between clusters,  together with a dissimilarity matrix
- Merges nodes that have the least dissimilarity
- Go on until all nodes are in the same cluster

#### DIANA (DIvisive ANAlysis)
- Inverse order of AGNES
- Go on until each distinct data object forms its own cluster


### Distance between Clusters

Whether using an agglomerative method or a divisive method, a core need is to measure the distance between two clusters, $C_i$, $C_j$ where each cluster is a set of objects.

- Single link (minimum distance, nearest-neighbour clustering): <mark style="background: #ADCCFFA6;">smallest distance</mark> between an element in one cluster and an element in the other
i.e., $dist(C_i, C_j) = min_{p \in C_i, q \in C_j}(|p-q|)$

- Complete link (maximum distance): <mark style="background: #FFB8EBA6;">largest distance</mark> between an element in one cluster and an element in the other
i.e., $dist(C_i, C_j) = max_{p \in C_i, q \in C_j}(|p-q|)$

- Average (average distance): average distance between an element in one cluster and an element in the other
i.e., $dist(C_i, C_j) = \frac{1}{|C_i| |C_j|}\sum_{p \in C_i, q \in C_j}(|p-q|)$

- Centroid: distance between the centroids of two clusters
i.e., $dist(C_i, C_j) = (|c_i-c_j|)$

- Medoid: distance between the medoids of two clusters
i.e., $dist(C_i, C_j) = (|o_i-o_j|)$


### Dendrogram

A binary-tree-structured diagram, called a dendrogram, is commonly used to <mark style="background: #ABF7F7A6;">represent the process of hierarchical clustering</mark>. It shows how objects are grouped together (in an agglomerative method) or partitioned (in a divisive method) step-by-step. The similarity of the cluster pairs selected at the step of their agglomeration or division may be shown on a similarity scale.  

A final clustering of the data objects is obtained by cutting the dendrogram at the desired level, then each connected component at that level forms a cluster.  

The <mark style="background: #ABF7F7A6;">desired level is usually determined by selecting a threshold for  similarity amongst clusters</mark>, but the desired number of clusters could be a factor too.  

Here's an example of a dendrogram on data objects _{a,b,c,d,e}_

![[dendrogram.png]]

For example, by setting the similarity threshold to 0.5, one can obtain 3 clusters (a,b), (c), (d,e) from the dendogram.
