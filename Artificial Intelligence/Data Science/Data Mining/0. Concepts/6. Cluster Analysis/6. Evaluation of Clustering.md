
### Major tasks of clustering evaluation 

-   _Before you start_: Assess <mark style="background: #ADCCFFA6;">clustering tendency</mark>. Assess whether a non-random structure exists in the data. Clustering analysis on a data set is meaningful only when there is a non-random structure in the data.  
    
-   _Next:_ Determine the <mark style="background: #CACFD9A6;">number of clusters in a dataset</mark>. How many clusters are there to find? Like k-means, many methods require the number of clusters in advance as a parameter to the method.  
    
-   _After clustering_: Measure the <mark style="background: #FFF3A3A6;">clustering quality</mark>. There are various quality measures according to different criteria.


### Assessing Clustering Tendency

Clustering tendency assessment is a method used to determine if a given dataset has a <mark style="background: #D2B3FFA6;">non-random structure</mark> that can lead to <mark style="background: #CACFD9A6;">meaningful clusters</mark>. It involves assessing whether the data follows a uniform distribution and testing for spatial randomness using the Hopkins Statistic.

#### Assessing clustering tendency using the Hopkins Statistic:

1.  Measure <mark style="background: #BBFABBA6;">Probability of Uniform Distribution</mark>: The goal is to determine how far the dataset D deviates from being uniformly distributed in the data space. This is done by sampling points from D and calculating their nearest neighbors.
    
2.  Random Sampling: Select n points ($p_1, ..., p_n$) uniformly from the range of the dataset D. Each point $p_i$ is chosen randomly within the range, with each value being equally likely to be picked.
    
3.  Nearest Neighbor Calculation: For each randomly sampled point $p_i$, find its <mark style="background: #ABF7F7A6;">nearest neighbor</mark> in the dataset D. This is done by calculating the distance between $p_i$ and all other points in D, and selecting the minimum distance as $x_i$.
    
4.  Existing Point Sampling: Sample another set of n points ($q_1, ..., q_n$) uniformly from the dataset D. These points $q_i$ are selected from the existing values in D.
    
5.  Nearest <mark style="background: #D2B3FFA6;">Neighbor Calculation</mark>: For each point $q_i$, find its nearest neighbor in the dataset D excluding $q_i$ itself. Calculate the minimum distance between $q_i$ and all other points in D (excluding $q_i$) as $y_i$.
    
6.  Calculate the Hopkins Statistic: Compute the Hopkins Statistic (H) using the formula: $H = (\text{sum of } y_i) / (\text{sum of } x_i + \text{sum of } y_i)$
    

#### Interpreting the Hopkins Statistic:

-   If the dataset D is uniformly distributed, <mark style="background: #D2B3FFA6;">H will be close to 0.5</mark>. This suggests the <mark style="background: #FFF3A3A6;">absence of meaningful clusters in the data</mark>.
-   If the sum of $y_i$ is significantly larger than the sum of $x_i$, H tends towards 1. This indicates that the data points have many more distant neighbors than expected in a uniform distribution, implying randomness and irregularity without distinct clusters.
-   If the sum of $x_i$ is much larger than the sum of $y_i$, H tends towards 0. This implies that the distribution of the data is <mark style="background: #FFB8EBA6;">non-uniform</mark>, with unusually low distances to nearest neighbors, indicating <mark style="background: #BBFABBA6;">strong clustering</mark>.

### Determine the Number of Clusters, k

While the best choice for _k_ may be inspired by something you know about why you are attempting the custering in the first place, a few general-purpose methods are given here.  Each can be varied acording to the clustering method,  the cluster quality heuristic or domain knowledge.  Note that the Elbow and Cross-validation methods require many clustering attempts with different numbers of clusters, so could be prohibitively expensive over the full data set. Consider selecting a random sample of the data for this purpose.

#### **Empirical method**
- Try number of clusters $\approx \sqrt{n/2}$ for a dataset of n points. Then each cluster would be expected to have $\sqrt{2n}$ points.

#### **Elbow method**
-   As the number of clusters goes up, the within-cluster variance, that is the distances amongst points in the cluster (defined as the sum of squared distances between each object and the centroid)  decreases to zero. So aim to choose a number that tends to <mark style="background: #FFB86CA6;">reduce the sum of each within-cluster variance</mark>, but  increasing the number any further would have only have marginal effect on the variance.
-   Use the turning point in the curve of sum of within cluster variance w.r.t the number of clusters.
-   To implement:  
    -   For many choices of k>0 (in the extreme, k = 1.., n),  execute  the clustering with parameter k and calculate sum of within-cluster variances for that k.  Plot  each k against its sum. <mark style="background: #FFB8EBA6;">Choose the k corresponding to a notable bend in the curve to be the "right" number of clusters.</mark>


**Cross validation method** [[1. Cross Validation]]
-   Divide a given data set into m parts
-   Use m – 1 parts to obtain a clustering model.
-   Use the remaining part to test the quality of the clustering.
    -   E.g., For each point in the test set, find the closest centroid, and use the sum of squared distance. between all points in the test set and the closest centroids to measure how well the model fits the test set
-   For several choices of  k > 0, repeat it m times and compute the overall quality as the average for each of the m times.  Compare the overall quality measure w.r.t. different k’s, and choose the  number of clusters that  corresponds to the k that has the best overall quality.

### Measure Clustering Quality
- Extrinsic: supervised, i.e., the ground truth is available
	- Compare a clustering against the ground truth using certain clustering quality measure
	- Ex. BCubed precision and recall metrics
- Intrinsic: unsupervised, i.e., the ground truth is unavailable
	- Evaluate the goodness of a clustering by considering how well the clusters are separated, and how compact the clusters are
	- e.g. Silhouette coefficient below is objectively quantitative, but subjective judgement can be just as useful.


#### **Extrinsic Methods**

To measure clustering quality, we need to define <mark style="background: #FFB8EBA6;">a score function</mark> $Q(C, C_g)$ for a clustering C and a ground truth clusters $C_g$.

Four criteria of a good score function:

- Cluster homogeneity: The more <mark style="background: #FFB86CA6;">pure the clusters</mark>, the better the clustering.
- Cluster completeness: The couterpart of homogeneity.  Any two objects belonging to the same category in the ground truth, should be assigned to the same cluster.
- Rag bag: Rag bag category: objects that cannot be merged with other objects. Putting a heterogeneous object into a pure cluster should be <mark style="background: #FFB8EBA6;">penalised more</mark> than putting it into a rag bag.
- Small cluster preservation: <mark style="background: #ABF7F7A6;">Splitting a small category into pieces is more harmful than splitting a large category into pieces</mark>.

BCubed precision and recall metrics satisfy the all four criteria.

- Let $C(o_i)$ be the cluster number of object $o_i$, $L(o_i)$ be the category of $o_i$ given by the ground truth, and $cor(o_i, o_j)$ be 1 if $L(o_i)=L(o_j)$ and $C(o_i) = C(o_j)$, otherwise 0.
- BCubed precision: how many <mark style="background: #ADCCFFA6;">other objects in the same cluster belong to the same category as the object</mark>.
$$p = \bigg(\sum_{i=1}^{n}\frac{\sum_{o_j:i\neq j, C(o_i) = C(o_j)} cor(o_i,o_j)}{||\{{o_j|i\neq j, C(o_i) = C(o_j)\}||}}\bigg)\times \frac{1}{n}$$

- BCubed recall: how many objects of the same category are assigned to the same cluster.
$$r = \bigg(\sum_{i=1}^{n}\frac{\sum_{o_j:i\neq j, L(o_i) = L(o_j)} cor(o_i,o_j)}{||\{{o_j|i\neq j, L(o_i) = L(o_j)\}||}}\bigg)\times \frac{1}{n}$$

#### **Intrinsic Methods**

Two criteria for intrinsic method:
- How well the clusters are separated
- How compact the clusters are

1. Silhouette coefficient satisfies above two conditions.

Let $C_i$ be the ith cluster from a clustering and let o be an object in the cluster $C_i$.

- compactness, $a(o) = \frac{\sum_{o' \in C_i, o\neq o'} dist(o, o')}{|C_i| - 1}$ where $o \in C_i$,
- separation, $b(o) = \min_{C_j: 1\leq j \leq k, j\neq i}\bigg\{ \frac{\sum_{o'\in C_j} dist(o, o')}{|C_j|} \bigg\}$

a(o) reflects the compactness of the cluster $C_i$, being the average distance of the object o in the cluster from every other object in the cluster. Low compactness is good.

b(o) reflects the degree to which object o is separated from other clusters it does not belong to, being the average distance to all objects in the next-closest cluster. High separation is good.

The silhouette coefficient of o is then defined as:

- $s(o) = \frac{b(o)- a(o)}{\max\{a(o), b(o)\}}$

The value lies between -1 and 1. A negative value means o is closer to the objects in another cluster than to the objects in the same cluster in expectation, and this is normally undesirable.

To evaluate a particular cluster, average the silhouette coefficient for every object in the cluster. To evaluate a clustering, average the silhouette coefficient for every object in the dataset. Negative values are poor.

2. Visual inspection Plot the clusters (or a small random sample of the data instead of the whole dataset) in 2 dimensions (choose several pairs of dimensions for several plots, or choose pairs of dimensions that are expected to  be important in the problem domain. You can plot in 3 dimensions if you prefer, but  more than that and it gets very hard to inspect visually).  Indicate the cluster membership by the colour coding of points on the plot. Do the clusters seem to be well separated and internally compact ?

3. Elbow method If you used the elbow method to choose an optimal number of clusters, was there a clear turning point in the plot, indicating that there is some inherent structural meaning to the k you chose?