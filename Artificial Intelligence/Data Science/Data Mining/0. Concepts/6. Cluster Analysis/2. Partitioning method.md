
Partitioning a database D of n objects into a set of k clusters. The quality of cluster $C_i$ can be measured by the within-cluster variation, which is the sum of squared distances between all objects in $C_i$ and the centroid $c_i$, defined as: 

$$E = \sum_{i=1}^{k}\sum_{p\in C_i} dist (p, c_i)^2 $$

Given k, the goal is to find a partition of k clusters that optimizes the chosen partitioning criterion. This can be achieved using the following methods:

1.  Globally optimal: Exhaustively enumerate all possible partitions and select the one with the <mark style="background: #ABF7F7A6;">minimum within-cluster variation</mark>. However, this approach is computationally expensive and not feasible for large datasets.
    
2.  Heuristic methods: These are more efficient and commonly used in practice. Two popular heuristic methods are: k-means and k-medoids algorithms
    
3. k-means algorithm: In this algorithm, each cluster is represented by the calculated mean center of the cluster. The steps involved are:
    
    -   Arbitrarily choose k objects to be the initial cluster centroids.
    -   Assign each object to the cluster that has the closest centroid.
    -   Update the cluster means by calculating the mean value of the objects for each cluster.
    -   Repeat the assignment and update steps until the assignments no longer change.
    
4. k-medoids (or PAM) algorithm: In this algorithm, <mark style="background: #D2B3FFA6;">each cluster is represented by one of the objects in the cluster</mark>, called a medoid. The steps are similar to k-means, but instead of using the mean center, the medoid is chosen as the representative.
    
5. k-modes algorithm: This is a variant of k-means for categorical data, where each cluster is represented by the most frequent category in the cluster.

![[K-MEANS.png]]

(a) Green points denote the data set in a two-dimensional Euclidean space. The initial choices for centres for $C_1$ and $C_2$ are shown by the red and blue crosses, respectively.

(b) Each data point is assigned either to the red cluster or to the blue cluster, according to which cluster centre is nearer. This is equivalent to classifying the points according to which side of the perpendicular bisector of the two cluster centres, shown by the magenta line, they lie on.

(c) In the subsequent step, each cluster centre is re-computed to be the mean of the points assigned to the corresponding cluster.

(d)–(i) show successive steps through to <mark style="background: #CACFD9A6;">final convergence</mark> of the algorithm.


### Strength and Weakness
- Strength
	- Computationally Efficient: time complexity is O(tkn), where n is the number of objects, k is the number of clusters, and t is the number of iterations. Normally, k, t .
	- Comparing: 
		- PAM: $O(k(n-k)^2 )$
		- CLARA: $O(ks^2 + k(n-k))$

- Weakness
	- Need to specify k, the number of clusters, in advance
	- Applicable <mark style="background: #FFB86CA6;">only to objects in a continuous n-dimensional space</mark>
		- k-modes variant method for categorical data: replaces the mean value by the  mode of a nominal attribute.
		- In comparison, k-medoids can be applied to a wide range of data
	- Sensitive to <mark style="background: #ABF7F7A6;">noisy data and outliers</mark>
	- Non deterministic algorithm. The final result depends on the first initialisation.
	- Often <mark style="background: #CACFD9A6;">terminates at a local optimum</mark>, rather than a global optimum.
	- Not suitable for clusters with non-convex shapes

#### Noisy data point example
- Consider six points in 1-D space having the values 1,2,3,8,9,10, and 25, respectively. Intuitively, by visual inspection we may imagine the points partitioned into the clusters {1,2,3} and {8,9,10}, where point 25 is excluded because it appears to be an outlier. How would k-means partition the values? If we apply k-means using k = 2.

- Case 1: partition values into {{1, 2, 3}, {8, 9, 10, 25}}. Within-cluster variation is (1-2)^2 +(2-2)^2 +(3-2)^2 +(8-13)^2 +(9-13)^2 +(10-13)^2 +(25-13)^2=196, given that the mean of cluster {1,2,3} is 2 and the mean of {8,9,10,25} is 13.
- Case 2: partition  values into  {{1, 2, 3, 8}, {9, 10, 25}}. Within-cluster variation is (1-3.5)^2 +(2-3.5)^2 +(3-3.5)^2 +(8-3.5)^2 +(9-14.67)^2 + (10-14.67)^2 + (25-14.67)^2 = 189.67, given that 3.5 is the mean of cluster {1, 2, 3, 8} and 14.67 is the mean of cluster {9, 10, 25}.

- The latter partitioning has the <mark style="background: #FFF3A3A6;">lowest within-cluster variation</mark>; therefore, the k-means method assigns the value 8 to a cluster different from that containing 9 and 10 due to the outlier point 25. Moreover, the centre of the second cluster, 14.67, is substantially far from all the members in the cluster. 

### K-Medoids (PAM)

- Instead of taking the mean value of the object in a cluster as a reference point, we can <mark style="background: #ADCCFFA6;">pick actual objects to represent the clusters</mark>.
- The k-medoids method is more robust than k-means in the presence of noise and outliers because a medoid is <mark style="background: #FFB86CA6;">less influenced by outliers or other extreme values than a mean</mark>.

#### Partitioning Around Medoids (PAM)

- PAM algorithm is a popular realisation of k-medoids clustering.

- Starts from an initial set of medoids and iteratively replaces one of the medoids by one of the non-medoids if it improves the total distance of the resulting clustering.

The quality of clustering can be measured by an absolute-error criterion (total cost):

$$E = \sum_{i=1}^{k}\sum_{p\in C_i} dist(p, o_i)$$

where E is the sum of the absolute error for all objects p in the dataset, and $o_i$ is the representative object of $C_i$.

### Algorithm

- Arbitrarily choose k objects in D as the initial representative objects
- Assign each remaining object to the cluster with the nearest representative object
- Randomly select a non-representative object, $o_{random}$
- For each representative object $o_j$,Compute the total cost of swapping representative object, $o_j$, with $o_{random}$
- If the swapping reduces the total cost, then swap $o_j$ with $o_{random}$ to form the new set of k representative objects
- Repeat 2-4 until there is no change.

### Illustration of k-medoids algorithm

![[k-medoids.png]]


### K-Modes

- K-means requires the mean of objects to be defined, but this makes little sense for nominal attributes. K-modes is a variant of  k-means that uses modes instead of means to cluster.

The distance between objects needs attention.  To compute distance we use simple matching as we studied before, although it is not usual for k-modes (just like for  k-means and k-medoids) to normalise the distance to a [0,1] interval by dividing by the number of attributes.   Although  normalising in this way would not change anything significantly.

- For objects $x = (x_1,..x_n)$ and  $y = (y_1,..y_n)$, dist(x,y) = $\Sigma_{i=1}^{n} d(x_i,y_i)$ where d(x,y) = 0  if x = y and 1 otherwise.

- We also need to know how to define the centroid of each cluster. For k-modes, we determine that the centroid is defined by an imaginary object that has every attribute having the value that is the most frequent value for that attribute in the cluster (ie the mode of the attribute).

- Now we can apply the <mark style="background: #ADCCFFA6;">same algorithm as basic kmeans</mark>: start with k random centroids, allocate objects to the cluster of the closest centroid, recalculate the centroids, and iterate until the sum of squared distances stabilises.

If we have <mark style="background: #ABF7F7A6;">mixed numeric and nominal data</mark>, we can simply calculate the appropriate distance attribute-by-attribute, but we would want to normalise each numeric attribute to a [0,1] scale first to make them fairly comparative.


### Density-Based Clustering

Model clusters as dense regions in the data space, separated by sparse regions. Does not attempt to assign every object to a cluster; <mark style="background: #D2B3FFA6;">many may be left out as "noise"</mark>.

Major features:

- Discovers clusters of arbitrary shape
	- Partitioning and hierarchical methods are designed to find spherical-shaped (convex) clusters
- Handles noise
- One scan through the data only
- Needs parameters to define threshold dense-ness (but not for the number of clusters)


### DBSCAN (Density-Based Spatial Clustering of Applications with Noise)

- Density of an object o: the number of objects close to o
- Core objects: Objects that have a dense neighbourhood
- DBSCAN: connects core objects and their neighbourhoods to form dense regions as cluster
- Two parameters:
	- \epsilon: Maximum radius of the neighbourhood
	- MinPts: Minimum number of points in an \epsilon-neighbourhood of that point
- $N_{\epsilon}(p)$: ${q \in D | dist(p,q) \leq \epsilon}$
	- Number of neighbourhood objects including p
	- If N_{\epsilon}(p) >=  MinPts, then p is core object
	- D is a data set.
- Directly density-reachable: A point p is directly density-reachable from a core point q if p is within the $\epsilon-\text{neighbourhood}$ of q
	- By definition, no points are directly density-reachable from a non-core point.
- Density-reachable: p is density-reachable from a core point q if
	- there is a chain of objects p_1, p_2, ... p_n such that p_1 = q, p_n=p and $p_{i+1}$ is directly density-reachable from p_i with respect to \epsilon and MinPts.
	- By definition, all the p_is other than p_n = p are core points
- Density-connected: Two objects p_1, p_2 are density-connected if
	- there is an object q such that both p_1 and p_2 are density-reachable from q with respect to \epsilon and MinPts.
	- By definition, q must be a core point, and p_1 and p_2 must be in the neighbourhood of a core point, but may not be core points themselves.

