
### Decision tree induction
- A popular machine learning method used for classification and regression problems. 
- The process of decision tree induction involves creating a tree-like model that classifies data by <mark style="background: #FF5582A6;">splitting it based on a set of attributes</mark>. At each level of the tree, the algorithm selects the attribute that best splits the data into classes, based on a criterion such as information gain or Gini impurity. 
- This process is repeated recursively for each subset of data until a stopping criterion is met. 
- The resulting tree can be used to classify new, unseen instances by following the path from the root to a leaf node, which indicates the class assignment.

### Basic, greedy, decision tree algorithm

- The basic, greedy, decision tree algorithm is a commonly used approach for decision tree induction. 
- It follows a top-down, recursive approach to divide a dataset into smaller subsets based on the values of the input attributes. At each step, the algorithm chooses the attribute that provides the most information gain, which is a measure of the reduction in entropy or impurity of the dataset.
- The basic, greedy decision tree algorithm selects the test attribute that optimizes the next step locally, without considering the effect on future steps. 
- It uses <mark style="background: #FF5582A6;">a heuristic or statistical measure</mark>, such as information gain or Gini index, to choose the attribute that provides the most useful information for partitioning the data at a given node. However, this does not necessarily result in the best overall decision tree. This process is repeated recursively for each subset until a stopping criterion is met, such as reaching a maximum depth, a minimum number of instances in a leaf node, or a threshold for information gain.
- It can handle <mark style="background: #FFB86CA6;">both categorical and continuous attributes</mark>, but it requires a discrete set of thresholds for each continuous attribute. It may also suffer from overfitting, where the tree is too complex and fits the training data too closely, resulting in poor performance on new, unseen data. 
- To prevent overfitting, the algorithm can be pruned by <mark style="background: #FFF3A3A6;">removing branches or nodes that do not improve the performance on a validation set</mark>. Additionally, ensemble methods, such as random forests or boosting, can be used to combine multiple decision trees to improve the predictive performance and reduce overfitting.

---
### Attribute Selection Methods

Attribute selection methods are used to identify the <mark style="background: #FF5582A6;">most relevant features or attributes that contribute the most to the target variable</mark> in a given dataset. The main objective of attribute selection is to improve the performance of a machine learning algorithm by reducing the number of irrelevant and redundant features, and thus, simplifying the model.

#### Splitting rules
-   Techniques  to choose a **splitting criterion** comprised of a **splitting attribute** and a **split point** or **splitting subset**
-   Aim to have partitions at each branch as **pure** as possible -- i.e. all examples at each sub- node  belong in the same class.

#### Heuristics
(or **attribute selection measures**) are used to choose the best splitting criterion. Information Gain, Gain ratio and Gini index are most popular.  

-   Information gain:
	-   biased towards multivalued attributes
-   Gain ratio:
	-  tends to prefer unbalanced splits in which one partition is much smaller than the others
-   Gini index:
	-  biased towards multivalued attributes
	-  has difficulty when number of classes is large
	- tends to favour tests that result in equal-sized partitions and purity in both partitions


#### Three main types of attribute selection methods:

1.  Filter methods: These methods use statistical measures to rank the importance of each feature and select a subset of relevant features. These measures are applied to the dataset independently of the machine learning algorithm.
    
2.  Wrapper methods: These methods use a specific machine learning algorithm to evaluate the performance of different subsets of features, and select the subset that yields the best performance.
    
3.  Embedded methods: These methods incorporate the attribute selection process into the model-building algorithm itself, so that the selection and building of the model are done simultaneously. This approach is often used in algorithms that have built-in feature selection capabilities, such as decision trees and support vector machines.

check: [[5. Feature Selection]]
#### Other Attribute Selection Methods

-   **CHAID**: a popular decision tree algorithm, measure based on χ2 test for independence
-   **C-SEP**: performs better than info. gain and gini index in certain cases
-   **G-statistic**: has a close approximation to χ2 distribution
-   **MDL (Minimal Description Length) principle** (i.e., the simplest solution is preferred):
    -   The best tree as the one that requires the fewest number of bits to both (1) encode the tree, and (2) encode the exceptions to the tree
-   **Multivariate splits** (partition based on multiple variable combinations), e.g. **CART**: finds multivariate splits based on a linear combination of attributes.

---

### Information Gain

Information Gain is a measure of the amount of information gained about a class variable when a certain attribute is used to split the data into subgroups. The basic idea behind Information Gain is to calculate the difference between the entropy of the target variable before and after the split on the given attribute. The attribute with the highest Information Gain is chosen as the splitting attribute.

Information Gain $$(S, A) = Entropy(S) - \sum [(\frac{|S_v|}{|S|}) * Entropy(S_v)]$$
where:
- $S$ is the original set of examples 
- $A$ is the attribute being considered for splitting the set
- $S_v$ is the subset of examples in $S$ that have the value $v$ for attribute $A$
- $Entropy(S)$ is the entropy of the set $S$ 
- $Entropy(S_v)$ is the entropy of the subset $S_v$

The entropy formula is:

$$Entropy(S) = - \sum_{i=1}^{m} p_i \log_2 p_i$$

where $S$ is a set of examples, $m$ is the number of classes, $p_i$ is the proportion of examples in $S$ that belong to class $i$.


#### Information Gain for continuous-valued attributes

Let attribute _A_ be a continuous-valued attribute

To determine the _best split point_ for A

-   Sort the values of  _A_ in increasing order

-   Typically, the **midpoint between each pair of adjacent values** is considered as a possible split point
    -   _(ai+ai+1)/2_ is the midpoint between the values of _a_i and _ai+1_

-   Of these, the point with the minimum expected information requirement for A, InforA(D) is selected as the split-point for _A_

Then Split:

_D1_ is the set of tuples in _D_ satisfying _A ≤ split-point_, and _D2_ is the set of tuples in _D_ satisfying _A > split-point_

This method can also be used for ordinal attributes with many values (where treating them  simply as nominals may cause too much branching).

---

### Gain Ratio

Gain Ratio is an extension of Information Gain used in decision trees to reduce the bias for attributes with a large number of values. Information Gain has a bias towards multi-valued attributes by giving them higher preference. Gain Ratio compensates for this bias by normalizing the information gain using Split Information.

The formula for Gain Ratio is given as:
$$\text{Gain Ratio}(S,A) = \frac{\text{Information Gain}(S,A)}{\text{Split Information}(S,A)}$$

where,

-   $S$ is the set of training examples.
-   $A$ is the attribute being considered for splitting the set.
-   $\text{Information Gain}(S,A)$ is the information gain of the set $S$ after splitting by attribute $A$.
-   $\text{Split Information}(S,A)$ is the measure of the amount of splitting done by attribute $A$ on set $S$.

Split Information is defined as:
$$\text{Split Information}(S,A) = -\sum_{i=1}^{v} \frac{|S_i|}{|S|} \log_2\frac{|S_i|}{|S|}$$
where $v$ is the number of possible values for attribute $A$, and $S_i$ is the subset of $S$ having the $i$-th value for attribute $A$.


### Gini index

The Gini index is a measure of the impurity or diversity of a set of examples, often used in the context of decision trees for classification. It ranges between 0 and 1, with 0 indicating a completely pure set (all examples belong to the same class) and 1 indicating a completely impure set (an equal number of examples belong to each class). In other words, a smaller Gini index indicates a more homogeneous set of examples in terms of their class distribution.

The Gini index for a set of examples $S$ with respect to a binary classification problem (i.e., two classes, positive and negative) can be calculated as follows:

$$G(S) = 1 - \sum_{i=1}^{c}p_i^2$$
where:

-   $S$ is the set of training examples
-   $c$ is the number of classes
-   $p_i$ is the fraction of examples in class $i$ in $S$

where $p_i$ is the fraction of examples in $S$ that belong to class $i$ (i.e., $p_i = \frac{|S_i|}{|S|}$, where $S_i$ is the subset of examples in $S$ that belong to class $i$). The Gini index is then used in the process of selecting the best attribute to split on at each node in the decision tree. The attribute with the lowest Gini index after the split is chosen as the splitting criterion, as it results in a more homogeneous set of examples in terms of their class distribution.



