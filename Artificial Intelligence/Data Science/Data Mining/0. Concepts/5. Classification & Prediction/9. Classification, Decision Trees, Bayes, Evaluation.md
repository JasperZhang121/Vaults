| age   | club | wins |
| ----- | ---- | ---- |
| child | red  | yes  |
| child | blue | no   |
| youth | red  | yes  |
| adult | red  | no   |
| adult | blue | yes  |

Consider the Athletics dataset, D, above which is a training set for a decision tree classifier aiming to predict the variable "wins" from age and club membership. Let $p_i$ be the probability that an arbitrary tuple in D belongs to class $C_i$, of m classes estimated by $p_i=\frac{|C_{i, D}|}{|D|}$. Then the information needed to classify a tuple in D is defined by $\it{Info}(D) = - \sum_{i=1}^{m} p_i\it{log}_2(p_i)$ and after using attribute A to split D into v partitions, the information needed is $\it{Info}_A(D) = \sum_{j=1}^{v}\frac{|D_j|}{|D|}\times\it{Info}(D_j)$ So the information gain by splitting is defined as $\it{Gain}(A) = \it{Info}(D) - \it{Info_A}(D)$

>1. What is the <mark style="background: #FF5582A6;">information gain</mark> for splitting D on the three categories of  attribute "age"? 

To calculate the information gain for a given attribute A, we first need to calculate the information needed to classify a tuple in D, $\it{Info}(D)$, and the information needed to classify a tuple in D after using A to split D into v partitions, $\it{Info_A}(D)$. We can then calculate the information gain as the difference between these two values.

$\it{Info}(D)$ is the entropy of the class distribution in D, and can be calculated as <mark style="background: #FF5582A6;">formula</mark>:

$$\it{Info}(D) = - \sum_{i=1}^{m} p_i\it{log}_2(p_i)$$

where $p_i$ is the probability that an arbitrary tuple in D belongs to class $C_i$, and m is the number of classes. Also, can be written as:
<mark style="background: #FFF3A3A6;">I(p,n) = -(p/(p+n)*log_2(p/(p+n)) + n/(p+n) * log_2(n/(p+n)))</mark>

In this case, we have two classes: "yes" and "no". So we can calculate $\it{Info}(D)$:

$\it{Info}(D) = - \frac{3}{5}\it{log}_2\left(\frac{3}{5}\right) - \frac{2}{5}\it{log}_2\left(\frac{2}{5}\right) = 0.971$

Next, we need to calculate $\it{Info_A}(D)$, which is the weighted average of the entropy of the class distribution in each partition created by splitting D on attribute A. It can be calculated as:

$\it{Info_A}(D) = \sum_{j=1}^{v}\frac{|D_j|}{|D|}\times\it{Info}(D_j)$

where $|D_j|$ is the number of tuples in partition j, and |D| is the total number of tuples in D.

To calculate the information gain for each attribute, we need to split D on that attribute and calculate $\it{Info_A}(D)$ for each resulting partition. We can then calculate the information gain as the difference between $\it{Info}(D$) and $\it{Info_A}(D)$ for each attribute.

For example, if we split D on the attribute "age", we get three partitions:

-   Partition 1: age = "child", with 2 tuples (1 "yes" and 1 "no")
-   Partition 2: age = "youth", with 1 tuple (1 "yes" and 0 "no")
-   Partition 3: age = "adult", with 2 tuples (1 "yes" and 1 "no")

The entropy of the class distribution in each partition can be calculated as:

-   $Partition 1: \it{Info}(D_1) = - \frac{1}{2}\it{log}_2\left(\frac{1}{2}\right) - \frac{1}{2}\it{log}_2\left(\frac{1}{2}\right) = 1.0$
-   $Partition 2: \it{Info}(D_2) = - \frac{1}{1}\it{log}_2\left(\frac{1}{1}\right) - \frac{0}{1}\it{log}_2\left(\frac{0}{1}\right) = 0$
-   $Partition 3: \it{Info}(D_3) = - \frac{1}{2}\it{log}_2\left(\frac{1}{2}\right) - \frac{1}{2}\it{log}_2\left(\frac{1}{2}\right) = 1.0$

Next, we need to calculate the <mark style="background: #ABF7F7A6;">weighted average</mark> of the entropy of the class distribution in each partition:

$\it{Info_A}(D) = \frac{2}{5}\times\it{Info}(D_1) + \frac{1}{5}\times\it{Info}(D_2) + \frac{2}{5}\times\it{Info}(D_3) = 0.8$

Therefore, the information gain of splitting the data on "age" into three partitions with the updated class distribution is:

$\it{Gain}(age) = \it{Info}(D) - \it{Info_A}(D) = 0.971 - 0.8 = 0.171$

This means that splitting the data on "age" into three partitions with the updated class distribution results in a reduction in the entropy of the class distribution in D by 0.171 bits, which is the amount of information gained by using the attribute "age" to split the data.


>2. If we split the data on the attribute "club" into two partitions based on the categories "red" and "blue", we get the following partitions:

To calculate the entropy of the class distribution in each partition, we can use the same formula as before:

-   $Partition 1: \it{Info}(D_1) = - \frac{2}{3}\it{log}_2\left(\frac{2}{3}\right) - \frac{1}{3}\it{log}_2\left(\frac{1}{3}\right) = 0.918$
-   $Partition 2: \it{Info}(D_2) = - \frac{1}{2}\it{log}_2\left(\frac{1}{2}\right) - \frac{1}{2}\it{log}_2\left(\frac{1}{2}\right) = 1.0$

Next, we need to calculate the weighted average of the entropy of the class distribution in each partition:

$\it{Info_A}(D) = \frac{3}{5}\times\it{Info}(D_1) + \frac{2}{5}\times\it{Info}(D_2) = 0.951$

Therefore, the information gain of splitting the data on the attribute "club" into two categories with the updated class distribution is:

$\it{Gain}(club) = \it{Info}(D) - \it{Info_A}(D) = 0.971 - 0.951 = 0.02$

This means that splitting the data on the attribute "club" into two categories with the updated class distribution results in a reduction in the entropy of the class distribution in D by 0.100 bits, which is the amount of information gained by using the attribute "club" to split the data.

>3.

A <mark style="background: #FFF3A3A6;">higher information gain from splitting an attribute indicates that the attribute is more useful for predicting the target variable</mark>, and splitting on that attribute will result in a greater reduction in the entropy of the class distribution in the data.

In the context of a decision tree classifier, we want to choose the attribute that provides the highest information gain as the root of the tree, as this will result in the most effective split of the data and the most accurate predictions for the target variable.

So in the case of the Athletics dataset, we saw that splitting on the attribute "age" resulted in a higher information gain than splitting on the attribute "club", indicating that "age" is a more useful attribute for predicting "wins". Therefore, we would choose "age" as the root of the decision tree and split the data accordingly.


----

| age         | credit    | buys_computer |
|-------------|-----------|-----------------|
| youth       | fair      | no              |
| youth       | fair      | yes            |
| middle_aged | excellent | yes            |
| middle_aged | fair      | no              |
| youth       | excellent | no              |
| middle_aged | excellent | no              |
| middle_aged | fair      | yes            |

And using Naive Bayes to predict the class that maximises $P(C_i|X) \propto P(X|C_i)P(C_i)$

>1. Calculate the prior probability of an arbitrary  customer buying a computer (i.e belonging to the class buys_computer = yes) regardless of other information about the customer.

$\text{prior probability} = P(buys\_computer = yes) = \frac{\text{count}(buys\_computer = yes)}{\text{count(total)}}$
$\text{prior probability} = P(buys\_computer = yes) = \frac{3}{7} = 0.43 \text{ (rounded to 2 decimal places)}$

> 2. Calculate the prior probability of an arbitrary  customer _not_ buying a computer (i.e belonging to the class buys_computer = no) regardless of other information about the customer.

$\text{prior probability} = P(buys\_computer = \text{no}) = \frac{\text{count}(buys\_computer = \text{no})}{\text{count(total)}}$
$\text{prior probability} = P(buys\_computer = \text{no}) = \frac{4}{7} = 0.57 \text{ (rounded to 2 decimal places)}$

> 3. Calculate the  likelihood probability P(age=middle_aged)| buys=yes)

$\text{likelihood probability} = P(age = \text{middle\_aged} | buys\_computer = \text{yes}) = \frac{\text{count}(age = \text{middle\_aged}, buys\_computer = \text{yes})}{\text{count}(buys\_computer = \text{yes})}$
- number of instances where age = middle_aged and buys_computer = yes = 2 
- number of instances where buys_computer = yes = 3

As a result:
$\text{likelihood probability} = P(age = \text{middle\_aged} | buys\_computer = \text{yes}) = \frac{2}{3} = 0.67 \text{ (rounded to 2 decimal places)}$

> 4. Calculate  the likelihood probability P(credit=fair|  buys=yes)

- number of instances where credit = fair and buys_computer = yes = 2
- number of instances where buys_computer = yes = 3

likelihood probability = 2 / 3 = 0.67 (rounded to 2 decimal places)

> 5. Calculate the likelihood P(X = (middle-aged, fair) | buys = yes)

P((middle-aged, fair) | buys = yes))
=  P(age=middle_aged)| buys=yes) * P(credit=fair|  buys=yes)
=  (2/3) * (2/3) = 4/9

> 6. Using $P(C_i|X) \propto P(X|C_i)P(C_i)$ - Calculate $P(X = (middle-aged, fair) | buys = yes) * P(buys = yes)$

P(X = (middle_aged, fair)) = P(X = (middle_aged, fair) | buys_computer = yes) * P(buys_computer = yes) = (4/9 * 3/7) = 12/63

> 7. Calculate the likelihood probability P(age=middle_aged| buys=no)

2/4 = 1/2

> 8. Calculate  the likelihood probability P(credit=fair|  buys=no)

2/4 = 1/2

> 9. Calculate the likelihood P(X = (middle-aged, fair) | buys = no)

1/4

> 10. Calculate P(X = (middle-aged, fair) | buys = no) * P(buys = no)

1/4 * 4/7 = 1/7


> 11. So X = (middle_aged, fair) is classified as someone who buys a computer, right?

True as 12/63 > 1/7. 

---

| actual class\predicted class | cancer = yes | cancer = no | Total | Recognition(%) |
|------------------------------|--------------|-------------|-------|----------------|
| cancer = yes                  | 90           | 210         | 300   | 30.00 (sensitivity) |
| cancer = no                   | 140          | 9560        | 9700  | 98.56 (specificity) |
| Total                        | 230          | 9770        | 10000 | 96.50 (accuracy)  |

Information:
- True Positives = 90
- True Negatives = 9560
- False Positives = 140
- Accuracy = Correct predictions / Size of the data
- Error Rate = 3.5%
- Error Rate = ( False Positives + False Negatives )/ Total Size of Data

> Say you have a labelled dataset D of 50 labelled objects and you want to evaluate the performance of your fantastic new learning algorithm you wrote yourself, called Z.  So you decide to evaluate the  performance of Z using 5-fold cross-validation.

- This would be a very small dataset and you need to get all you can out of the data to train your models.  Maybe use leave-one-out, or better,  bootstrap.
- Need 5 disjoint subsets and you make 5 training sets by each combination of 4 of them.




