
### Definition

- Bayesian classifiers are a family of probabilistic classifiers based on Bayes' theorem, which assumes that the probability of a particular class given an observation or input is proportional to the probability of that observation given the class. These classifiers are widely used in various fields, including natural language processing, machine learning, and computer vision.

The Bayes theorem states that:

$$P(c|x) = \frac{P(x|c) \cdot P(c)}{P(x)}$$

where:
- $P(c|x)$ is the probability of class $c$ given the observation $x$, $P(x|c)$ is the probability of observation $x$ given class $c$
- $P(c)$ is the prior probability of class $c$
- $P(x)$ is the probability of observation $x$.

To classify an observation or input x, the Bayesian classifier <mark style="background: #FFF3A3A6;">calculates the probability of each class</mark> given the observation x, and then <mark style="background: #ADCCFFA6;">selects the class with the highest probability as the predicted class</mark>.

Bayesian classifiers can be trained using a set of labeled training data, and can be used to classify new and unseen data. They are relatively simple and efficient, and can be used in a variety of classification tasks. However, they may not be suitable for complex classification problems with a large number of features or classes.

The Bayesian classifier is an incremental method, which means that it can adapt over time to gradual or incremental changes in labelled training data.
It is a "black box" method, which means that it is <mark style="background: #ABF7F7A6;">not easily interpretable by humans</mark>, although its relationship to its training data is straightforward to understand.

### Limitation

The Bayesian classifier has a limitation in that it 
- <mark style="background: #FF5582A6;">requires sufficient observations</mark> in the training data to obtain reliable posterior probabilities. 
- If an observation in the testing data has a combination of attribute values that have not been seen in the training data, the classifier will assign a <mark style="background: #FF5582A6;">zero probability</mark> to it.
- This tendency toward zero probability increases as more attributes are incorporated into the model, as there needs to be <mark style="background: #ADCCFFA6;">at least one observation</mark> for every possible combination of attributes and target classes.
- The problem can be mitigated somewhat with naive Bayes that assumes class conditional independence, but the Laplacian correction is still needed when there is an attribute value that has not been seen in some class in the training data.

### Naive Bayes Classification

a classification algorithm that aims to derive the maximum posteriori, i.e., the maximal $P(C_i|X)$ using Bayesâ€™ theorem $P(C_i|X) = \frac{P(X|C_i)P(C_i)}{P(X)}$. In Naive Bayes, we simplify Bayes' theorem to reduce the computation cost of each likelihood in the training phase. We make an assumption that, within each class, each attribute is independent of all the others. This greatly reduces the computation cost as it only counts the distribution of values for each attribute in each class, independently of the other attributes. If $A_k$ is categorical, $P(x_k|C_i)$ is the number of tuples in $C_i$ having value $x_k$ for $A_k$ divided by $|C_{i, D}|$ (number of tuples of $C_i$ in $D$).

The Naive Bayes method assumes class conditional independence of attributes, which is not always true, but it seems to work. We can also use it for incremental learning. It is not really possible to humanly interpret the results (i.e. it is known as a "black box" method), although its relationship to its training data is straightforward to understand. It works best when there are less noise and redundant features, and the size of the data is not very large.

The formula for the <mark style="background: #FF5582A6;">Naive Bayes classifier with conditional independence</mark> assumption is:

$$P(C_i|X) = \frac{P(X|C_i)P(C_i)}{P(X)} = \frac{P(C_i) \prod_{k=1}^n P(x_k|C_i)}{\sum_{j=1}^m P(C_j) \prod_{k=1}^n P(x_k|C_j)}$$
where:
-   $P(C_i|X)$ is the posterior probability of class $C_i$ given the observation $X$
-   $P(C_i)$ is the prior probability of class $C_i$
-   $P(x_k|C_i)$ is the likelihood of attribute $k$ given class $C_i$
-   $P(X)$ is the marginal probability of observation $X$, which can be computed as $\sum_{i=1}^m P(C_i) \prod_{k=1}^n P(x_k|C_i)$


The Naive Bayes assumption of conditional independence of attributes given class $C_i$ can be expressed as:
$$P(x_1, x_2, ..., x_n|C_i) = \prod_{k=1}^n P(x_k|C_i)$$
where $x_k$ is the value of attribute $k$ in observation $X$. This assumption greatly simplifies the likelihood computation, since we only need to compute the likelihood of each attribute value for each class.

### Laplace correction

Laplace correction, also known as add-one smoothing, is <mark style="background: #ABF7F7A6;">a technique used to prevent zero probability estimates</mark> in cases where the frequency of an event is zero in the given training data. It is a simple and widely used method in probability theory and statistics to smooth categorical data.

The basic idea of Laplace correction is to <mark style="background: #ABF7F7A6;">add a small positive value</mark> to each count in the frequency table before calculating the probabilities. This ensures that no probability estimate is zero and that all possible outcomes have a non-zero probability estimate. The correction factor is usually set to 1, which is why it is sometimes called add-one smoothing.

The formula for Laplace correction can be expressed as follows: $P(x_i | c_j) = (count(x_i, c_j) + 1) / (count(c_j) + n)$

where:

-   $P(x_i | c_j)$ is the probability of observing feature $x_i$ given class $c_j$.
-   count$(x_i, c_j)$ is the number of times feature $x_i$ appears in class $c_j$.
-   count$(c_j)$ is the number of times class $c_j$ appears in the training set.
-   n is the total number of features in the dataset.

### Numerical attributes
This approach can also be used for ordinal variables, although depending on the application, and where the range of possible values is small, it may be more useful to treat ordinals as categorical even though the information of the order will not be used for prediction. It is common to assume that a continuous attribute follows a Gaussian distribution (also called normal, or bell curve). 

- Two parameters define : a Gaussian distribution mean $\mu$ and standard deviation $\sigma$ 
- Probability density function of Gaussian: $g(x,\mu,\sigma)$ = $\frac{1}{\sqrt{2\pi}\sigma}e^{\frac{-(x-\mu)^2}{2\sigma^2}}$ 
- Class conditional likelihood of kth-continuous attribute given class $C_i$ is $p(x_k|C_i) = g(x_k, \mu_{C_i}, \sigma_{C_i})$
