
### Problems
- Overfitting is a common problem in decision tree induction. It occurs when the <mark style="background: #FF5582A6;">model is too complex and fits the training data too well</mark>, leading to poor generalization on new, unseen data. 
- Overfitting can lead to poor predictive performance, and the decision tree may become too large and complex, making it difficult to interpret.

### Tree Pruning
Tree pruning is a technique used to <mark style="background: #FF5582A6;">prevent overfitting by reducing the size of the decision tree</mark>. It involves removing branches from the tree that do not contribute to improving its accuracy on new data. The goal is to create a smaller tree that is more interpretable and has better predictive performance.

- Pre-pruning, which involves stopping the tree construction process before it reaches a fully-grown tree. 
	- This is done by <mark style="background: #ADCCFFA6;">setting a threshold on some measure of impurity or information gain</mark>, such that if the gain falls below the threshold, the tree is not expanded further. This can help to prevent overfitting by limiting the complexity of the decision tree.
- Post-pruning, which involves constructing a complete decision tree and then removing branches to <mark style="background: #FFF3A3A6;">create a smaller tree</mark>. 
	- This is done by recursively removing the subtrees with the least improvement in accuracy on a validation set. The decision tree is then pruned at the node where removing the subtree leads to the highest improvement in accuracy.

