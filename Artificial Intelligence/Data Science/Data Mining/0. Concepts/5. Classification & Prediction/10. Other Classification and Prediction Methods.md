
>You use multiple linear regression to predict the value for a response variable  years_to_live from 4 numeric variables, age, income,  weight and  numchildren. Your trained model gives intercept $w_o$  = 9.0 and  coefficients $w_{age}$ = 0.13 and $w_{income}$= 0.0000 and $w_{weight}$ = 0.19 and $w_{numchildren}$ = 0.5. What does your model predict for the years to live for Joe who is 30 years old, earns $70000 p.a., weighs 80 kg and has no children? Give your answer to 1 decimal place.

```
years_to_live = 9.0 + 0.13*30 + 0.0000*70000 + 0.19*80 + 0.5*0 = 9.0 + 3.9 + 15.2 + 0 = 28.1
```

> Multiple linear regression and linearly separable SVM are similar in that they both find a hyperplane, but different because linear regression  finds a plane that _minimises_ the distance to the _target_ variable (numerical) values in the training set wheras the latter finds a plane that _maximises_ the distance to the independent (_input)_ variable values of each class in the training set.

```
True

The SVM plane maximises the distance to the independent variable values  (specifically the distance to the support vectors, which are those  objects that turn out to be the minimally distant ones from the plane)  but also must divide the dependent variable (which is a class label) into two, ie one class on each side of the plane. SVM does not care how far away the target variable (which is a class label) is from the plane.
```

> While a low RMSE and a high R-squared are great things to have, often when evaluating a linear regression model  it is a good idea to look at a  scatter plot of the predicted values for objects vs the observed values, together with the ideal line showing predicted = observed.  This may not be objective, but it is a useful subjective view of the quality.

```
True
```

> An analysis of variance (ANOVA) over a linear model built using linear regression can identify the input variables of high signficance in the model, using a t-test to determine the significance.  <mark style="background: #FF5582A6;">Highly significant variables</mark>

```
a.have non-zero coefficients in the model

b.are typically indicated by "***"

c.can be regarded as the most influential variables on the response variable
```

> One of the things  for the data miner to decide when training a neural network model  is what _activation function_ to use. What is the role of an activation function in a neural network learner?

```
To produce an output value of a network node based on a function of the weighted linear sum of input values and a bias value
```

> K-NN classifiers are fantastic because you do not need to train them at all, and they can start working for you with very little labelled data, and  then they just get better as you provide more labelled data over time. Really the only thing the data miner needs to do is to choose k, and maybe to choose an appropriate distance metric for the problem.

```
K-NN are lazy learners and  do not scale very well  because  they never build a model  that compacts and summarises the training data -- instead the training data itself acts as the model at run-time and a large training set is slow to process every time some new object needs to be classified.(slow)
```

> A 5-NN learner  (i.e a kNN leaner for k =5) predicts a value or a class label  for an object  X by:

```
Choosing the majority label or averaging the dependent variable value amongst the 5 training objects with the least distance from  X
  
Using the class labels or predicted values of the top 5 closest training objects, but weighting the contribution of each to the decision according to the inverse of their distance from X
```

> Which of the following learning methods rely on randomness, in which case it makes sense to attempt to train multiple models, even over the same training data, with different  random number seeds, and to choose the best of those models to test?

```
Neural Network

Genetic Algorithms
```

```
Neural networks use random initialization of weights, and genetic algorithms use random mutation and crossover operations. 

In both cases, different random number seeds can lead to different models and potentially different performance.
```

> When a classifier built for one training set is <mark style="background: #ADCCFFA6;">adapted (but not built from scatch) to a similar</mark>, but known to be different learning problem with its training set

```
Transfer learning
```

> When more than two classes are needed (say, n) and so you train one binary classifier for every <mark style="background: #FFB8EBA6;">pair of classes</mark>, that is n(n-1)/2 classifiers, using the tuples of the two classes

```
All-versus-all
```

```
All-versus-all, also known as pairwise classification, is a strategy or approach used in machine learning and classification tasks. In all-versus-all classification, a binary classifier is trained to distinguish between every pair of classes in a multi-class classification problem.

Instead of training a single classifier to differentiate between all classes simultaneously (one-versus-all or one-versus-rest approach), all-versus-all classification trains multiple binary classifiers, each dedicated to distinguishing between a specific pair of classes.
```

> When the classifier <mark style="background: #ABF7F7A6;">queries the user or its environment</mark> to label  signifcant training data that it needs.

```
Active learning
```

```
Active learning is a machine learning approach that involves an iterative process of selecting informative and relevant data samples to label and incorporate into the training set. Unlike traditional supervised learning, where a large labeled dataset is provided from the beginning, active learning aims to minimize the labeling effort by actively choosing the most valuable instances to be labeled.
```

```
Instead of passively relying on a fixed labeled dataset, active learning enables the model to interact with the environment to obtain additional labeled data points that are most informative or uncertain. By querying the environment for labels, the model can focus on challenging or ambiguous instances that are likely to have a high impact on its learning process.

The active learning process involves a feedback loop where the model learns from the labeled data obtained through querying and updates its knowledge to make better predictions. This iterative approach allows the model to gradually improve its performance by actively seeking the most relevant and informative training data.
```

> When you have a limited amount of labelled data and so you use unlabelled data as well, by having your label-trained classifier label the unlabelled data, or having multiple independent label-trained classifiers vote on the missing labels, and then train a new classifier with the bigger labelled training set.

```
Semi-supervised classification
```

```
Semi-supervised classification combines both labeled and unlabeled data for training a classification model. In traditional supervised learning, only labeled data is used to train the model, whereas in unsupervised learning, only unlabeled data is utilized. However, in semi-supervised classification, the model leverages both labeled and unlabeled data to improve its performance.
```

>When more than two classes are needed (say, n) and so you train n binary classifiers, each having <mark style="background: #FFF3A3A6;">its own class</mark> as positive examples and all the rest as negative examples

```
One-versus-all
```

```
One-versus-all (OVA), also known as one-versus-rest (OVR), is a technique used in multi-class classification problems. In this approach, a binary classification model is trained for each class, considering it as the positive class and the remaining classes as the negative class.
```

