
### IF-THEN rules for classification
Rules are a good way of representing information or bits of knowledge. A rule-based classifier uses a set of IF-THEN rules for classification.

-   "IF" part
    -   Rule antecedent / precondition
    -   Condition consists of one or more attribute tests that are logically ANDed
-   "Then" part
	-   Rule consequent
	-   Contains a class prediction

### Rule Evaluation
How to measure the quality of a single rule **R**? Use both _coverage_ and _accuracy._
The key thing to note is that the accuracy of a rule is in proportion to its coverage, not to the size of the dataset as a whole. 

-   _n_covers_ = number of tuples covered by **R,** i.e. that satisfy the antecedent of **R**
-   _n_correct_ = number of tuples correctly classified by **R** i.e that satisfy both the antecedent and the consequent
-   _D =_ training data set
-   **_coverage_**(**R**) = _n_covers_ /|_D_|
    -   the proportion of tuples that are covered by the rule 
-   **_accuracy_**(**R**) = _n_correct_ / _n_covers_
-   <mark style="background: #BBFABBA6;">Proportion</mark> of covered tuples that are correctly labelled
-   <mark style="background: #D2B3FFA6;">Knowledge</mark> graph learning, is usually called _standard confidence_.  

### Conflict Resolution
If more than one rule is triggered, need conflict resolution

-   Size ordering: assign the <mark style="background: #FFB8EBA6;">highest priority to the triggering rules</mark> that has the “toughest” requirement
	-   i.e., with the most attribute tests
-   Rule ordering: prioritise the rules beforehand by class-based or rule-based
	-   Class-based ordering: prioritise classes beforehand. If a tuple is classified into multiple classes, <mark style="background: #ABF7F7A6;">choose a class by the class order</mark>.
	-   Rule-based ordering: the rules are organised into one long priority list, according to some measure of rule quality

### Default Rule
- A default rule can be applied if there is no rule satisfied by a tuple.

---

### Rule Induction

IF-THEN rules can also be derived directly from the training data (i.e., without having to generate a decision tree first) using a sequential covering algorithm. Sometimes this is called _induction_ or _abduction,_ by reference to the language of logical inference, where rule languages have been extensively studied over a very long period.  

Well-known methods to use association rule mining to build classification rules:

#### Sequential covering algorithm

-   Extracts rules directly from training data
-   Typical sequential covering algorithms: FOIL, AQ, CN2, RIPPER
-   Rules are learned sequentially, each for a given class Ci will cover many tuples of Ci but none (or few) of the tuples of other classes
-   Steps:
    -   Rules are learned one at a time
    -   Each time a rule is learned, the tuples covered by the rules are removed from the training set  
        
    -   The process repeats on the remaining tuples until a  termination condition
        -   e.g., when no more training examples or when the quality of a rule returned is below a user-specified threshold

**How each rule is learned**

-   Rules are grown in a general-to-specific manner.
-   Choose a rule consequent (usually the positive class)  
-   Initialise an empty antecedent
-   Repeat:  
	-   Add an attribute test to the rule antecedent
	-   Until satisfied with this rule.


**Compared to rules derived from a Decision Tree**

-   Rules  may be presented in a  more expressive language, such as a relational language (e.g. can use relations like "adjacent"  or "greater than"  that relate attributes of covered tuples).
-   Rules may be more compact and readable than decision tree rules (because they do not branch from a common stem and tuples covered by an earlier rule are removed from the training set for learning the next rule).