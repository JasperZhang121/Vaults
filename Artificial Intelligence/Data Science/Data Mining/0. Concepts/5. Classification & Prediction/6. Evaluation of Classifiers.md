
### Confusion Matrix

-   The confusion matrix is a tool for <mark style="background: #BBFABBA6;">evaluating the performance of a classifier</mark> on a binary classification problem.
-   It is a 2 by 2 matrix where each entry represents the number of tuples classified as belonging to a particular class (positive or negative) versus the actual class of those tuples.
-   The entries of the confusion matrix are calculated as follows:
    -   True Positives (TP): number of positive tuples that were correctly labeled as positive by the classifier.
    -   True Negatives (TN): number of negative tuples that were correctly labeled as negative by the classifier.
    -   False Positives (FP): number of negative tuples that were incorrectly labeled as positive by the classifier.
    -   False Negatives (FN): number of positive tuples that were incorrectly labeled as negative by the classifier.
-   The TP and TN entries indicate correct predictions, while the FP and FN entries indicate incorrect predictions.
-   The confusion matrix can be used to calculate <mark style="background: #FFF3A3A6;">various evaluation metrics such as accuracy, precision, recall, and F1-score</mark>.
-   Accuracy is the proportion of correctly labeled tuples out of the total number of tuples. However, it can be misleading if the dataset is imbalanced.
-   Precision is the proportion of true positive predictions out of all positive predictions, while recall (or sensitivity) is the proportion of true positive predictions out of all actual positive tuples.
-   F1-score is the harmonic mean of precision and recall, and is useful when you want to balance precision and recall.
-   It's important to choose an appropriate evaluation metric based on the specific problem and the domain of application.
-   Confusion matrices can also be used for multiclass classification problems by extending the matrix to include all possible combinations of predicted and actual classes.

**General Format:**

| Actual Class (rows) \ Predicted Class (columns) | C1=Positive | C2=Negative |
| --- | --- | --- |
| C1=Positive | True Positives (TP) | False Negatives (FN) |
| C2=Negative | False Positives (FP) | True Negatives (TN) |

---

### Evaluation Measures:
| Evaluation Measure | Formula |
|--------------------|---------|
| Accuracy           | $accuracy = \frac{TP+TN}{P+N}$ |
| Error rate         | $error \ rate = \frac{FP+FN}{P+N}$ |
| Precision          | $precision = \frac{TP}{TP+FP}$ |
| Sensitivity (Recall)| $sensitivity = \frac{TP}{P}$ |
| Specificity        | $specificity = \frac{TN}{TN+FP}$ |
| F1-score           | $F1 = 2 \cdot \frac{precision \cdot recall}{precision + recall}$ |
| $F_\beta$, where beta is a non-negative real number       | $F_\beta = (1+\beta^2)\frac{precision \cdot recall}{(\beta^2 \cdot precision) + recall}$ |


### Class Imbalance Problem
- The <mark style="background: #FFF3A3A6;">distribution of classes in a dataset is not balanced</mark>. 
- This can lead to skewed performance evaluation, particularly if the evaluation measure used is accuracy. 
- When dealing with a <mark style="background: #ABF7F7A6;">rare class, a high accuracy may not indicate good classification performance</mark>.
- Instead, evaluation measures such as sensitivity, specificity, precision, recall, and F1-score can be used to better assess the performance of the classifier.

#### Cancer example
- Consider a cancer classification problem where only a small proportion of patients have cancer. 
- A classifier with high accuracy may still fail to identify many cancer patients, leading to a <mark style="background: #ADCCFFA6;">high rate of false negatives</mark>. 
- In this case, <mark style="background: #D2B3FFA6;">sensitivity</mark> (the proportion of actual positives that are correctly identified by the classifier) becomes a more relevant evaluation measure than specificity (the proportion of actual negatives that are correctly identified by the classifier).

- Precision and recall are alternative measures that can be useful in evaluating classification performance, particularly when the classes are imbalanced. 
- Precision measures the proportion of true positive predictions among all positive predictions, while recall measures the proportion of true positive predictions among all actual positive tuples. 
- These measures are often combined into an F1-score, which provides a <mark style="background: #FFF3A3A6;">single measure of performance that balances precision and recall</mark>. However, it's important to choose the appropriate evaluation metric for the specific problem and domain of application, as different evaluation measures may emphasize different aspects of classification performance.

---

### Estimating a classifierâ€™s accuracy

The accuracy or error rate on the training data is not a good way to judge how well a model will perform on new data because the model may have learned to remember the training data instead of learning to generalize to new data. This is called <mark style="background: #FFB8EBA6;">overfitting</mark> and can result in poor performance on new data.

### Holdout method

-   <mark style="background: #BBFABBA6;">Accuracy or error rate</mark> on the training data is <mark style="background: #FFF3A3A6;">not a reliable</mark> indicator of future performance, as the model may be overfitting to the training data, leading to poor performance on new, unseen data.
-   The holdout method is a technique used to estimate the <mark style="background: #D2B3FFA6;">performance of a model on new, unseen data</mark> by randomly partitioning the available data into two sets: a training set, used for model construction, and a test set, used for accuracy estimation.
-   Typically, around two-thirds of the data is used for training and one-third for testing, although this split can vary depending on the size and nature of the dataset.
-   The holdout method provides a simple and effective way to estimate the generalization error of a model and to compare the performance of different models. However, it has some limitations, such as potentially yielding unstable estimates for small datasets, and potentially introducing bias if the test set is not representative of the overall population.
- The <mark style="background: #FF5582A6;">difference between the holdout method and other</mark> training and testing approaches lies mainly in the way the data is split. In the holdout method, the data is usually <mark style="background: #ABF7F7A6;">split into two roughly equal parts</mark>, whereas other approaches such as k-fold cross-validation involve splitting the data into multiple parts and using each part in turn for testing and training.

### Training / Validation / Test

- In this method, the given data is randomly partitioned into <mark style="background: #ADCCFFA6;">three different sets</mark>
	- Training set
	- Validation set
	- and Test set
- The training set is used to construct a classification model, while the validation set is used to find the best parameters for the model. This is typically done by studying the effect of various parameters on the accuracy or other quality measures of the training set. Finally, the test set is used to measure the final performance of the model, as it would be reported.

This method is useful when you want to experiment with the parameters of the model-building algorithm. By having a separate validation set, you can ensure that you're not overfitting to the training set and that you're selecting the best parameters for the model. The test set provides a final evaluation of the model's performance on unseen data. It's important to keep the test set separate from the training and validation sets to avoid bias and ensure that the reported performance is accurate.


### Cross-validation

- Cross-validation is a popular technique used in machine learning to evaluate the accuracy of a model's performance. 
- The most commonly used method is<mark style="background: #ADCCFFA6;"> k-fold cross-validation</mark>, where k=10, which is preferred due to its low bias and variance. In this method, the data is randomly partitioned into k mutually exclusive subsets, each of approximately equal size. In each iteration, <mark style="background: #ABF7F7A6;">one of the subsets is used as the test set, while the remaining subsets are used as the training set</mark>. The leave-one-out method is a special case for small-sized datasets, use k-fold where k is equal to the number of tuples. 
- Another special case is <mark style="background: #ADCCFFA6;">stratified cross-validation</mark>, where folds are not randomly selected but stratified so that the class distribution in each fold is approximately the same as that in the initial data to achieve low bias. The overall accuracy of the model is computed by averaging the accuracy of each model on its respective test set.


### Bootstrap

- Works well for small data sets where, otherwise, the requirement to split the data into training and testing sets makes both sets too small for purpose.
- <mark style="background: #FFF3A3A6;">Samples the given training tuples uniformly with replacement</mark>, i.e., each time a tuple is selected, it is equally likely to be selected again and added to the training set again.

#### Several bootstrap methods
 - Acommon one is .632 bootstrap. A data set with $d$ tuples is sampled $d$ times, with replacement, resulting in a training set of $d$ samples. 
 - The data tuples that did not make it into the training set end up forming the test set. About <mark style="background: #BBFABBA6;">63.2% of the original data end up in the training set, and the remaining 36.8% form the test set</mark> (since $(1 - 1/d)^d \approx e^{-1} = 0.368$ if $d$ is very large).

Repeat the sampling procedure $k$ times. The overall accuracy of the model is:
$\text{bootstrap overall accuracy} = \frac{1}{k} \sum_{i=1}^{k} \left[ 0.632 \cdot Acc(M_i)_{\text{test set}} + 0.368 \cdot Acc(M_i)_{\text{train set}} \right]$

where $Acc(M_i)_{\text{test set}}$ is the accuracy of the model trained with training set $i$ when it is applied to test set $i$, and $Acc(M_i)_{\text{train set}}$ is the accuracy of the model obtained with training set $i$ when it is applied to the training set $i$.


### Comparing classifiers using t-tests

In comparing two classifiers, M1 and M2, mean error rates are obtained using 10-fold cross-validation. 
- While it may seem intuitive to choose the classifier with the lowest error rate, these mean error rates are only estimates of future data cases. 
- There may also be a chance that the <mark style="background: #D2B3FFA6;">difference between the two error rates is attributed to chance</mark>. In order to confirm the statistical significance, confidence limits are obtained for the error estimates. 
- Once the confidence limits have been obtained, it can be concluded whether one classifier is statistically better than the other.


#### Estimating Confidence Intervals: Null Hypothesis

Null Hypothesis: M1 & M2 are the same Test the null hypothesis with t-test Use t-distribution with k-1 degree of freedom If we can reject null hypothesis, then we <mark style="background: #ABF7F7A6;">conclude that the difference between M1 & M2 is statistically significant</mark>. Chose model with lower error rate Perform 10-fold cross-validation (k=10) For i-th round of 10-fold cross-validation, the same cross partitioning is used to obtain $err(M1)_i$ and $err(M2)_i$. Average over 10 rounds to get $\overline{err}(M1) = \frac{1}{k} \sum\limits_{i=1}^{k} err(M1)$ and similarly for $\overline{err}(M2)$ t-test computes t-statistic with k-1 degrees of freedom: $$t = \frac{\overline{err}(M1) - \overline{err}(M2)}{\frac{var(M_1 - M_2)}{k}}$$
where (using the variance for the population, as given in the text): $$var(M_1 - M_2) = \frac{1}{k}\sum\limits_{i=1}^{k} (err(M1)_i - \overline{err}(M1))^2 - (err(M2)_i - \overline{err}(M2))^2$$To determine whether M1 and M2 are significantly different, we compute t-statistic and select a significance level. 5% significance levels: The difference between M1 and M2 is significantly different for 95% of population. 1% significance levels: The difference between M1 and M2 is significantly different for 99% of population. Based on t-statistics and significance level, we consult a table for the t-distribution. We need to find the t-distribution value corresponding to k âˆ’ 1 degrees of freedom (or 9 degrees of freedom for our example) from the table (Two-sided). T-distribution table ([https://www.statisticshowto.com/tables/t-distribution-table/](https://www.statisticshowto.com/tables/t-distribution-table/))

If the t-statistic we calculated above is not between the corresponding value in the table and its negative (i.e. the corresponding value in the table multiplied by -1), then we reject the null hypothesis and conclude that M1 and M2 are significantly different (at the significance level we chose above). Alternatively, if the t-statistic we calculated above is between the corresponding value in the table and its negative, we conclude that M1 and M2 are essentially the same and any difference is attributed to chance.


### ROC Curve to Compare Probablistic Classifiers

-   ROC curves are used for <mark style="background: #FFF3A3A6;">comparing probabilistic classification models and selecting a decision threshold</mark>.
-   The curves show the <mark style="background: #BBFABBA6;">trade-off</mark> between the true positive rate (TPR) and the false positive rate (FPR).
-   A deterministic classifier is represented as a single point on the ROC chart, while a probabilistic classifier is plotted as a curve.
-   The ROC curve helps in choosing a decision threshold for the probabilistic classifier that reflects the tradeoff between TPR and FPR.
-   The area under the ROC curve (ROC-AUC) is often used to measure the performance of a probabilistic model.
-   The diagonal line on the graph represents a model that randomly labels the tuples according to the distribution of labels in the data.
-   Many deterministic models with distinct (FPR, TPR) points on the graph share the same ROC-AUC, but the visual study of the chart might help distinguish them.
-   A ROC-AUC of 1 indicates a perfect classifier, while a ROC-AUC of 0 denotes a worst-case model.
-   The ROC-AUC allows for unbalanced datasets by valuing errors in each class of the dataset independently of the proportion of each class in the dataset as a whole.
-   The ROC can be used to choose a cost or risk function to be used with a deterministic classifier.
-   The ROC may be used together with cross-validation to avoid being overly influenced by a particular training set.

#### Plotting an ROC curve for a probabilistic classifier

1.  Rank the test tuples in <mark style="background: #ABF7F7A6;">decreasing order based on their probability of belonging to the positive class</mark>, with the highest probability at the top of the list.
    
2.  Start at the bottom left corner of the graph, where TPR = FPR = 0.
    
3.  Check the tupleâ€™s actual class label at the top of the list. If it is a true positive (i.e., a positive tuple that was correctly classified), both TP and TPR increase, and we move up and plot a point on the graph.
    
4.  If the model classifies a negative tuple as positive, it is a false positive (FP), and both FP and FPR increase. We move right on the graph and plot a point.
    
5.  Repeat steps 3 and 4 for each of the test tuples in ranked order, each time moving up on the graph for a true positive or to the right for a false positive.
    

The vertical y-axis of an ROC curve represents TPR, while the horizontal x-axis represents FPR.


#### Explanation 

-   ROC (Receiver Operating Characteristic) graphs<mark style="background: #ADCCFFA6;"> summarize the performance of a binary classifier</mark> across all possible decision thresholds by showing the tradeoff between the true positive rate (TPR) and false positive rate (FPR).
-   The ROC curve is created by plotting the TPR on the y-axis against the FPR on the x-axis at various threshold settings.
-   A <mark style="background: #FFB86CA6;">good classifier will have a ROC curve that is close to the upper left corner</mark> of the graph, indicating <mark style="background: #FF5582A6;">high TPR with low FPR</mark>.
-   The area under the ROC curve (ROC-AUC) is a common metric used to evaluate the performance of a classifier.
-   A perfect classifier will have an ROC-AUC of 1, while a random classifier will have an ROC-AUC of 0.5.
-   ROC curves can be used to compare the performance of different classifiers or to determine the optimal threshold for a given classifier.
-   The optimal threshold can be chosen by selecting the point on the ROC curve that balances TPR and FPR according to the needs of the application.

### Other issues affecting the quality of a model

Factors to consider when comparing the quality of probabilistic models:

1.  Accuracy: The ability of the classifier to <mark style="background: #ABF7F7A6;">correctly predict</mark> the class label of a tuple.
    
2.  Speed: The <mark style="background: #ADCCFFA6;">time taken</mark> to construct the model (training time) and use the model (classification/prediction time).
    
3.  Robustness: The ability of the model to <mark style="background: #D2B3FFA6;">handle noise and missing values</mark>.
    
4.  Scalability: The efficiency of the model in disk-resident databases.
    
5.  Interpretability: The <mark style="background: #FFB8EBA6;">understanding and insight provided</mark> by the model. This is important in cases where the model is intended to influence systemic behaviors or enable qualitative evaluation of the model for embedded bias.
    
6.  Availability and Trust: The <mark style="background: #FFF3A3A6;">technical infrastructure, skills, policy or governance framework required to use the model</mark>, and whether the business environment will trust the results for the intended purpose.
    
7.  Other measures specific to the method: Goodness of rules, such as decision tree size or compactness of classification rules.

