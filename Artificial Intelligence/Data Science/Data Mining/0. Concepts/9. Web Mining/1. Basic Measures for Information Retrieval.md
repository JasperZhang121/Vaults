Evaluation in information retrieval involves assessing the quality of retrieved documents based on their relevance to a query. The following measures are commonly used:

### Precision (P): 
It is the proportion of retrieved documents that are actually relevant to the query. Precision is calculated using the formula: (relevant: actual positives)

$$P = (relevant ∧ retrieved) / retrieved$$

where TP (True Positives) represents the number of relevant documents correctly retrieved, and FP (False Positives) represents the number of irrelevant documents incorrectly retrieved.

### Recall (R) 
It is the proportion of relevant documents that were successfully retrieved by the system. Recall is calculated using the formula:

$$R = (relevant ∧ retrieved) / relevant$$

where FN (False Negatives) represents the number of relevant documents that were not retrieved.

### F-score (F1 score):
It combines precision and recall into a single measure, considering the balance between them. The F-score is calculated using the formula:

$$F1 = 2 * (P * R) / (P + R)
$$
The F1 score is the harmonic mean of precision and recall, ensuring that both measures contribute equally to the overall score.

---
### Typical information retrieval systems

-   Online library catalogues
-   Online document management systems
-   News and legal databases
-   Recommender systems (that _push_ rather  than _pull_ information)
-   While IR has a long history, the innovation of **Web search engines** has driven development in the past 25 years.

#### **Information retrieval (IR) versus database (DB) systems**

-   Some DB problems are not present in IR, such as: updates, transaction management, complex structured objects
-   Some IR problems are not addressed well in DBMS, for example:
    -   unstructured documents
