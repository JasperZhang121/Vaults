> Text mining is founded in Information Retrieval research, which also developed Web Search. How does Information Retrieval relate to Database Systems?

Database systems are designed for some things that IR systems do not offer, including
`transaction management, structured objects, specialist data types like spatial and temporal, update`

IR systems work well for some things that Database systems do not, such as
`unstructured and semi-structured documents, approximate queries, best answers amongst many alternatives`

> It is usual for any text retrieval or text mining application to pre-process collection data as follows.

- _Tokenise_ to split text into words and remove punctuation, 

- _Normalise_ to translate each token to a form that will be considered equivalent to the original form,. <mark style="background: #ABF7F7A6;">example (convert "run," "runs," and "running" into the base form "run.")</mark>

- Remove _stop words_, 

- _Stop words_ could be customised to the domain of application so that overly frequent, non-selective words in the domain are removed

> Which of the following terms has been stemmed as a stage of text preprocessing?

`Run`

>![[web_mining.png]]
>The table here shows a <mark style="background: #FFB8EBA6;">term-document matrix (TDM)</mark>. Which of the following are true statements about the TDM?

- If we update the TDM for Doc 4 which says "Elephants roam freely in Melbourne." then the Total count for term "melbourne" will increase by 1 to 5
- Allowing for stop words and normalisation, it is plausible that Doc2 is the sentence "Zoo animals eat their food very slowly."

> Consider the  query "Who is crazy enough to eat food in Melbourne"?Referring to the DTM above, what is the feature vector for the query?

- (0,1,0,1,0,0,1,0,1,0)

> Consider the  query "Who is crazy enough to eat food in Melbourne"?
> 
> Referring to the DTM above, what is the cosine similarity of the query and Doc1?
> 
> Give your answer to two decimal places.

$$\text{cosine similarity}(A, B) = (A . B) / (||A|| * ||B||)$$
where:
-   (A . B) represents the dot product of vectors A and B
-   ||A|| represents the Euclidean norm or length of vector A
-   ||B|| represents the Euclidean norm or length of vector B

To calculate the cosine similarity, follow these steps:

1.  Compute the dot product of vectors A and B. The dot product is the sum of the element-wise multiplication of corresponding elements in the vectors. $(A . B) = A[0] * B[0] + A[1] * B[1] + ... + A[n] * B[n]$
    
2.  Compute the Euclidean norm or length of vector A. The Euclidean norm of a vector is the square root of the sum of the squares of its elements. $||A|| = sqrt(A[0]^2 + A[1]^2 + ... + A[n]^2)$
    
3.  Compute the Euclidean norm or length of vector B. $||B|| = sqrt(B[0]^2 + B[1]^2 + ... + B[n]^2)$
    
4.  Divide the dot product by the product of the vector lengths to obtain the cosine similarity. $\text{cosine similarity}(A, B) = (A . B) / (||A|| * ||B||)$

```
A.B = 3+4+5 = 12

||A|| = sqrt(4+9+4+25+16) = sqrt(58)

||B|| = sqrt(1+1+1+1) = sqrt(4) = 2

similarity(A, B) = (A . B) / (||A|| * ||B|| = 12/(sqrt(58)*2) = 0.79

```


> Refining the term frequency vector, we can use _logarithmic term frequency_  or _inverse document frequency_ <mark style="background: #D2B3FFA6;">instead of raw term frequency</mark> as above because

- compared to raw term frequency, logarithmic term frequency <mark style="background: #ADCCFFA6;">reduces the importance of terms seeming very frequent</mark> due to the document being long.,

- compared to raw term frequency, logarithmic term frequency recognises that while term importance  to a document  increases with each repetition,  the <mark style="background: #CACFD9A6;">significance of each additional repetition drops off when frequencies are higher</mark>.,

- logarithmic term frequency  (or even raw term frequency) and inverse document frequency may be multiplied to give  _TF-IDF_ which gives more weight to frequent terms (from  the TF part) and to discriminative, selective terms (the IDF part).


> Match the mining method to the  text mining problem it can solve.

Hierarchical clsutering can be used to `group related documents and to derive a taxonomy of terms to describe the groups, Classification algorithms, such as SVM, Naive Bayes, and Decision trees can be used to`

Classification algorithms, such as SVM, Naive Bayes, and Decision trees can be used to `classify documents modelled as weighted term vectors into topic classes`

By viewing the set of terms in a document as a transaction, associating mining can be used to `discover multi-word named entities because they occur frequently together`

> Web mining techniques include

- mining  the <mark style="background: #CACFD9A6;">structure of XML or HTML</mark> to relate text to images for image labelling or to recreate the relationship of textual fragments to each other, 

- mining the <mark style="background: #ABF7F7A6;">content (text)</mark> in nodes together with the link structure, for example for finding authoritative pages on some topic, 

- mining site <mark style="background: #FFB8EBA6;">access logs for trend analysis</mark>, improved site design, and drivers to purchase


> For training a language model, a massive corpus of text should be fed in to a neural network. This is often called _pretraining_. The model thus learned is task-independent, so it could be used in many different downstream tasks (e.g. translation and sentiment analysis). Hence, in this setting, two learning mechanisms are involved, language model learning in the first phase and task-dependent learning in a second phase.  Considering the two learning phases respectively, which type of learning alorgithm is both viable and minimises the effort for obtaining training data?

`1. unsupervised learning 2. few-shot learning`

