 Text processing involves working with a sequence of words or a bag of words, where words are referred to as terms. These terms are aggregated into documents, and a collection or corpus is formed by aggregating multiple documents. A document can be described using representative keywords known as index terms.

-   Index terms serve as <mark style="background: #ABF7F7A6;">attributes</mark> in Information Retrieval and can be binary-valued, indicating their absence or presence.

However, not all index terms have the same relevance in describing document contents. This is addressed by assigning numerical weights to each index term, such as frequency or TF-IDF, which act as attribute values.

**Basic Process:**

1.  Select index terms.
2.  Build an index, which involves creating high-dimensional term and document frequency matrices.
3.  Match the query to the index to retrieve optimal answers using approaches like the Boolean model, Vector space model, or Probabilistic model (where categories are modeled using probability distributions to determine the likelihood of a document belonging to a specific category, similar to Bayesian classification).

---
  
### Selecting index terms

1.  Tokenising: The document is divided into individual words or tokens by separating them based on spaces and other punctuation marks. Punctuation is also removed.
    
2.  Normalisation and Stemming: The words are transformed to a canonical or standard form to eliminate variations in syntax. Stemming involves cutting off suffixes (and sometimes prefixes) that serve grammatical purposes but do not contribute to the word's meaning. For example, "skipping" is stemmed to "skip." Normalisation maps a term to another term that has the same meaning. For instance, "Practise" is normalised to "practice."
    
3.  Remove Stop Words: Common words that carry little meaning, such as articles (e.g., "the," "of," "for," "to," "with"), are removed from the index. Stop words can also be chosen based on the specific problem domain. For example, in a legal database, the term "law" might be considered a stop word.

![[text_mining.png]]

#### Building an Index

Indexes need to link the (preprocessed) words in the document collection to the documents in which they occur. This is typically a **term-document matrix**  or similar **inverted index**. A **signature file** is an alternative approach.

An inverted index is well-suited to parallel computation using methods like MapReduce over distributed file systems.

![[text_indexing.png]]

### Matching the query to the index 
Assessing the similarity between a document and a query, which consists of a set of keywords. Different similarity metrics are used to measure the closeness of a document to the query.

Boolean queries: In Boolean queries, such as those used in a library catalogue, the query is constructed using <mark style="background: #FFB8EBA6;">index terms connected by logical operators (not, and, or)</mark>. The index can directly retrieve matching documents based on these queries, sometimes after preprocessing to remove stop words.

Vector space model: The vector space model and document clustering approaches require a more sophisticated notion of document similarity. In this model, the query itself is treated as a document and is compared to other documents based on similarity. Ranking of matching documents is done by utilizing dot product or cosine similarity with weighted term vectors.

-   Dot Product similarity: This measures the similarity between two documents by taking the dot product of their weighted term vectors.
    $$Dot_Product(A, B) = A · B = ∑(A_i * B_i)$$
-   Normalized Dot Product (cosine similarity): This is a variation of dot product similarity that normalizes the vectors, resulting in the cosine of the angle between them. If the vectors are orthogonal (angle of 90°), the similarity is 0, while if they are co-linear (angle of 0°), the similarity is 1.
$$Cosine_Similarity(A, B) = (A · B) / (||A|| * ||B||)$$
Sophisticated weighting schemes, beyond simple boolean weights or term frequency counts, have proven to be beneficial in capturing the true semantic relationships between documents and queries.


### Assign weights to term occurrences:

Here are three weighting heuristics based on term frequency and document frequency, but many other variants are used in practice. TF-IDF is a very common choice.

1.  TF (Term Frequency):

<mark style="background: #ABF7F7A6;">More frequent</mark> within a document -> more <mark style="background: #BBFABBA6;">relevant to the semantics</mark> of the document as a whole. Example: "classification" versus "SVM" Raw TF (Term Frequency) = tf(t, d) from the term-document matrix, which counts how many times term t appears in document d. However, document length varies, so the relative frequency within the document is preferred to avoid bias against short documents. Relevance is not linearly proportional to the term frequency, so normalization or scaling is performed. There are various ways to achieve this. 

One example is the Logarithmic Term Frequency:
TF(t, d) = { 0, if tf(t, d) = 0, 1 + log10(tf(t, d)), otherwise. } 

2.  IDF (Inverse Document Frequency):

Terms that are less frequent among documents in the collection are more discriminative and hence more useful. Example: "algebra" versus "science" To assign a higher weight to rare terms than frequent terms, IDF (Inverse Document Frequency) is used. IDF(t) = log(n / k), where: n = total number of documents k = number of documents with term t appearing in them at least once (also called DF(t)) Example: Inverse document frequency, Kerry.

3.  TF-IDF (Term Frequency - Inverse Document Frequency):

TF and IDF may be combined to form the TF-IDF measure.

$$TF-IDF(t, d) = TF(t, d) * IDF(t) 
$$
Terms that are frequent within a document tend to have a high TF, which in turn leads to a high TF-IDF weight. Terms that are selective among documents tend to have a high IDF, which also contributes to a high TF-IDF weight.