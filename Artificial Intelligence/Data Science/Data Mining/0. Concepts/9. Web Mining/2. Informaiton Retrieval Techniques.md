Text processing involves working with a sequence of words or a bag of words, where words are referred to as terms. These terms are aggregated into documents, and a collection or corpus is formed by aggregating multiple documents. A document can be described using representative keywords known as index terms.

Index terms serve as <mark style="background: #ABF7F7A6;">attributes</mark> in Information Retrieval and can be binary-valued, indicating their absence or presence.

However, not all index terms have the same relevance in describing document contents. This is addressed by assigning numerical weights to each index term, such as frequency or TF-IDF, which act as attribute values.

### **Basic Process:**

1.  Select index terms.
2.  Build an index, which involves creating high-dimensional term and document frequency matrices.
3.  Match the query to the index to retrieve optimal answers using approaches like the Boolean model, Vector space model, or Probabilistic model (where categories are modeled using probability distributions to determine the likelihood of a document belonging to a specific category, similar to Bayesian classification).

---
  
### Selecting index terms

1.  Tokenising: The document is divided into individual words or tokens by separating them based on spaces and other punctuation marks. Punctuation is also removed.
    
2.  Normalisation and Stemming: The words are transformed to a canonical or standard form to eliminate variations in syntax. Stemming involves cutting off suffixes (and sometimes prefixes) that serve grammatical purposes but do not contribute to the word's meaning. For example, "skipping" is stemmed to "skip." Normalisation maps a term to another term that has the same meaning. For instance, "Practise" is normalised to "practice."
    
3.  Remove Stop Words: Common words that carry little meaning, such as articles (e.g., "the," "of," "for," "to," "with"), are removed from the index. Stop words can also be chosen based on the specific problem domain. For example, in a legal database, the term "law" might be considered a stop word.

![[text_mining.png]]

#### Building an Index

Indexes need to link the (preprocessed) words in the document collection to the documents in which they occur. This is typically a **term-document matrix**  or similar **inverted index**. A **signature file** is an alternative approach.

An inverted index is well-suited to parallel computation using methods like MapReduce over distributed file systems.

![[text_indexing.png]]

### Matching the query to the index 
Assessing the similarity between a document and a query, which consists of a set of keywords. Different similarity metrics are used to measure the closeness of a document to the query.

Boolean queries: In Boolean queries, such as those used in a library catalogue, the query is constructed using <mark style="background: #FFB8EBA6;">index terms connected by logical operators (not, and, or)</mark>. The index can directly retrieve matching documents based on these queries, sometimes after preprocessing to remove stop words.

Vector space model: The vector space model and document clustering approaches require a more sophisticated notion of document similarity. In this model, the query itself is treated as a document and is compared to other documents based on similarity. Ranking of matching documents is done by utilizing dot product or cosine similarity with weighted term vectors.

-   Dot Product similarity: This measures the similarity between two documents by taking the dot product of their weighted term vectors.
    $$Dot_Product(A, B) = A · B = ∑(A_i * B_i)$$
-   Normalized Dot Product (cosine similarity): This is a variation of dot product similarity that normalizes the vectors, resulting in the cosine of the angle between them. If the vectors are orthogonal (angle of 90°), the similarity is 0, while if they are co-linear (angle of 0°), the similarity is 1.
$$Cosine_Similarity(A, B) = (A · B) / (||A|| * ||B||)$$
Sophisticated weighting schemes, beyond simple boolean weights or term frequency counts, have proven to be beneficial in capturing the true semantic relationships between documents and queries.


### Assign weights to term occurrences:

Here are three weighting heuristics based on term frequency and document frequency, but many other variants are used in practice. TF-IDF is a very common choice.

1.  TF (Term Frequency):

<mark style="background: #ABF7F7A6;">More frequent</mark> within a document -> more <mark style="background: #BBFABBA6;">relevant to the semantics</mark> of the document as a whole. Example: "classification" versus "SVM" Raw TF (Term Frequency) = tf(t, d) from the term-document matrix, which counts how many times term t appears in document d. 

However, document length varies, so the <mark style="background: #FFF3A3A6;">relative frequency within the document is preferred to avoid bias against short documents</mark>. Relevance is not linearly proportional to the term frequency, so normalization or scaling is performed. There are various ways to achieve this. 

One example is the Logarithmic Term Frequency:
TF(t, d) = { 
				0, if tf(t, d) = 0, 
				1 + log10(tf(t, d)), otherwise. } 

2.  IDF (Inverse Document Frequency):

Terms that are less frequent among documents in the collection are more discriminative and hence more useful. Example: "algebra" versus "science" To assign a higher weight to rare terms than frequent terms, IDF (Inverse Document Frequency) is used. $IDF(t) = log(n / k)$, where: n = total number of documents k = number of documents with term t appearing in them at least once (also called DF(t)) Example: Inverse document frequency, Kerry.

3.  TF-IDF (Term Frequency - Inverse Document Frequency):

$$IDF(t) = log(n/k)$$
_n_ = total number of documents  
_k_ = number of documents with term _t_ appearing  in them at least once (also called _DF(t)_)

TF and IDF may be combined to form the TF-IDF measure.

$$TF-IDF(t, d) = TF(t, d) * IDF(t) 
$$
Terms that are frequent within a document tend to have a high TF, which in turn leads to a high TF-IDF weight. Terms that are selective among documents tend to have a high IDF, which also contributes to a high TF-IDF weight.

#### Ranking in the vector space model

1. Documents: Weight document terms by TF.
2.  Query: Weight query terms as TF-IDF.
3.  Compute relevance (Q, $D_i$) = cosine(Q, $D_i$).

That is, the relevance of document $D_i$, expressed as a TF-weighted vector, to the query Q, expressed as a TF-IDF-weighted vector, is given by the cosine similarity of the vectors.

In the above, sometimes the Query and Document vectors are normalized in steps 1 and 2, respectively, by dividing each element of the vector by the length of the vector (also called the vector norm or Euclidean norm). This computation involves taking the square root of the sum of the squares of each element (similar to the Pythagorean theorem). Normalizing ensures that each element in the vector is within the interval [0,1]. However, it is worth noting that in step 3, when applying the cosine formula, the divisor is the product of the lengths of the two vectors. Therefore, normalizing the vectors beforehand is not strictly necessary to achieve the same result.

Example: The visualization below depicts the query and three documents (D1, D2, and D3) projected onto a 2-dimensional vector space (or where |v| = 2, representing two dimensions). Among these, D3 is the most relevant because the angle between the Query and D3 is the smallest. Similarly, the ranking order based on relevance will be D3, D2, D1.

![[cosine_90.png]]

**Evaluate relevance ranking**
-   Carry out experiments using Precision, Recall or F-score, where  you have a "**gold standard**" set of validation queries with the "right" answers already selected by people.
-   On-line learning from user behaviour and feedback can be used to improve performance: _relevance feedback_
-   This is the underlying basis of modern web search, but there are **many** more things done, too.
