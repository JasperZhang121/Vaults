
Recently, the **Word2Vec** models have become very popular for their ability to  represent word meaning as a vector of probabilities of association with other _context_ words, partially replacing earlier approaches such as _latent semantic analysis_.

One important training algorithm is <mark style="background: #BBFABBA6;">Continuous Bag of Words</mark> whereby neural net classifiers are trained on a _window_ of the words surrounding the target word in the corpus to predict the missing word. The other algorithm, **Continuous Skip-gram** does the reverse and predicts the surrounding words based on a single focus word. 

It turns out that this causes the neural net to represent _meaning_ in the sense that the vectors in the learnt classifiers for words can be arithmetically manipulated to derive the representations for similar words. Famously,

vector(”King”) - vector(”Man”) + vector(”Woman”) = vector("Queen")


### Language Models

The success of word2vec initiated a furious interest in <mark style="background: #D2B3FFA6;">building language models that aim to build a domain-independent representation of words</mark> (that is, language) that can be used in many different language tasks.  These language models typically comprise both a trained neural network and _embeddings,_ where embeddings are vectors of the form produced by word2vec, that is, for each word, a vector of values that represent the meaning of a word via its relationship to other words in training text.   

-   The new representation (e.g. trained neural networks and embeddings) has some features that make it easy to feed in to ML pipelines. In particular,  labelled training data is not required.  
    
-   A language model predicts what is the next word given a sequence of words.  
    
    -   Input: A sequence of words $x_1$, …, $x_n-1$
        
    -   Output: Predict P($x_n$|$x_1$,…,$x_{n-1}$)

#### The applications of language models are

-   Completing search queries 
    
-   Suggesting the next word on keyboards
    
-   Chatbots     

#### How the language models are built

-   Deep learning algorithms, especially Recurrent Neural Networks and Transformers are common methods for building language models. 
    
-   Pre-trained language models are available which can then be tuned to be used in different downstream tasks. 
    
-   The off-the-shelf pretrained models vary with respect to: 
    
    -   Data that was used for training them 
        
    -   Learning network architectures  

The language models are domain-independent which means they are not trained for any specific task. The learned (pre-trained) language model could be the input to a variety of specific problems like sentiment analysis and language translation. For a specific problem, we might have some labelled data that we use to learn a domain-dependent model.  This data can be much smaller than that used for pre-training the language model.  This task-dependent stage is called _few-__shot learning_ to recognise the small data needs, or _tuning_ to recognise the minor adjustments to the pre-trained model for the task at hand.  

Google BERT and GPT-3 from OpenAI are two state-of-the-art language models.