
Maximum likelihood estimation (MLE) is a method of estimating the parameters of a probability distribution by maximizing a likelihood function. The likelihood function is the probability of observing the data given the model parameters.

## Steps for MLE

1. Choose a probability distribution for the data.
2. Write down the <mark style="background: #FF5582A6;">likelihood function</mark>, which is the probability of observing the data given the model parameters.
3. Take the natural logarithm of the likelihood function to simplify the calculations and maximize the log-likelihood function instead.
4. Find the derivative of the <mark style="background: #FFB8EBA6;">log-likelihood function</mark> with respect to each parameter and set them equal to zero to find the maximum likelihood estimates.
5. Check that the estimates obtained are indeed the maximum by verifying that the second derivative of the log-likelihood function at those estimates is negative.


## Advantages and Disadvantages of MLE

Advantages:
- Produces <mark style="background: #BBFABBA6;">unbiased estimates</mark> of the parameters.
- Has desirable asymptotic properties.
- Can be used to estimate parameters for a wide range of distributions.

Disadvantages:
- Assumes that the data is <mark style="background: #ADCCFFA6;">independent and identically distributed</mark>.
- Can be sensitive to outliers in the data.
- Can produce estimates that are <mark style="background: #ADCCFFA6;">biased or unstable when the sample size is small</mark>.


## Example

#### Continuous Distribution
Suppose we have a sample of n independent and identically distributed observations from a normal distribution with unknown mean $mu$ and variance $sigma^2$. The<mark style="background: #FFB8EBA6;"> normal likelihood function</mark> for this data is:

$$L(mu, sigma^2) = (1 / sqrt(2 * pi * sigma^2))^n * exp(-1 / (2 * sigma^2) * sum((xi - mu)^2))$$

Taking the natural logarithm of the likelihood function gives:

$$log L(mu, sigma^2) = -n / 2 * log(2 * pi) - n / 2 * log(sigma^2) - 1 / (2 * sigma^2) * sum((xi - mu)^2)$$

To find the maximum likelihood estimates, we take the derivatives of the log-likelihood function with respect to mu and sigma^2:

$$d / d mu (log L(mu, sigma^2)) = 1 / (2 * sigma^2) * sum(xi - mu)
d / d (sigma^2) (log L(mu, sigma^2)) = -n / (2 * sigma^2) + 1 / (2 * sigma^4) * sum((xi - mu)^2)$$

Setting these derivatives equal to zero and solving for mu and sigma^2 gives:

$$mu = sum(xi) / n$$
$$sigma^2 = sum((xi - mu)^2) / n$$

These are the maximum likelihood estimates for the mean and variance of the normal distribution.

#### Descrete Distribution

Suppose we have a sample of n independent and identically distributed observations from a <mark style="background: #BBFABBA6;">Bernoulli distribution</mark> with unknown success probability p. The Bernoulli distribution models a binary outcome (e.g., success or failure) with a single parameter p, representing the probability of success. The likelihood function for this data is:

$$L(p|x1, x2, ..., xn) = ∏(p^xi * (1-p)^(1-xi))$$

where xi is the i-th observed binary outcome (1 for success, 0 for failure), and the product is taken over all n observations.

Taking the logarithm of the likelihood function and simplifying the resulting expression yields the log-likelihood function:

$$log L(p|x1, x2, ..., xn) = ∑(xi * log(p) + (1-xi) * log(1-p))$$

To find the maximum likelihood estimate of p, we take the derivative of the log-likelihood function with respect to p and set it equal to zero:

$$d / dp (log L(p|x1, x2, ..., xn)) = ∑(xi / p - (1-xi) / (1-p)) = 0$$

Solving for p gives the maximum likelihood estimate:

$$p = (1/n) * ∑xi$$

This is the sample proportion of successes, which is the maximum likelihood estimate of the unknown success probability p.



