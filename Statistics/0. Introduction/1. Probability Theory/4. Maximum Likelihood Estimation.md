
Maximum likelihood estimation (MLE) is a method of estimating the parameters of a probability distribution by maximizing a likelihood function. The likelihood function is the probability of observing the data given the model parameters.

## Steps for MLE

1. Choose a probability distribution for the data.
2. Write down the <mark style="background: #FF5582A6;">likelihood function</mark>, which is the probability of observing the data given the model parameters.
3. Take the natural logarithm of the likelihood function to simplify the calculations and maximize the log-likelihood function instead.
4. Find the derivative of the log-likelihood function with respect to each parameter and set them equal to zero to find the maximum likelihood estimates.
5. Check that the estimates obtained are indeed the maximum by verifying that the second derivative of the log-likelihood function at those estimates is negative.

The formula for the likelihood function depends on the specific model being used. In general, for a set of <mark style="background: #FFF3A3A6;">independent and identically distributed (i.i.d.) random variables</mark> $X1, X2, ..., Xn$ with probability density function or mass function $f(X;θ)$, where θ is a vector of unknown parameters, the likelihood function L(θ) is defined as:

$$L(θ) = f(X1;θ) × f(X2;θ) × ... × f(Xn;θ)$$

or

$$L(θ) = ∏ i=1 to n f(Xi;θ)$$

where $∏$ is the product symbol and f is the probability density function or mass function.

## Example

Suppose we have a sample of n independent and identically distributed observations from a normal distribution with unknown mean $mu$ and variance $sigma^2$. The likelihood function for this data is:

$$L(mu, sigma^2) = (1 / sqrt(2 * pi * sigma^2))^n * exp(-1 / (2 * sigma^2) * sum((xi - mu)^2))$$

Taking the natural logarithm of the likelihood function gives:

$$log L(mu, sigma^2) = -n / 2 * log(2 * pi) - n / 2 * log(sigma^2) - 1 / (2 * sigma^2) * sum((xi - mu)^2)$$

To find the maximum likelihood estimates, we take the derivatives of the log-likelihood function with respect to mu and sigma^2:

$$d / d mu (log L(mu, sigma^2)) = 1 / (2 * sigma^2) * sum(xi - mu)
d / d (sigma^2) (log L(mu, sigma^2)) = -n / (2 * sigma^2) + 1 / (2 * sigma^4) * sum((xi - mu)^2)$$

Setting these derivatives equal to zero and solving for mu and sigma^2 gives:

$$mu = sum(xi) / n$$
$$sigma^2 = sum((xi - mu)^2) / n$$

These are the maximum likelihood estimates for the mean and variance of the normal distribution.

## Advantages and Disadvantages of MLE

Advantages:
- Produces unbiased estimates of the parameters.
- Has desirable asymptotic properties.
- Can be used to estimate parameters for a wide range of distributions.

Disadvantages:
- Assumes that the data is independent and identically distributed.
- Can be sensitive to outliers in the data.
- Can produce estimates that are biased or unstable when the sample size is small.
