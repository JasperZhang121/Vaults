
In Kubernetes, some objects are designated as **owners** of other objects, which are referred to as their **dependents**. For example, a ReplicaSet is the owner of a set of Pods, making those Pods dependents of the ReplicaSet.

#### Ownership vs. Labels and Selectors

Ownership is distinct from the labels and selectors mechanism that some Kubernetes resources also use. For example:
- A **Service** uses **labels** to manage its associated EndpointSlice objects.
- Each EndpointSlice managed on behalf of the Service has an **owner reference** to indicate ownership. 

**Owner references** ensure that different parts of Kubernetes don't interfere with objects they don’t control, offering a more robust relationship mechanism beyond labels and selectors.

### Owner References in Object Specifications

- **Dependent objects** have a `metadata.ownerReferences` field that references their owner object. 
  - A valid owner reference includes the **name** and **UID** of the owner, and both must be in the same namespace as the dependent object.
  - Kubernetes automatically manages owner references for objects like ReplicaSets, DaemonSets, Deployments, Jobs, CronJobs, and ReplicationControllers.

You can also manually configure the `ownerReferences` field if needed, but it’s typically managed automatically by Kubernetes.

#### Block Owner Deletion

- Dependent objects may have an `ownerReferences.blockOwnerDeletion` field. This boolean value controls whether dependents can block their owner's deletion. 
  - If set to `true`, the dependent resource will prevent the owner from being deleted until certain conditions are met.
  - Kubernetes sets `blockOwnerDeletion` to `true` automatically for controllers like Deployments.

An **admission controller** ensures that only users with appropriate delete permissions for the owner can modify the `blockOwnerDeletion` field for dependents. This helps prevent unauthorized users from blocking the deletion of resources they do not control.

### Cross-Namespace Owner References

- **Cross-namespace owner references are not allowed** by design:
  - **Namespaced dependents** can specify cluster-scoped or namespaced owners, but the owner must reside in the same namespace as the dependent.
  - **Cluster-scoped dependents** can only specify cluster-scoped owners.

If a cross-namespace owner reference is invalid (e.g., a namespaced owner is specified for a cluster-scoped dependent), Kubernetes treats it as if the owner reference is absent. This can lead to the dependent being deleted if the owner is considered unresolvable.

Starting in **v1.20**, Kubernetes will report a warning event (`OwnerRefInvalidNamespace`) if it detects an invalid cross-namespace owner reference. You can check for such events by running:

```bash
kubectl get events -A --field-selector=reason=OwnerRefInvalidNamespace
```

### Ownership and Finalizers

When deleting a resource, Kubernetes allows the managing controller to process any finalizer rules associated with the resource. **Finalizers** are used to prevent accidental deletion of resources that may still be required.

For example:
- If you try to delete a **PersistentVolume** that is still in use by a Pod, the deletion does not happen immediately because the PV has the `kubernetes.io/pv-protection` finalizer. The volume remains in a **Terminating** status until the finalizer is cleared.

#### Cascading Deletion and Finalizers

Kubernetes uses finalizers to manage cascading deletion in two modes:

1. **Foreground Deletion:**
   - Kubernetes adds a `foreground` finalizer to the owner.
   - The controller must delete all dependent resources with `ownerReferences.blockOwnerDeletion=true` before deleting the owner.

2. **Orphan Deletion:**
   - Kubernetes adds an `orphan` finalizer.
   - The controller deletes the owner resource without deleting its dependents, effectively orphaning them.

---
### Example

#### Scenario

Imagine you are managing a web application using a **Deployment** in Kubernetes. This Deployment controls a set of **Pods**, and you want to make sure that the Pods are properly cleaned up before the Deployment is deleted. Additionally, you want to prevent accidental deletion of resources while they are still in use, using finalizers.

#### 1. **Creating the Deployment**
First, you define a Deployment for your web application. This Deployment manages 3 Pods and ensures that a specific finalizer is applied, preventing accidental deletion before cleanup.

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app
  finalizers:
    - example.com/deployment-cleanup
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web-app
  template:
    metadata:
      labels:
        app: web-app
    spec:
      containers:
      - name: nginx
        image: nginx
```

Here’s what happens:
- The Deployment `web-app` is created, and it has a **custom finalizer** `example.com/deployment-cleanup` in its `metadata.finalizers` field.
- The Deployment manages 3 Pods with the label `app=web-app`.

#### 2. **Understanding Owner References in Pods**

Each Pod that the Deployment manages has an **owner reference** that links it to the Deployment. This relationship is set automatically by Kubernetes. The `metadata.ownerReferences` field in each Pod specifies that the Pods are dependents of the `web-app` Deployment.

For example, the Pods created by the Deployment will have this `ownerReferences` field:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: web-app-pod-1
  ownerReferences:
    - apiVersion: apps/v1
      kind: Deployment
      name: web-app
      uid: <deployment-uid>
      blockOwnerDeletion: true
```

Explanation:
- The **ownerReferences** field indicates that the Pod is owned by the Deployment named `web-app`.
- The `blockOwnerDeletion: true` ensures that the Deployment cannot be deleted until all its Pods are properly cleaned up.

#### 3. **Attempting to Delete the Deployment**

Now, let’s say you try to delete the Deployment:

```bash
kubectl delete deployment web-app
```

Here’s what happens:
- Kubernetes marks the Deployment for deletion by setting the `deletionTimestamp` field, but it **does not immediately delete the Deployment** because the `example.com/deployment-cleanup` finalizer is still present.
- The Deployment enters a **"Terminating"** state, but the actual deletion is blocked by the finalizer.

You can check the Deployment status:

```bash
kubectl get deployment web-app -o yaml
```

The output will show the `deletionTimestamp` and that the object is still in the **Terminating** state:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app
  deletionTimestamp: 2024-10-23T10:00:00Z
  finalizers:
    - example.com/deployment-cleanup
spec:
  ...
status:
  replicas: 3
  phase: Terminating
```

#### 4. **Custom Cleanup Process (Handled by the Controller)**

Since the Deployment has the custom finalizer `example.com/deployment-cleanup`, it prevents the Deployment from being deleted until the cleanup process is complete. Your custom controller (or another external process) must now detect this and perform any required cleanup actions.

For example, you might have a custom process that:
- Archives logs or metrics from the Pods.
- Updates a database with the state of the Deployment.
- Sends an alert to administrators that the Deployment is being cleaned up.

Once the cleanup process is complete, the finalizer is removed. Your controller can remove the finalizer by issuing the following command:

```bash
kubectl patch deployment web-app --type='json' -p='[{"op": "remove", "path": "/metadata/finalizers"}]'
```

#### 5. **Deleting the Deployment and its Pods**

After the finalizer is removed, Kubernetes proceeds to delete the Deployment. The **Pods** associated with the Deployment will also be deleted because of the **owner references**.

- Kubernetes follows **foreground deletion** in this case, ensuring that all dependent Pods are deleted before the Deployment itself is removed.
- Once the Pods are deleted, the Deployment is also deleted from the cluster.

You can confirm the deletion by running:

```bash
kubectl get deployment web-app
```

If the finalizer has been removed and the cleanup process is complete, the Deployment will no longer appear, confirming its deletion.

#### 6. **Handling Cascading Deletion (Foreground and Orphan Deletion)**

Kubernetes supports two types of cascading deletion when it comes to owners and dependents: **foreground deletion** and **orphan deletion**.

- **Foreground Deletion**:
  - In foreground deletion, the owner resource (Deployment) is deleted **only after** all its dependent resources (Pods) are deleted.
  - This is useful when you want to ensure that dependents are cleaned up first before deleting the owner.

- **Orphan Deletion**:
  - In orphan deletion, the dependent resources are not deleted when the owner is deleted. The dependent resources (Pods) are orphaned and remain in the cluster.
  - This is useful when you want to keep dependents for further use even after deleting the owner.

#### 7. **Example of Orphan Deletion**

You can set the orphan deletion policy by specifying `--cascade=orphan` when deleting a Deployment:

```bash
kubectl delete deployment web-app --cascade=orphan
```

- This command will delete the Deployment `web-app`, but its Pods will not be deleted. Instead, they will become **orphaned** and remain in the cluster without an owner.
- You can later manually manage or delete the orphaned Pods.

#### 8. **Cross-Namespace Owner References**

In Kubernetes, **cross-namespace owner references** are not allowed. A dependent resource must exist in the same namespace as its owner. If you attempt to create a cross-namespace owner reference, Kubernetes will treat the owner as absent, and the dependent resource will be subject to deletion.

Example of a cross-namespace issue:
- You try to create a Pod in namespace `ns-1` with an owner reference to a Deployment in namespace `ns-2`. This is invalid because the owner and dependent are in different namespaces.

Kubernetes will report a warning event:

```bash
kubectl get events -A --field-selector=reason=OwnerRefInvalidNamespace
```

This command will show any invalid owner reference issues in the cluster.
