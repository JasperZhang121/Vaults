
Nodes in Kubernetes are the machines (virtual or physical) where Kubernetes runs workloads by placing containers into Pods. Each node is managed by the control plane and contains the necessary services to run the Pods, such as the kubelet, a container runtime, and the kube-proxy.

### Node Components

- **Kubelet**: The agent that ensures containers are running in a Pod on the node.
- **Container Runtime**: The software that runs containers (e.g., Docker, containerd).
- **Kube-proxy**: Handles network routing for services and maintains network rules.

### Node Management

Nodes can be added to the Kubernetes API server in two ways:
1. **Self-registration**: The kubelet on a node self-registers with the control plane.
2. **Manual registration**: A human user or controller manually adds a Node object to the API.

#### Self-registration of Nodes

When the kubelet starts with the `--register-node=true` flag (default), it automatically registers the node with the API server. It uses options like:

- **--kubeconfig**: Path to the credentials for authenticating with the API server.
- **--cloud-provider**: For retrieving metadata from a cloud provider (if applicable).
- **--register-with-taints**: To register the node with specific taints (constraints).
- **--node-ip**: To set the node's IP address (useful for multi-stack configurations).
  
If the `--register-node=false` flag is used, manual node registration is required.

#### Manual Node Registration

You can create and modify Node objects manually using `kubectl`. When manually adding a node, ensure the kubelet is started with the `--register-node=false` flag. For example:

```yaml
{
  "kind": "Node",
  "apiVersion": "v1",
  "metadata": {
    "name": "10.240.79.157",
    "labels": {
      "name": "my-first-k8s-node"
    }
  }
}
```

After the Node object is created, the control plane verifies its validity by checking if the kubelet has registered. If the node is healthy, it is eligible to run Pods.

**Note**: Kubernetes keeps the Node object even if the node is invalid and checks periodically to see if the node becomes healthy.

### Node Name Uniqueness

Each node in a Kubernetes cluster must have a unique name (DNS subdomain format). If a node is updated or replaced, the existing Node object should be deleted and re-added to prevent inconsistencies.

### Node Configuration Updates

If the node’s configuration needs to be updated (e.g., changing labels), re-registration is necessary. Pods already scheduled on the node may encounter issues if the configuration is changed without re-registering.

### Node Status

A Node's status includes information such as:
- **Addresses**: The IP address or hostname of the node.
- **Conditions**: The health status of the node (e.g., Ready, DiskPressure).
- **Capacity**: The total resources available on the node (CPU, memory).
- **Allocatable**: The resources available for Pods after system overhead.

You can view the status of a node using `kubectl describe node <node-name>`.

### Node Heartbeats

Nodes periodically send heartbeats to the control plane to signal their availability. Kubernetes uses two forms of heartbeats:
1. **Node status updates**: Updates the `.status` field of the Node object.
2. **Lease objects**: Each node has a corresponding Lease object in the `kube-node-lease` namespace, which helps track node liveness more efficiently.

### Node Controller

The **Node Controller** is a component of the Kubernetes control plane responsible for managing nodes, including:
- **CIDR assignment**: Assigning IP ranges to nodes when they register.
- **Cloud provider monitoring**: Ensuring nodes listed by the cloud provider match the cluster’s nodes.
- **Node health monitoring**: If a node becomes unreachable, the Node Controller updates the node’s status to `Unknown` and eventually triggers eviction of its Pods.

By default, the Node Controller checks the health of each node every 5 seconds, and it evicts Pods from nodes that remain unreachable for 5 minutes.

### Node Eviction Policies

- **Eviction Rate**: The Node Controller limits evictions to 0.1 nodes per second by default, meaning no more than 1 node’s Pods are evicted every 10 seconds.
- **Zone-specific Eviction**: If a high percentage of nodes in a specific availability zone are unhealthy, the eviction rate is reduced to avoid excessive disruption.

### Node Resource Management

Nodes track their resource capacity (e.g., CPU, memory), and the Kubernetes scheduler ensures that Pods only run on nodes with sufficient resources. The scheduler checks that the sum of resource requests for all containers on a node does not exceed the node’s capacity.

- If necessary, resources can be reserved for non-Pod processes using configurations such as **system reserved resources**.

### Node Topology

Starting from Kubernetes v1.27, the **Topology Manager** can use hints when making resource assignment decisions. This is useful for ensuring optimal resource allocation on nodes with NUMA architectures (non-uniform memory access).

### Swap Memory Management

From Kubernetes v1.30, Kubernetes supports **swap memory** (beta). You can enable swap by configuring the **NodeSwap** feature and setting appropriate flags. Swap behavior can be configured as follows:
- **NoSwap (default)**: Pods cannot use swap memory.
- **LimitedSwap**: Only Pods with Burstable QoS can use swap memory.

### Node Maintenance

To safely perform maintenance on a node, you can mark it as **unschedulable** to prevent new Pods from being scheduled on it:

```bash
kubectl cordon <node-name>
```

To drain all Pods from a node, use:

```bash
kubectl drain <node-name>
```

This ensures Pods are rescheduled on other nodes, except for DaemonSets, which are allowed to run on unschedulable nodes.
