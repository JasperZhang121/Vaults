
In Kubernetes, **leases** provide a <mark style="background: #FFB8EBA6;">mechanism to lock shared resources, enabling distributed coordination between system components</mark>. Leases are represented by **Lease objects** in the `coordination.k8s.io` API group and are used for tasks like node heartbeats, leader election, and API server identity tracking.

### Key Uses of Leases

1. **Node Heartbeats**
   - Kubernetes uses the Lease API to monitor the availability of each node through **kubelet heartbeats**. Each node has a corresponding Lease object in the `kube-node-lease` namespace, with a matching name. 
   - Each heartbeat updates the `spec.renewTime` field in the Lease, which the control plane uses to check if a node is healthy and available.

   For more information, refer to the Node Lease objects in Kubernetes documentation.

2. **Leader Election**
   - Leases support **leader election** among Kubernetes components in high availability (HA) configurations. For example, the `kube-controller-manager` and `kube-scheduler` run in active-passive modes, where only one instance actively operates while the others remain on standby.
   - Leases allow components to communicate which instance is the leader, ensuring only one instance runs a specific task at any given time.

   Learn more about coordinated leader election in the Kubernetes documentation.

3. **API Server Identity**
   - **Feature State**: Kubernetes v1.26 (beta, enabled by default)
   - Starting in v1.26, each `kube-apiserver` instance uses the Lease API to publish its identity, providing a way to track the number of active API server instances. While not directly actionable, these leases support future functionalities that may require API server coordination.
   - Each `kube-apiserver` instance creates a Lease with a name based on the SHA256 hash of its hostname and stores it in the `kube-system` namespace. These leases are garbage-collected after one hour if the `kube-apiserver` instance is no longer present.

   To check API server leases:
   ```bash
   kubectl -n kube-system get lease -l apiserver.kubernetes.io/identity=kube-apiserver
   ```

   Example output:
   ```
   NAME                                        HOLDER                                             AGE
   apiserver-07a5ea9b9b072c4a5f3d1c3702        apiserver-07a5ea9b9b072c4a5f3d1c3702_instance1     5m33s
   ```

   You can view details of a specific Lease:
   ```bash
   kubectl -n kube-system get lease apiserver-07a5ea9b9b072c4a5f3d1c3702 -o yaml
   ```

### Using Leases for Workloads

Custom workloads can define and use Lease objects for coordination purposes. For example:
- In a **custom controller**, one replica can perform operations as a primary (leader), while others remain passive. By defining a Lease, controllers can use the Kubernetes API to perform leader election and ensure only one active instance.
- It’s recommended to name Lease objects clearly, linking them to the specific component (e.g., `example-foo` for a component named "Example Foo").

To prevent name conflicts:
- Choose unique Lease names, or use a consistent naming mechanism, like a hash based on the component name or deployment.

---

### Example 1: Node Heartbeats Using Leases

In Kubernetes, **node heartbeats** rely on Lease objects to signal the availability of nodes to the API server. Each node has a Lease object in the `kube-node-lease` namespace, and the kubelet on each node updates this Lease object’s `renewTime` field at regular intervals.

#### Step-by-Step Walkthrough

1. **Viewing Node Lease Objects**:
   To inspect the Lease objects for node heartbeats, run:
   ```bash
   kubectl get leases -n kube-node-lease
   ```
   Output:
   ```
   NAME           HOLDER                             AGE
   node1          node1                              3m
   node2          node2                              3m
   ```

2. **Viewing Details of a Node’s Lease Object**:
   You can inspect a specific Lease object for a node, such as `node1`, with:
   ```bash
   kubectl get lease node1 -n kube-node-lease -o yaml
   ```
   Example output:
   ```yaml
   apiVersion: coordination.k8s.io/v1
   kind: Lease
   metadata:
     name: node1
     namespace: kube-node-lease
   spec:
     holderIdentity: node1
     leaseDurationSeconds: 40
     renewTime: "2024-10-24T15:23:45Z"
   ```
   - `holderIdentity`: Identifies the node that holds the Lease.
   - `renewTime`: Indicates the last time the Lease was renewed by the kubelet on this node.
   - `leaseDurationSeconds`: Specifies how long the Lease is considered valid from the `renewTime`. If this time expires without a new `renewTime`, the node is considered unreachable.

This Lease mechanism allows the Kubernetes control plane to monitor node health efficiently.

---

### Example 2: Leader Election with Leases

Kubernetes uses Leases to ensure that only one instance of a component (e.g., `kube-controller-manager` or `kube-scheduler`) is active at a time in high availability (HA) environments. This is called **leader election**.

#### Step-by-Step Walkthrough

1. **Finding Leader Election Leases**:
   Run the following command to list the Lease objects used for leader election in the `kube-system` namespace:
   ```bash
   kubectl get leases -n kube-system
   ```
   Example output:
   ```
   NAME                              HOLDER                                         AGE
   kube-controller-manager           kube-controller-manager-abc123                 15m
   kube-scheduler                    kube-scheduler-def456                          15m
   ```

2. **Inspecting a Leader Election Lease**:
   You can inspect the `kube-controller-manager` Lease to see details of the leader:
   ```bash
   kubectl get lease kube-controller-manager -n kube-system -o yaml
   ```
   Example output:
   ```yaml
   apiVersion: coordination.k8s.io/v1
   kind: Lease
   metadata:
     name: kube-controller-manager
     namespace: kube-system
   spec:
     holderIdentity: kube-controller-manager-abc123
     leaseDurationSeconds: 15
     acquireTime: "2024-10-24T15:20:10Z"
     renewTime: "2024-10-24T15:20:25Z"
   ```
   - `holderIdentity`: Identifies the instance holding the Lease (the active leader).
   - `acquireTime`: The time when this instance became the leader.
   - `renewTime`: The last time the Lease was renewed by the leader.
   - `leaseDurationSeconds`: How long the Lease is valid without renewal. If this time passes without renewal, another instance can assume leadership.

In this configuration, if the current leader (e.g., `kube-controller-manager-abc123`) fails to renew the Lease in time, another standby instance will assume leadership, ensuring the component remains available.

---

### Example 3: Using Leases for Custom Workload Coordination

Let’s say you have a custom workload where multiple instances of a component run in the cluster, but only one instance should act as the primary (leader) to perform specific tasks.

#### Step-by-Step Walkthrough

1. **Defining a Lease Object for Your Custom Controller**

   First, define a Lease object in YAML. Name it `custom-controller-lease` and specify a Lease duration.

   ```yaml
   apiVersion: coordination.k8s.io/v1
   kind: Lease
   metadata:
     name: custom-controller-lease
     namespace: custom-namespace
   spec:
     holderIdentity: ""
     leaseDurationSeconds: 30
   ```

   Apply the Lease:
   ```bash
   kubectl apply -f custom-controller-lease.yaml
   ```

2. **Leader Election Logic in Your Custom Controller**

   In the custom controller application, add code that checks and acquires the Lease. This code periodically checks the Lease and attempts to become the leader by setting itself as the `holderIdentity`.

   ```python
   import time
   from kubernetes import client, config

   config.load_kube_config()
   v1 = client.CoordinationV1Api()

   lease_name = "custom-controller-lease"
   namespace = "custom-namespace"
   instance_id = "custom-controller-instance-1"

   def try_acquire_lease():
       lease = v1.read_namespaced_lease(name=lease_name, namespace=namespace)
       renew_time = time.time()

       # Attempt to become the leader if Lease is available or expired
       if lease.spec.holderIdentity == "" or renew_time > lease.spec.renewTime:
           lease.spec.holderIdentity = instance_id
           lease.spec.renewTime = renew_time + lease.spec.leaseDurationSeconds
           v1.replace_namespaced_lease(name=lease_name, namespace=namespace, body=lease)
           print(f"{instance_id} is now the leader")
       else:
           print(f"{instance_id} is not the leader")

   while True:
       try_acquire_lease()
       time.sleep(10)
   ```

3. **Updating and Renewing the Lease**

   The controller instance that holds the Lease continues to renew it by updating the `renewTime` and `holderIdentity`. If the current leader stops updating the Lease (e.g., due to failure), other instances can take over by acquiring the Lease and setting themselves as the new leader.

4. **Inspecting the Lease Status**

   You can check the current leader by inspecting the Lease:
   ```bash
   kubectl get lease custom-controller-lease -n custom-namespace -o yaml
   ```

   Output:
   ```yaml
   apiVersion: coordination.k8s.io/v1
   kind: Lease
   metadata:
     name: custom-controller-lease
     namespace: custom-namespace
   spec:
     holderIdentity: custom-controller-instance-1
     leaseDurationSeconds: 30
     renewTime: "2024-10-24T15:30:00Z"
   ```
   - The `holderIdentity` field identifies the active leader.

