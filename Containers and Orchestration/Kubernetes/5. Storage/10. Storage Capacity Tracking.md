
Kubernetes tracks **storage capacity** to ensure that the scheduler places Pods on nodes with sufficient available storage, minimizing scheduling retries and improving reliability. This feature is particularly useful for clusters using **network-attached** or **node-local storage** where storage resources are not uniformly accessible by all nodes.

### Overview

In Kubernetes, **storage capacity tracking** is achieved through API support, specifically tailored for use with **CSI drivers** that support this feature. Capacity tracking helps the scheduler make informed decisions about which nodes can meet the storage requirements for newly created volumes, thus enhancing scheduling efficiency.

### Key API Components

1. **CSIStorageCapacity Objects**:
   - Created by a CSI driver in its installation namespace.
   - Contains capacity information for each storage class and defines which nodes have access to this storage.

2. **CSIDriverSpec.StorageCapacity Field**:
   - When set to `true`, the Kubernetes scheduler takes storage capacity into account for volumes using this CSI driver.

### Scheduling with Storage Capacity Tracking

Storage capacity tracking affects scheduling in the following conditions:

1. **Pod Requirements**:
   - The Pod requires a volume that has not been created yet.
   - The volume is associated with a **StorageClass** that uses a CSI driver supporting storage capacity tracking.
   - **VolumeBindingMode** is set to **WaitForFirstConsumer**.

2. **Scheduler Behavior**:
   - The scheduler selects nodes that have sufficient storage, verified against `CSIStorageCapacity` objects relevant to the Pod's topology.
   - For **Immediate volume binding mode**, the storage driver creates the volume independently of Pod scheduling, allowing the scheduler to assign Pods to nodes where the volume is available.

3. **CSI Ephemeral Volumes**:
   - **CSI ephemeral volumes** skip storage capacity tracking during scheduling, as they are often node-local with minimal resource demands.

### Rescheduling

If a node selection fails due to **outdated capacity information**, Kubernetes resets the node assignment and attempts to reschedule the Pod. This allows Kubernetes to dynamically handle changes in storage availability and retry scheduling as needed.

### Limitations

1. **Data Consistency**:
   - Capacity information may become outdated, leading to retries or failures.

2. **Multiple Volumes**:
   - Scheduling can fail if a Pod uses multiple volumes and capacity runs out after the initial volume creation. Manual intervention, like increasing capacity or deleting volumes, may be needed in such cases.
