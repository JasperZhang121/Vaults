### Event-Driven
Event-driven applications are stateful applications that extract data from one or more event streams and trigger computations, state updates, or other external actions based on incoming events. A typical example is message queues, such as Kafka, which are almost all event-driven applications.

**Event-Driven:**
![EventDrivenFlink](EventDrivenFlink.png)

### Worldview of Stream and Batch
Batch processing is characterized by being bounded, persistent, and large-scale, making it suitable for computations that require accessing the complete set of records, typically used for offline analytics. Stream processing, on the other hand, is unbounded and real-time, operating on each data item as it flows through the system, typically used for real-time analytics.

In Spark's worldview, everything consists of batches: offline data is a large batch, while real-time data is composed of an infinite series of small batches.

In Flink's worldview, everything consists of streams: offline data is a bounded stream, and real-time data is an unbounded stream. This is known as bounded and unbounded streams.
- **Unbounded Streams**: Unbounded streams have a start but no end. They do not terminate and provide data continuously, requiring immediate processing of each event upon arrival. We cannot wait for all data to arrive because the input is unbounded and will never complete. Processing unbounded streams often requires processing events in a specific order (e.g., in the order they occurred) to ensure result completeness.
- **Bounded Streams**: Bounded streams have a clearly defined start and end. They can be processed by acquiring all data before performing any computations, and they do not need to be processed in order. Bounded stream processing is also known as batch processing.

This stream-oriented architecture offers the significant benefit of extremely low latency.

### Layered API

Flink provides a layered API to cater to different levels of abstraction and use cases:

1. **Low-Level API**:
    - The lowest level of abstraction provides stateful streams embedded in the DataStream API via Process Functions. Process Functions offer fine-grained control over event handling from one or more streams with consistent fault tolerance. Users can register event time and process time callbacks for complex computations.

2. **Core API**:
    - Most applications use the Core APIs for programming, such as the DataStream API (for bounded or unbounded streams) and the DataSet API (for bounded datasets). These APIs provide general building blocks for data processing, such as user-defined transformations, joins, aggregations, and window operations. The DataSet API offers additional support for bounded datasets, like loops and iterations. The data types handled by these APIs are represented as classes in their respective programming languages.

3. **Table API**:
    - The Table API offers table-centric declarative programming where tables can dynamically change (when expressing stream data). It follows an extended relational model: tables have a two-dimensional structure (schema) similar to relational databases, and the API offers comparable operations such as select, project, join, group-by, and aggregate. The Table API allows users to define the logical operations declaratively without specifying how these operations are executed in code. While it can be extended with various user-defined functions (UDFs), it is less expressive than the Core APIs but more concise (less code). Additionally, Table API programs are optimized by a built-in optimizer before execution.
    - You can seamlessly switch between tables and DataStream/DataSet, allowing programs to mix Table API with DataStream and DataSet.

4. **SQL**:
    - The highest level of abstraction in Flink is SQL. This layer shares syntax and expressive power with the Table API but represents programs as SQL query expressions. SQL interacts closely with the Table API, and SQL queries can be executed directly on tables defined in the Table API.

Currently, Flink is not as mature in batch processing as Spark, so DataSet is not widely used. Flink's Table API and Flink SQL are also not fully developed, with many custom implementations by various companies. Therefore, we mainly focus on using the DataStream API. Flink, as the closest implementation to the Google DataFlow model, embodies the unified view of stream and batch processing, so using DataStream is generally sufficient.

### Major Modules in Flink

- **Flink Table & SQL** (still under development)
- **Flink Gelly** (graph processing)
- **Flink CEP** (Complex Event Processing)