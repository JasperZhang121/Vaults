### Overview

Flink is a stream-oriented and real-time computation engine. Two key concepts in Flink are **streaming** and **real-time**:

1. **Streaming**: Data continuously flows in without boundaries. However, computations need to operate on bounded data, which requires defining boundaries. These boundaries can be set based on time intervals or data volume.

2. **Real-time**: Data is processed as soon as it arrives, producing results immediately. There are two types of computations:
   - **Within boundaries**: E.g., counting the number of news articles read by users in the last five minutes.
   - **Combining with external data**: E.g., identifying the regions where users reading news in the last five minutes come from by joining with an external table.

### Time

Time plays a crucial role when defining boundaries in Flink. There are three types of time in Flink:

1. **Event Time**: The time when an event is created. For example, logs often include timestamps representing their creation time. Event time is used for accurate event processing.
2. **Ingestion Time**: The time when data enters Flink.
3. **Processing Time**: The local system time of the operator performing the computation. This is the default time attribute in Flink.

For example, if a log enters Flink at `2021-01-22 10:00:00.123` and reaches a window operator at `2021-01-22 10:00:01.234`, the event time might be `2021-01-06 18:37:15.624`. To analyze logs within a specific time range, **event time** is the most meaningful.

### Window

A **window** defines boundaries for computations over unbounded data streams by <mark style="background: #FFB8EBA6;">splitting the stream into finite "buckets"</mark>. This is crucial for processing infinite data.

#### Window Types

##### 1. **Based on Time (Time-driven Window)**
- Defines boundaries using time intervals, such as every minute or every 10 minutes.

##### 2. **Based on Data (Data-driven Window)**
- Defines boundaries based on the number of records, such as every 5 or 50 records.

#### Common Window Types

1. **Tumbling Window**: Non-overlapping windows of fixed size.
   - Example: A 5-minute tumbling window creates distinct, non-overlapping windows.
   - Use case: BI statistics or periodic aggregation.

2. **Sliding Window**: Overlapping windows of fixed size and a slide interval.
   - Example: A 10-minute window sliding every 5 minutes creates overlapping windows.
   - Use case: Analyzing recent activity, e.g., failure rates in the last 5 minutes.

3. **Session Window**: Windows created based on gaps between events, typically used to group related events.
   - Example: A new session starts when no data is received for a specified timeout period.


### Window API

#### 1. **TimeWindow**
Groups data into a window based on time and processes all data within the window.

##### Tumbling Time Window Example
```scala
val env = StreamExecutionEnvironment.getExecutionEnvironment
val text = env.socketTextStream("localhost", 9999)

case class CarWc(sensorId: Int, carCnt: Int)

val ds1 = text.map { line =>
  val tokens = line.split(",")
  CarWc(tokens(0).toInt, tokens(1).toInt)
}

val ds2 = ds1
  .keyBy("sensorId")
  .timeWindow(Time.seconds(5))
  .sum("carCnt")

ds2.print()
env.execute("TumblingWindowExample")
```

##### Sliding Time Window Example
```scala
val ds2 = ds1
  .keyBy("sensorId")
  .timeWindow(Time.seconds(10), Time.seconds(5))
  .sum("carCnt")
```

#### 2. **CountWindow**
Defines windows based on the number of records instead of time.

##### Tumbling Count Window Example
```scala
val ds2 = ds1
  .keyBy("sensorId")
  .countWindow(5)
  .sum("carCnt")
```

##### Sliding Count Window Example
```scala
val ds2 = ds1
  .keyBy("sensorId")
  .countWindow(5, 3)
  .sum("carCnt")
```


### Event Time and Watermarks

#### Watermarks
Watermarks handle out-of-order events by introducing a delay mechanism. A watermark ensures that a window is triggered after a defined delay to allow late events to arrive.

- For ordered streams: Watermark is generated with no delay.
- For unordered streams: Watermark is generated with a delay (e.g., `maxEventTime - 2` seconds).

#### Late Data Handling
Flink provides mechanisms to handle late data:
1. **Allowed Lateness**: Defines how long to wait for late events.
   ```scala
   allowedLateness(Time.seconds(2))
   ```
2. **Side Outputs**: Stores late data for separate processing.


### Flink Integration with Hive Partitioned Tables

Flink 1.12 introduces support for Hive partitioned tables as temporal tables. Flink can automatically monitor and join with the latest partitions in Hive.

#### Example
```sql
CREATE VIEW hive_catalog.flink_db.view_fact_bill_master AS
SELECT * FROM (
  SELECT t1.*, t2.group_id, t2.shop_id, t2.group_name
  FROM hive_catalog.flink_db.kfk_fact_bill_master_12 t1
  JOIN hive_catalog.flink_db.dim_extend_shop_info /*+ OPTIONS('streaming-source.enable'='true',
    'streaming-source.partition.include'='latest',
    'streaming-source.monitor-interval'='1 h') */
  FOR SYSTEM_TIME AS OF t1.proctime AS t2
  ON t1.groupID = t2.group_id AND t1.shopID = t2.shop_id
) t;
```

#### Parameters
- `streaming-source.enable`: Enables streaming source for Hive tables.
- `streaming-source.partition.include`: Options are `latest` (latest partition) or `all` (all partitions).
- `streaming-source.monitor-interval`: Interval for monitoring new partitions.
