### Usage

#### Creating Users
1. **Create a test user**
    ```bash
    root@doris1:~# mysql -hdoris1 -P 9030 -uroot
    mysql> create user 'test' identified by 'test';
    ```

2. **Log in using the test user**
    ```bash
    mysql> exit;
    root@doris1:~# mysql -hdoris1 -P 9030 -utest -ptest
    ```

#### Creating Tables

##### Creating a Database
```sql
mysql> create database test_db;
```

##### Granting Permissions
Grant the `test_db` database to the `test` user to give them read and write permissions.
```sql
mysql> grant all on test_db to test;
```

##### Creating a Table
Switch to the database:
```sql
mysql> use test_db;
```

###### Field Types

- **TINYINT**: 1 byte, Range: `-2^7 + 1 ~ 2^7 - 1`
- **SMALLINT**: 2 bytes, Range: `-2^15 + 1 ~ 2^15 - 1`
- **BIGINT**: 8 bytes, Range: `-2^63 + 1 ~ 2^63 - 1`
- **LARGEINT**: 16 bytes, Range: `-2^127 + 1 ~ 2^127 - 1`
- **FLOAT**: 4 bytes, supports scientific notation
- **DOUBLE**: 12 bytes, supports scientific notation
- **DECIMAL[(precision, scale)]**: 16 bytes, exact decimal type. Default is `DECIMAL(10, 0)`
  - `precision`: 1 ~ 27
  - `scale`: 0 ~ 9
  - Integer part: 1 ~ 18
  - Does not support scientific notation
- **DATE**: 3 bytes, Range: `0000-01-01 ~ 9999-12-31`
- **DATETIME**: 8 bytes, Range: `0000-01-01 00:00:00 ~ 9999-12-31 23:59:59`
- **CHAR[(length)]**: Fixed-length string. Length range: 1 ~ 255. Default is 1
- **VARCHAR[(length)]**: Variable-length string. Length range: 1 ~ 65533
- **HLL**: 1~16385 bytes, HLL column type, does not require length or default value specification. HLL columns can only be queried or used with functions like `hll_union_agg`, `Hll_cardinality`, `hll_hash`.
- **BITMAP**: Bitmap column type, does not require length or default value specification. Represents a set of integers, with a maximum element of `2^64 - 1`.
- **agg_type**: Aggregation type, if not specified, the column is a key column. Otherwise, it is a value column. Options: `SUM`, `MAX`, `MIN`, `REPLACE`.

##### Partitioning
Doris supports single partition and composite partition table creation.

In composite partitioning:
- The first level is **Partition**, where the user specifies a column (currently only integer and time columns are supported) as the partition column and defines the value range for each partition.
- The second level is **Distribution**, where the user can specify one or more columns and the number of buckets for hash distribution.

**Recommended scenarios for using composite partitions:**
1. If you have a time dimension or a similar ordered value dimension, you can use this dimension column as the partition column. The partition granularity can be evaluated based on the import frequency and partition data volume.
2. Historical data deletion: If there is a need to delete historical data (e.g., keeping only the last N days of data), composite partitioning can achieve this by deleting historical partitions. Alternatively, DELETE statements can be sent within a specified partition.
3. Solving data skew: Each partition can specify the number of buckets individually. For example, in daily partitioning, if there is a significant difference in daily data volume, you can plan the data of different partitions reasonably by specifying the number of buckets for each partition. It's recommended to select a column with high distinction for bucketing.

Users can also use only a single partition if composite partitioning is not required. In this case, data will be hashed without further partitioning.

##### Creating a Single Partition Table

```sql
CREATE TABLE student (
    id INT,
    name VARCHAR(50),
    age INT,
    count BIGINT SUM DEFAULT '0'
) 
AGGREGATE KEY (id, name, age) 
DISTRIBUTED BY HASH(id) BUCKETS 10 
PROPERTIES("replication_num" = "1");
```

This creates a `student` table with `id` as the bucketing column, 10 buckets, and 1 replica.

##### Creating a Composite Partition Table

```sql
CREATE TABLE student2 (
    dt DATE,
    id INT,
    name VARCHAR(50),
    age INT,
    count BIGINT SUM DEFAULT '0'
)
AGGREGATE KEY (dt, id, name, age)
PARTITION BY RANGE(dt) (
    PARTITION p202007 VALUES LESS THAN ('2020-08-01'),
    PARTITION p202008 VALUES LESS THAN ('2020-09-01'),
    PARTITION p202009 VALUES LESS THAN ('2020-10-01')
)
DISTRIBUTED BY HASH(id) BUCKETS 10
PROPERTIES("replication_num" = "1");
```

This creates a `student2` table with `dt` as the partition column, and 3 partitions:
- `P202007`: Data with values less than `2020-08-01`
- `P202008`: Data from `2020-08-01` to `2020-08-31`
- `P202009`: Data from `2020-09-01` to `2020-09-30`

### Data Models

Doris supports three data models: `AGGREGATE KEY`, `UNIQUE KEY`, and `DUPLICATE KEY`. All three models are sorted by key.

#### AGGREGATE KEY

When AGGREGATE KEYs are the same, new and old records will be aggregated. Currently, `SUM`, `MIN`, `MAX`, and `REPLACE` are supported. The AGGREGATE KEY model can pre-aggregate data, making it suitable for reporting and multidimensional business.

1. **Creating a table:**
    ```sql
    CREATE TABLE site_visit (
        siteid INT,
        city SMALLINT,
        username VARCHAR(32),
        pv BIGINT SUM DEFAULT '0'
    )
    AGGREGATE KEY(siteid, city, username)
    DISTRIBUTED BY HASH(siteid) BUCKETS 10;
    ```

2. **Inserting 2 records:**
    ```sql
    mysql> insert into site_visit values(1,1,'name1',10);
    mysql> insert into site_visit values(1,1,'name1',20);
    ```

3. **Viewing the results**

#### UNIQUE KEY

When UNIQUE KEYs are the same, the new record overwrites the old record. Currently, UNIQUE KEY and AGGREGATE KEY's REPLACE aggregation method are the same. It is suitable for businesses that require updates.

1. **Creating a table:**
    ```sql
    CREATE TABLE sales_order (
        orderid BIGINT,
        status TINYINT,
        username VARCHAR(32),
        amount BIGINT DEFAULT '0'
    )
    UNIQUE KEY(orderid)
    DISTRIBUTED BY HASH(orderid) BUCKETS 10;
    ```

2. **Inserting 2 records:**
    ```sql
    mysql> insert into sales_order values(1,1,'name1',100);
    mysql> insert into sales_order values(1,1,'name1',200);
    ```

3. **Querying the table**

##### DUPLICATE KEY

This model only specifies the sorting column; rows with the same values will not be merged. It is suitable for analytical businesses that do not require pre-aggregated data.

1. **Creating a table:**
    ```sql
    CREATE TABLE session_data (
        visitorid SMALLINT,
        sessionid BIGINT,
        city CHAR(20),
        ip VARCHAR(32)
    )
    DUPLICATE KEY(visitorid, sessionid)
    DISTRIBUTED BY HASH(sessionid, visitorid) BUCKETS 10;
    ```

2. **Inserting data:**
    ```sql
    mysql> insert into session_data values(1,1,'shanghai','www.111.com');
    mysql> insert into session_data values(1,1,'shanghai','www.111.com');
    mysql> insert into session_data values(3,2,'shanghai','www.111.com');
    mysql> insert into session_data values(2,2,'shanghai','www.111.com');
    mysql> insert into session_data values(2,1,'shanghai','www.111.com');
    ```

3. **Querying the table**

#### Rollup

Rollup can be understood as a materialized index structure of a table. Rollup can adjust the order of columns to increase the hit rate of prefix indexes and can reduce key columns to increase data aggregation.

1. **Adding Rollup to `session_data`:**
    ```sql
    mysql> alter table session_data add rollup rollup_city_ip(city,ip);
    ```

2. **Creating a Rollup for frequently viewed data**
    - If you frequently view the number of IPs for a particular city, create a Rollup with only `ip` and `city`.

3. **Viewing the table structure after creation**

4. **Using `EXPLAIN` to check the execution plan and see if the Rollup is used**

### Data Import

To accommodate different data import needs, Doris provides five different import methods. Each method supports different data sources and operates asynchronously or synchronously.

1. **Broker Load**
    - Uses Broker processes to access and read external data sources (HDFS) and import data into Doris. The import task is submitted via MySQL protocol and executed asynchronously. The import result can be checked with the `show load` command.
   
2. **Stream Load**
    - Users can submit requests via HTTP protocol and include raw data to create an import. It is mainly used for quickly importing data from local files or data streams into Doris. The import command synchronously returns the import result.

3. **Insert**
    - Similar to MySQL's `insert` statement, Doris provides `insert into tbl select ...` to read data from one Doris table and import it into another. You can also insert single rows with `insert into tbl values(...)`.

4. **Multi Load**
    - Users can submit multiple import tasks via HTTP protocol. Multi Load ensures the atomicity of multiple import tasks.

5. **Routine Load**
    - Users can submit routine import tasks via MySQL protocol, generating a persistent thread that continuously reads data from a data source (e.g., Kafka) and imports it into Doris.

#### Broker Load

Broker Load is an asynchronous import method, and the data source depends on the data sources supported by the Broker process.

**Applicable scenarios:**
- 1. The source data is stored in a system accessible by the Broker, such as HDFS.
- 2. The data volume is between tens to hundreds of gigabytes.

**Basic Principle:** 
After submitting an import task, the Frontend (FE) (Doris system's metadata and scheduling node) will generate a corresponding PLAN (import execution plan). The Backend (BE) (Doris system's computation and storage node) will execute the import plan to import the data into Doris. Depending on the number of BEs and the size of the file, the PLAN is distributed to multiple BEs, with each BE importing a portion of the data. During execution, the BE pulls data from the Broker, transforms it, and imports it into the system. Once all BEs complete the import, the FE decides whether the import was successful.

1. **Start the HDFS cluster**
    ```bash
    [root@hadoop101 ~]# /opt/module/apache-zookeeper-3.5.7-bin/bin/zkServer.sh start
    [root@hadoop102 ~]# /opt/module/apache-zookeeper-3.5.7-bin/bin/zkServer.sh start
    [root@hadoop103 ~]# /opt/module/apache-zookeeper-3.5.7-bin/bin/zkServer.sh start
    [root@hadoop101 ~]# start-all.sh
    [root@hadoop102 ~]# mr-jobhistory-daemon.sh start historyserver
    [root@hadoop102 ~]# start-history-server.sh
    ```

2. **Create `student_tmp` table in Hive**
    - Although the official documentation states support for columnar storage, testing reveals that it is not supported, and an error message will be displayed. Therefore, create a row-oriented storage table in Hive.
    ```bash
    [root@hadoop101 ~]# hive
    create table student_tmp_h(
        id int,
        name string,
        age int,
        score decimal(10,4))
    partitioned by (
    `dt` string)
    row format delimited fields terminated by '\t';
    ```

3. **Insert Data**
    ```bash
    hive (default)> set hive.exec.dynamic.partition=true;
    hive (default)> set hive.exec.dynamic.partition.mode=nonstrict;
    insert into student_tmp_h values(1,'张三',11,99.8,20200908),(2,'李四',12,99.9,20200908),
    (3,'王五',13,100,20200908),(4,'赵六',14,55.5,20200908),(5,'test1',13,66.5,20200908),
    (7,'test2',14,80,20200908),(8,'test3',19,75,20200908);
    ```

4. **Modify `hosts` file on doris1, doris2, doris3 to add hadoop101, hadoop102, hadoop103**
    ```bash
    root@doris1:~# vim /etc/hosts
    172.26.16.63    doris1  doris1
    172.26.16.61    doris2  doris2
    172.26.16.62    doris3  doris3
    172.26.16.41    hadoop101 hadoop101
    172.26.16.39    hadoop102 hadoop102
    172.26.16.40    hadoop103 hadoop103
    root@doris1:~# scp /etc/hosts doris2:/etc
    root@doris1:~# scp /etc/hosts doris3:/etc
    ```

5. **Copy Hadoop cluster configuration files to Doris cluster's broker**
    ```bash
    [root@hadoop101 ~]# cd /opt/module/hadoop-3.1.3/etc/hadoop/
    [root@hadoop101 hadoop]# scp hdfs-site.xml 172.26.16.63:/opt/software/
    root@doris1:~# cd /opt/software/
    root@doris1:/opt/software# cp hdfs-site.xml /opt/module/apache_hdfs_broker/conf/
    root@doris1:/opt/software# scp hdfs-site.xml doris2:/opt/module/apache_hdfs_broker/conf/
    root@doris1:/opt/software# scp hdfs-site.xml doris3:/opt/module/apache_hdfs_broker/conf/
    ```

6. **Use MySQL client to log in to Doris and create the corresponding `student_result` table**
    ```bash
    root@doris1:~# mysql -hdoris1 -P 9030 -uroot
    mysql> use test_db;
    create table student_result (
        id int,
        name varchar(50),
        age int,
        score decimal(10,4),
        dt varchar(20)
    )
    DUPLICATE KEY(id)
    DISTRIBUTED BY HASH(id) BUCKETS 10;
    ```

7. **Write the import statement** (Note: `dt` is the partition column and is not read in the data block, so a fixed value is used.)
    ```sql
    LOAD LABEL test_db.student_result_h_2 (
        DATA INFILE("hdfs://mycluster/user/hive/warehouse/student_tmp_h/dt=20200908/*")
        INTO TABLE student_result
        COLUMNS TERMINATED BY "\t"
        (co1, co2, co3, co4)
        set(
            id = co1,
            name = co2,
            age = co3,
            score = co4,
            dt = '20200908'
        )
    )
    WITH BROKER "broker_name" (
        "dfs.nameservices"="mycluster",
        "dfs.ha.namenodes.mycluster"="nn1,nn2,nn3",
        "dfs.namenode.rpc-address.mycluster.nn1"= "hadoop101:8020",
        "dfs.namenode.rpc-address.mycluster.nn2"= "hadoop102:8020",
        "dfs.namenode.rpc-address.mycluster.nn3"="hadoop103:8020",
        "dfs.client.failover.proxy.provider.mycluster"="org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider"
    );
    ```

8. **After successful import, query Doris for data**

#### Stream Load

Stream Load is a synchronous import method. Users can use the HTTP protocol to send local files or data streams to Doris. Stream Load executes the import synchronously and returns the result. You can check the success of the import directly from the returned result.

**For detailed help, use `HELP STREAM LOAD`.**

1. **Create a CSV file**

2. **Upload the CSV file to the cluster**

3. **Import the CSV data into Doris using the command** (`-H` specifies parameters, `column_separator` specifies the delimiter, `-T` specifies the data source file).
    ```bash
    root@doris1:/opt# curl --location-trusted -u root -H "label:123" -H"column_separator:," -T 1.csv -X PUT http://doris1:8030/api/test_db/student_result/_stream_load
    ```

4. **Check the corresponding table to ensure successful import**

#### Routine Load

Routine Load provides users with the ability to automatically import data from a specified data source.

Currently, only Kafka systems are supported for routine imports.

**Usage Restrictions:**
1. Supports unauthenticated Kafka access and Kafka clusters authenticated via SSL.
2. Only supports Kafka version `0.10.0.0` and above.

First, install Zookeeper and Kafka, create a topic, and load a batch of

 data into the topic.

1. **Create a Kafka topic and load a batch of test data using Java producer code**
    ```bash
    root@doris1:~# /opt/module/kafka_2.11-2.4.0/bin/kafka-topics.sh --zookeeper doris1:2181/kafka_2.4 --create --replication-factor 2 --partitions 3 --topic test
    ```

2. **Java producer code example:**
    ```java
    import org.apache.kafka.clients.producer.KafkaProducer;
    import org.apache.kafka.clients.producer.ProducerRecord;

    import java.util.Properties;

    public class TestProducer {
        public static void main(String[] args) {
            Properties props = new Properties();
            props.put("bootstrap.servers", "doris1:9092,doris2:9092,doris3:9092");
            props.put("acks", "-1");
            props.put("batch.size", "16384");
            props.put("linger.ms", "10");
            props.put("buffer.memory", "33554432");
            props.put("key.serializer",
                    "org.apache.kafka.common.serialization.StringSerializer");
            props.put("value.serializer",
                    "org.apache.kafka.common.serialization.StringSerializer");
            KafkaProducer<String, String> producer = new KafkaProducer<String, String>(props);
            for (int i = 0; i < 100000; i++) {
                producer.send(new ProducerRecord<String,String>("test2",i+"\tname"+i+"\t18"));
            }
            producer.flush();
            producer.close();
        }
    }
    ```

3. **Create the corresponding table in Doris:**
    ```sql
    create table student_kafka (
        id int,
        name varchar(50),
        age int
    )
    DUPLICATE KEY(id)
    DISTRIBUTED BY HASH(id) BUCKETS 10;
    ```

4. **Create an import task, specifying the `desired_concurrent_number` for concurrency:**
    ```sql
    CREATE ROUTINE LOAD test_db.kafka_test ON student_kafka
    PROPERTIES (
        "desired_concurrent_number"="3",
        "strict_mode" = "false"
    )
    FROM KAFKA (
        "kafka_broker_list"= "doris1:9092,doris2:9092,doris:9092",
        "kafka_topic" = "test2",
        "property.group.id"="test_group_2",
        "property.kafka_default_offsets" = "OFFSET_BEGINNING",
        "property.enable.auto.commit"="false"
    );
    ```

5. **After creating the import task, query Doris**

### Dynamic Partitioning

Dynamic partitioning was introduced in Doris 0.12 and aims to manage the lifecycle of table-level partitions (TTL), reducing the user's burden.

Currently, dynamic partitioning supports dynamically adding and deleting partitions.

**Principle:**
In certain scenarios, users partition tables by day and execute routine tasks regularly. Without dynamic partitioning, users must manually manage partitions, which may lead to failures if users forget to create data. Dynamic partitioning reduces this maintenance overhead. The Frontend (FE) will start a background thread that reads the properties of dynamic partition tables from the registry during each schedule, based on the `dynamic_partition_enable` and `dynamic_partition_check_interval_seconds` parameters in the `fe.conf` file.

#### Parameters

- `dynamic_partition.enable`: Enable or disable dynamic partitioning. Set to `true` or `false`. Default is `true`.
- `dynamic_partition.time_unit`: The unit for dynamic partitioning scheduling. Can be `day`, `week`, or `month`. If `day`, the format is `yyyyMMDD`. If `week`, the format is `yyyy_ww`, indicating the week number within the year. If `month`, the format is `yyyyMM`.
- `dynamic_partition.start`: The start time for dynamic partitioning. Based on the current day, partitions beyond this time range will be deleted. If not specified, the default value is `Integer.MIN_VALUE`, which is `-2147483648`.
- `dynamic_partition.end`: The end time for dynamic partitioning. Partitions for N units of time from this point will be created in advance.
- `dynamic_partition.prefix`: The prefix for dynamically created partition names.
- `dynamic_partition.buckets`: The number of buckets for dynamically created partitions.

#### Usage

1. **Enable Dynamic Partitioning**
    - You can enable dynamic partitioning in `fe.conf` by setting `dynamic_partition_enable=true`. Alternatively, you can modify it via command. In the command, set `dynamic_partition_check_interval_seconds` to 5 seconds, meaning the partition will be refreshed according to the configuration every 5 seconds. For testing, it's set to 5 seconds here; in real scenarios, 12 hours might be more appropriate.
    ```bash
    root@doris1:/opt/module# curl --location-trusted -u root:123456 -XGET http://doris1:8030/api/_set_config?dynamic_partition_enable=true
    root@doris1:/opt/module# curl --location-trusted -u root:123456 -XGET http://doris1:8030/api/_set_config?dynamic_partition_check_interval_seconds=5
    mysql> ADMIN SET FRONTEND CONFIG ("dynamic_partition_enable" = "true");
    mysql> ADMIN SET FRONTEND CONFIG ("dynamic_partition_check_interval_seconds"="5");
    ```

2. **Create a dynamic partition table with a daily scheduling unit and no deletion of historical partitions:**
    ```sql
    create table student_dynamic_partition1 (
        id int,
        time date,
        name varchar(50),
        age int
    )
    duplicate key(id, time)
    PARTITION BY RANGE(time) ()
    DISTRIBUTED BY HASH(id) buckets 10
    PROPERTIES(
        "dynamic_partition.enable" = "true",
        "dynamic_partition.time_unit" = "DAY",
        "dynamic_partition.end" = "3",
        "dynamic_partition.prefix" = "p",
        "dynamic_partition.buckets" = "10",
        "replication_num" = "1"
    );
    ```

3. **Check the status of the partition table using `SHOW DYNAMIC PARTITION TABLES`. Refresh the last schedule time.**

4. **Insert test data, all of which should be successful:**
    ```sql
    mysql> insert into student_dynamic_partition1 values(1,'2020-09-10 11:00:00','name1',18);
    mysql> insert into student_dynamic_partition1 values(1,'2020-09-11 11:00:00','name1',18);
    mysql> insert into student_dynamic_partition1 values(1,'2020-09-12 11:00:00','name1',18);
    ```

5. **Check all partitions under the table with `show partitions from student_dynamic_partition1`;**

### Data Export

Doris provides a feature for exporting data from tables or partitions in text format through the Broker process to remote storage, such as HDFS or BOS.

1. **Start the Hadoop cluster**

2. **Execute the export plan:**
    ```sql
    export table student_dynamic_partition1
    partition(p20200910,p20200911,p20200912,p20200913)
    to "hdfs://mycluster/user/atguigu"
    WITH BROKER "broker_name" (
        "dfs.nameservices"="mycluster",
        "dfs.ha.namenodes.mycluster"="nn1,nn2,nn3",
        "dfs.namenode.rpc-address.mycluster.nn1"= "hadoop101:8020",
        "dfs.namenode.rpc-address.mycluster.nn2"= "hadoop102:8020",
        "dfs.namenode.rpc-address.mycluster.nn3"="hadoop103:8020",
        "dfs.client.failover.proxy.provider.mycluster"="org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider"
    );
    ```

3. **After exporting, check the corresponding path in HDFS, where many files will have been created.**

4. **Use the command to view the content of a file with data. It's just simple text content.**
    ```bash
    [root@hadoop101 apache-hive-3.1.2-bin]# hadoop dfs -cat /user/atguigu/export-data-69208f5fdf5e4e63-b3f75089d7f7611f-1599717109072
    ```

### SQL Functions

1. **View function names:**
    ```sql
    mysql> show builtin functions in test_db;
    ```

2. **View detailed information for a specific function, such as the `year` function:**
    ```sql
    mysql> show full builtin functions in test_db like 'year';
    ```

3. **Official documentation:**
    - [SQL Functions Documentation](http://doris.apache.org/master/zh-CN/sql-reference/sql-functions/date-time-functions/convert_tz.html#syntax)

### Code Operations

#### Spark

1. **Create a Maven project**

2. **Write the `pom.xml` file and add build tools:**
    ```xml
    <!-- Build Information -->
    <build>
        <plugins>
            <!-- Plugin to compile Scala code into class files -->
            <plugin>
                <groupId>net.alchim31.maven</groupId>
                <artifactId>scala-maven-plugin</artifactId>
                <version>4.4.0</version>
                <executions>
                    <execution>
                        <!-- Bind to Maven's compile phase -->
                        <goals>
                            <

goal>testCompile</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-assembly-plugin</artifactId>
                <version>3.0.0</version>
                <configuration>
                    <descriptorRefs>
                        <descriptorRef>jar-with-dependencies</descriptorRef>
                    </descriptorRefs>
                </configuration>
                <executions>
                    <execution>
                        <id>make-assembly</id>
                        <phase>package</phase>
                        <goals>
                            <goal>single</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
        </plugins>
    </build>
    ```

3. **Create a Spark SQL submodule and add Scala support**

4. **Modify the submodule's `pom.xml` to add necessary JARs and build tools:**
    ```xml
    <dependencies>
        <!-- Spark Dependencies -->
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_2.11</artifactId>
            <version>2.4.5</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql_2.11</artifactId>
            <version>2.4.5</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-hive_2.11</artifactId>
            <version>2.4.5</version>
        </dependency>
        <!-- Scala Dependency -->
        <dependency>
            <groupId>org.scala-lang</groupId>
            <artifactId>scala-library</artifactId>
            <version>2.11.8</version>
        </dependency>

        <dependency>
            <groupId>com.alibaba</groupId>
            <artifactId>fastjson</artifactId>
            <version>1.2.47</version>
        </dependency>

        <!-- MySQL Connector Dependency -->
        <dependency>
            <groupId>mysql</groupId>
            <artifactId>mysql-connector-java</artifactId>
            <version>5.1.46</version>
        </dependency>
    </dependencies>
    ```

5. **Write code to read table data:**
    ```scala
    import org.apache.spark.SparkConf
    import org.apache.spark.sql.SparkSession

    object TestReadDoris {
        def main(args: Array[String]): Unit = {
            val sparkConf = new SparkConf().setAppName("testReadDoris").setMaster("local[*]")
            val sparkSession = SparkSession.builder().config(sparkConf).getOrCreate()
            val df = sparkSession.read.format("jdbc")
                .option("url", "jdbc:mysql://doris1:9030/test_db")
                .option("user", "root")
                .option("password", "123456")
                .option("dbtable", "student_dynamic_partition1")
                .load()
            df.show()
        }
    }
    ```

6. **Write data:**
    ```scala
    import java.util.Properties

    import org.apache.spark.SparkConf
    import org.apache.spark.sql.{SaveMode, SparkSession}

    object TestWriteDoris {
        def main(args: Array[String]): Unit = {
            val sparkConf = new SparkConf().setAppName("testWriteDoris").setMaster("local[*]")
            val sparkSession = SparkSession.builder().config(sparkConf).getOrCreate()
            val df = sparkSession.createDataFrame(Array((1, "2020-09-13", "testName1", 18), (1, "2020-09-13", "testName2", 18), (1, "2020-09-13", "testName3", 18)))
                .toDF("id", "time", "name", "age")
            val prop = new Properties()
            prop.setProperty("user", "root")
            prop.setProperty("password", "123456")
            df.write.mode(SaveMode.Append).jdbc("jdbc:mysql://doris1:9030/test_db", "student_dynamic_partition1", prop)
        }
    }
    ```

7. **Query table data**

### Colocation Join

Colocation Join was introduced in Doris 0.9 to optimize Join queries by reducing data transfer time between nodes, thereby speeding up queries.

**Principle:**
Colocation Join ensures that a group of tables with CGS are in the same CG (Colocation Group). This ensures that corresponding data slices of these tables are placed on the same BE node. When joining two tables, the local data can be joined directly, reducing data transfer time between nodes.

**Usage Restrictions:**
1. When creating the tables, both tables must have the same bucketing columns and the same number of buckets.
2. The replica count of all partitions in both tables must be the same.

1. **Usage: Create two tables with int-type bucketing columns, and each with 8 buckets. Both tables have the default number of replicas.**
    ```sql
    CREATE TABLE `tbl1` (
        `k1` date NOT NULL COMMENT "",
        `k2` int(11) NOT NULL COMMENT "",
        `v1` int(11) SUM NOT NULL COMMENT ""
    ) ENGINE=OLAP
    AGGREGATE KEY(`k1`, `k2`)
    PARTITION BY RANGE(`k1`) (
        PARTITION p1 VALUES LESS THAN ('2019-05-31'),
        PARTITION p2 VALUES LESS THAN ('2019-06-30')
    )
    DISTRIBUTED BY HASH(`k2`) BUCKETS 8
    PROPERTIES (
        "colocate_with" = "group1"
    );

    CREATE TABLE `tbl2` (
        `k1` datetime NOT NULL COMMENT "",
        `k2` int(11) NOT NULL COMMENT "",
        `v1` double SUM NOT NULL COMMENT ""
    ) ENGINE=OLAP
    AGGREGATE KEY(`k1`, `k2`)
    DISTRIBUTED BY HASH(`k2`) BUCKETS 8
    PROPERTIES (
        "colocate_with" = "group1"
    );
    ```

2. **Write a query and check the execution plan. If `colocate` shows as true in `HASH JOIN`, it indicates successful optimization.**
    ```sql
    mysql> explain SELECT * FROM tbl1 INNER JOIN tbl2 ON (tbl1.k2 = tbl2.k2);
    ```
