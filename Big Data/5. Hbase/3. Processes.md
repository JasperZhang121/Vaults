### Architecture Principles

![HBaseArchitecture](HBaseArchitecture.png)

1. **StoreFile**
   - The physical file that stores the actual data. StoreFiles are stored as HFiles in HDFS. Each store will have one or more StoreFiles (HFiles), and the data in each StoreFile is ordered.

2. **MemStore**
   - The write cache. Since data in HFiles must be ordered, data is first stored in MemStore, sorted, and then flushed to HFiles when the flush trigger is reached. Each flush creates a new HFile.

3. **WAL (Write-Ahead Log)**
   - Since data must be ordered in MemStore before being flushed to HFiles, storing data only in memory has a high risk of data loss. To address this, data is first written to a Write-Ahead Log (WAL) file and then written to MemStore. In case of system failure, data can be reconstructed from this log file.

### Write Process

1. **Client Access to Zookeeper**: The client first accesses Zookeeper to determine which Region Server hosts the `hbase:meta` table.
2. **Access Region Server**: The client then accesses the corresponding Region Server to query the `hbase:meta` table, identifying which Region Server and Region the target data is located in based on the namespace:table/rowkey. The region information and meta table location are cached in the client's meta cache for future requests.
3. **Communicate with Target Region Server**: The client communicates with the target Region Server.
4. **Write Data to WAL**: Data is sequentially written (appended) to the WAL.
5. **Write Data to MemStore**: Data is then written to the corresponding MemStore, where it is sorted.
6. **Send Acknowledgment**: An acknowledgment is sent back to the client.
7. **Flush Data to HFile**: When the MemStore flush trigger is reached, data is flushed to HFiles.


### MemStore Flush Triggers:

![MemStoreFlush](MemStoreFlush.png)

1. **MemStore Size Limit**: When the size of a MemStore reaches `hbase.hregion.memstore.flush.size` (default 128MB), all MemStores in the same Region will be flushed.
   - When the size of a MemStore reaches `hbase.hregion.memstore.flush.size` (default 128MB) * `hbase.hregion.memstore.block.multiplier` (default 4), further writes to that MemStore will be blocked.

2. **RegionServer MemStore Size Limit**: When the total size of MemStores in a RegionServer reaches `java_heapsize * hbase.regionserver.global.memstore.size` (default 0.4) * `hbase.regionserver.global.memstore.size.lower.limit` (default 0.95), regions will be flushed in descending order of MemStore size until the total MemStore size drops below the limit.
   - When the total MemStore size in a RegionServer reaches `java_heapsize * hbase.regionserver.global.memstore.size` (default 0.4), further writes to all MemStores will be blocked.

3. **Automatic Flush Time**: MemStores will also be flushed when the automatic flush time interval is reached.

4. **WAL File Limit**: When the number of WAL files exceeds `hbase.regionserver.max.logs`, regions will be flushed in chronological order until the number of WAL files drops below the limit (this property name has been deprecated, and manual setting is no longer required; the maximum value is 32).

## Read Process

### HBase Read Process
1. **Client Requests Zookeeper**: The client first accesses Zookeeper to find out which Region Server hosts the `hbase:meta` table.
2. **Access Region Server**: The client then accesses the corresponding Region Server to query the `hbase:meta` table, which identifies which Region Server and Region the target data is located in, based on the namespace:table/rowkey. The region information and meta table location are cached in the client's meta cache for future requests.
3. **Communicate with Target Region Server**: The client communicates with the target Region Server.
4. **Query Data**: The client searches for the target data in the Block Cache (read cache), MemStore, and Store File (HFile), merging all found data, which includes different versions (timestamp) or types (Put/Delete) of the same data.
5. **Cache Data Blocks**: The queried data blocks (Block, HFile data storage unit, default size 64KB) are cached in the Block Cache.
6. **Return Results**: The merged final result is returned to the client.

### StoreFile Compaction
Due to each MemStore flush generating a new HFile, and different versions (timestamps) and types (Put/Delete) of the same field potentially being spread across different HFiles, queries need to traverse all HFiles. To reduce the number of HFiles and clean up expired and deleted data, StoreFile Compaction is performed.

#### Types of Compaction:

1. **Minor Compaction**: Merges several small, adjacent HFiles into a larger HFile without cleaning up expired and deleted data.
2. **Major Compaction**: Merges all HFiles in a store into a single large HFile, cleaning up expired and deleted data.

### Compaction Process

1. **Minor Compaction**:
   - Selects some small, adjacent HFiles and merges them into a larger HFile.
2. **Major Compaction**:
   - Merges all HFiles in a store into a single large file, performing physical deletion operations.

### Region Split Process
By default, each Table starts with a single Region. As data is continuously written, the Region will automatically split. Initially, both child Regions remain on the current Region Server, but for load balancing, the HMaster may transfer one of the Regions to another Region Server.

#### Region Split Triggers:

1. **StoreFile Size Limit**: When the total size of all StoreFiles in a store of a region exceeds `hbase.hregion.max.filesize`, the Region will split (pre-0.94 version).
2. **Adjusted StoreFile Size Limit**: When the total size of all StoreFiles in a store of a region exceeds `Min(R^2 * "hbase.hregion.memstore.flush.size", hbase.hregion.max.filesize")`, the Region will split, where R is the number of regions in the current Region Server belonging to the table (post-0.94 version).