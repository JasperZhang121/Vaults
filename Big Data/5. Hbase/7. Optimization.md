### High Availability

In HBase, HMaster is responsible for monitoring the lifecycle of HRegionServer and balancing the load of RegionServer. If HMaster goes down, the entire HBase cluster will be in an unhealthy state and will not remain operational for long. Therefore, HBase supports high availability configuration for HMaster.

1. **Shut down the HBase cluster (skip this step if it is not running)**
    ```bash
    [a@hadoop102 hbase]$ bin/stop-hbase.sh
    ```

2. **Create the `backup-masters` file in the `conf` directory**
    ```bash
    [au@hadoop102 hbase]$ touch conf/backup-masters
    ```

3. **Configure high availability HMaster nodes in the `backup-masters` file**
    ```bash
    [a@hadoop102 hbase]$ echo hadoop103 > conf/backup-masters
    ```

4. **Copy the entire `conf` directory to other nodes**
    ```bash
    [a@hadoop102 hbase]$ scp -r conf/ hadoop103:/opt/module/hbase/
    [a@hadoop102 hbase]$ scp -r conf/ hadoop104:/opt/module/hbase/
    ```

5. **Open the web page to verify**
    ```
    http://hadooo102:16010
    ```

### Pre-Splitting Regions

Each region maintains a `StartRow` and `EndRow`. If the data being added falls within the `RowKey` range maintained by a specific region, that data is handled by that region. Following this principle, we can pre-plan the regions where the data will be placed to improve HBase performance.

1. **Manual pre-splitting**
    ```sql
    Hbase> create 'staff1', 'info', 'partition1', SPLITS => ['1000', '2000', '3000', '4000']
    ```

2. **Generate hexadecimal sequence pre-splitting**
    ```sql
    create 'staff2', 'info', 'partition2', {NUMREGIONS => 15, SPLITALGO => 'HexStringSplit'}
    ```

3. **Pre-split according to rules set in a file**
    - Create a file `splits.txt` with the following content:
        ```
        aaaa
        bbbb
        cccc
        dddd
        ```
    - Execute:
        ```sql
        create 'staff3', 'partition3', SPLITS_FILE => 'splits.txt'
        ```

4. **Use Java API to create pre-split regions**
    ```java
    // Custom algorithm to generate a series of hash values stored in a two-dimensional array
    byte[][] splitKeys = ... // some hash function
    // Create HBaseAdmin instance
    HBaseAdmin hAdmin = new HBaseAdmin(HbaseConfiguration.create());
    // Create HTableDescriptor instance
    HTableDescriptor tableDesc = new HTableDescriptor(tableName);
    // Create HBase table with pre-split regions using HTableDescriptor and hash values
    hAdmin.createTable(tableDesc, splitKeys);
    ```

### RowKey Design

A RowKey uniquely identifies a piece of data. The partition where this data is stored depends on the `RowKey`'s range within a pre-split region. The main purpose of designing `RowKey` is to ensure data is evenly distributed across all regions, preventing data skew to some extent. Here are common RowKey design schemes:

1. **Generate random numbers, hashes, and scatter values**
    - Example:
        - Original RowKey `1001` becomes `dd01903921ea24941c26a48f2cec24e0bb0e8cc7` after SHA1.
        - Original RowKey `3001` becomes `49042c54de64a1e9bf0b33e00245660ef92dc7bd` after SHA1.
        - Original RowKey `5001` becomes `7b61dec07e02c188790670af43e717f0f46e8913` after SHA1.
    - Usually, we extract samples from the dataset to decide what type of RowKey to hash as a critical value for each partition.

2. **String reversal**
    - Convert `20170524000001` to `10000042507102`
    - Convert `20170524000002` to `20000042507102`
    - This can scatter gradually put data to some extent.

3. **String concatenation**
    - `20170524000001_a12e`
    - `20170524000001_93i7`

### Memory Optimization

HBase operations require significant memory usage as Tables can be cached in memory. Generally, 70% of the available memory is allocated to HBase's Java heap. However, allocating a very large heap is not recommended as prolonged garbage collection (GC) processes can make the RegionServer unavailable for long periods. Typically, 16-48GB of memory is sufficient. If high memory usage by the framework leads to insufficient system memory, the framework itself could be dragged down by system services.

### Basic Optimization

1. **Allow appending content in HDFS files**
    - Configuration files: `hdfs-site.xml`, `hbase-site.xml`
    - Property: `dfs.support.append`
    - Description: Enabling HDFS append syncs well with HBase data synchronization and persistence. Default value is `true`.

2. **Optimize the maximum number of files that can be opened by DataNode**
    - Configuration file: `hdfs-site.xml`
    - Property: `dfs.datanode.max.transfer.threads`
    - Description: HBase typically operates on many files simultaneously. Depending on the cluster size and data operations, set this to `4096` or higher. Default value is `4096`.

3. **Optimize the waiting time for high-latency data operations**
    - Configuration file: `hdfs-site.xml`
    - Property: `dfs.image.transfer.timeout`
    - Description: For high-latency data operations, the socket needs to wait longer. It's recommended to set this value higher (default `60000` milliseconds) to ensure the socket doesn't timeout.

4. **Optimize data write efficiency**
    - Configuration file: `mapred-site.xml`
    - Properties:
        ```xml
        mapreduce.map.output.compress
        mapreduce.map.output.compress.codec
        ```
    - Description: Enabling these properties can significantly improve file write efficiency and reduce write time. Set the first property to `true` and the second property to `org.apache.hadoop.io.compress.GzipCodec` or another compression method.

5. **Set the number of RPC listeners**
    - Configuration file: `hbase-site.xml`
    - Property: `hbase.regionserver.handler.count`
    - Description: Default value is `30`. This property specifies the number of RPC listeners. Adjust it based on the number of client requests. Increase this value when there are many read/write requests.

6. **Optimize HStore file size**
    - Configuration file: `hbase-site.xml`
    - Property: `hbase.hregion.max.filesize`
    - Description: Default value is `10737418240` (10GB). If you need to run HBase MR tasks, reduce this value because each region corresponds to a map task. If a single region is too large, the map task execution time will be prolonged. This value means that when the HFile size reaches this value, the region will be split into two HFiles.

7. **Optimize HBase client cache**
    - Configuration file: `hbase-site.xml`
    - Property: `hbase.client.write.buffer`
    - Description: This property specifies the HBase client cache. Increasing this value can reduce the number of RPC calls but will consume more memory. Conversely, decreasing it will reduce memory consumption but increase RPC calls. Generally, we need to set a proper cache size to reduce the number of RPC calls.

8. **Specify the number of rows retrieved by `scan.next`**
    - Configuration file: `hbase-site.xml`
    - Property: `hbase.client.scanner.caching`
    - Description: This property specifies the default number of rows retrieved by the `scan.next` method. The larger the value, the more memory is consumed.

9. **Flush, Compact, and Split Mechanism**
    - When MemStore reaches a threshold, data in MemStore is flushed into StoreFile. The compaction mechanism merges small files generated by flush into larger StoreFiles. When a region reaches a threshold, it is split into two regions.
    - Relevant properties:
        - `hbase.hregion.memstore.flush.size`: `134217728` (128MB, default threshold for MemStore).
        - Description: This parameter flushes all MemStores in a single HRegion when the total size exceeds the specified value. RegionServer flushes requests by adding them to a queue, simulating a producer-consumer model to process them asynchronously. If the queue cannot be processed in time and a large backlog of requests occurs, memory usage can spike, potentially causing OOM (Out of Memory).
        - `hbase.regionserver.global.memstore.upperLimit`: `0.4`
        - `hbase.regionserver.global.memstore.lowerLimit`: `0.38`
        - Description: When the total memory usage of MemStore reaches the `hbase.regionserver.global.memstore.upperLimit` value, multiple MemStores will be flushed to files. The flush order of MemStores is performed in descending order of size until the memory usage is slightly below `lowerLimit`.