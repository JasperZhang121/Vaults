### MapReduce
By using HBase's Java API, we can implement MapReduce operations that interact with HBase, such as importing data from the local file system into an HBase table or performing data analysis with MapReduce after reading raw data from HBase.

#### Official HBase-MapReduce
1. **View HBase's MapReduce tasks execution**
    ```bash
    $ bin/hbase mapredcp
    ```
2. **Import environment variables**
    - Temporary import (effective only for the current session):
      ```bash
      $ export HBASE_HOME=/opt/module/hbase
      $ export HADOOP_HOME=/opt/module/hadoop-2.7.2
      $ export HADOOP_CLASSPATH=`${HBASE_HOME}/bin/hbase mapredcp`
      ```
    - Permanent import: add to `/etc/profile`
      ```bash
      export HBASE_HOME=/opt/module/hbase
      export HADOOP_HOME=/opt/module/hadoop-2.7.2
      ```
      Also configure in `hadoop-env.sh` (after the `for` loop):
      ```bash
      export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:/opt/module/hbase/lib/*
      ```
3. **Run official MapReduce tasks**
    - **Example 1**: Count the number of rows in the `Student` table
      ```bash
      $ /opt/module/hadoop-2.7.2/bin/yarn jar lib/hbase-server-1.3.1.jar rowcounter student
      ```
    - **Example 2**: Import local data into HBase using MapReduce
        1. Create a local TSV file: `fruit.tsv`
           ```
           1001    Apple   Red
           1002    Pear    Yellow
           1003    Pineapple   Yellow
           ```
        2. Create HBase table
           ```bash
           Hbase(main):001:0> create 'fruit','info'
           ```
        3. Create `input_fruit` folder in HDFS and upload `fruit.tsv` file
           ```bash
           $ /opt/module/hadoop-2.7.2/bin/hdfs dfs -mkdir /input_fruit/
           $ /opt/module/hadoop-2.7.2/bin/hdfs dfs -put fruit.tsv /input_fruit/
           ```
        4. Execute MapReduce to import data into HBase `fruit` table
           ```bash
           $ /opt/module/hadoop-2.7.2/bin/yarn jar lib/hbase-server-1.3.1.jar importtsv \
           -Dimporttsv.columns=HBASE_ROW_KEY,info:name,info:color fruit \
           hdfs://hadoop102:9000/input_fruit
           ```
        5. Use the `scan` command to view the imported results
           ```bash
           Hbase(main):001:0> scan 'fruit'
           ```

#### 4.3.2 Custom HBase-MapReduce1
**Goal**: Migrate part of the data from the `fruit` table to the `fruit_mr` table using MapReduce.

Steps to implement:
1. **Construct `ReadFruitMapper` class to read data from `fruit` table**
    ```java
    package com.a;

    import java.io.IOException;
    import org.apache.hadoop.hbase.Cell;
    import org.apache.hadoop.hbase.CellUtil;
    import org.apache.hadoop.hbase.client.Put;
    import org.apache.hadoop.hbase.client.Result;
    import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
    import org.apache.hadoop.hbase.mapreduce.TableMapper;
    import org.apache.hadoop.hbase.util.Bytes;

    public class ReadFruitMapper extends TableMapper<ImmutableBytesWritable, Put> {
        @Override
        protected void map(ImmutableBytesWritable key, Result value, Context context) 
                throws IOException, InterruptedException {
            // Extract 'name' and 'color' from 'fruit' and put into Put object
            Put put = new Put(key.get());
            // Traverse and add columns
            for (Cell cell : value.rawCells()) {
                // Add/clone column family 'info'
                if ("info".equals(Bytes.toString(CellUtil.cloneFamily(cell)))) {
                    // Add/clone column 'name'
                    if ("name".equals(Bytes.toString(CellUtil.cloneQualifier(cell)))) {
                        put.add(cell);
                    // Add/clone column 'color'
                    } else if ("color".equals(Bytes.toString(CellUtil.cloneQualifier(cell)))) {
                        put.add(cell);
                    }
                }
            }
            // Write each row's data read from 'fruit' to context as map output
            context.write(key, put);
        }
    }
    ```

2. **Construct `WriteFruitMRReducer` class to write data read from `fruit` table to `fruit_mr` table**
    ```java
    package com.atguigu.Hbase_mr;

    import java.io.IOException;
    import org.apache.hadoop.hbase.client.Put;
    import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
    import org.apache.hadoop.hbase.mapreduce.TableReducer;
    import org.apache.hadoop.io.NullWritable;

    public class WriteFruitMRReducer extends TableReducer<ImmutableBytesWritable, Put, NullWritable> {
        @Override
        protected void reduce(ImmutableBytesWritable key, Iterable<Put> values, Context context) 
                throws IOException, InterruptedException {
            // Write each row's data to 'fruit_mr' table
            for (Put put : values) {
                context.write(NullWritable.get(), put);
            }
        }
    }
    ```

3. **Construct `Fruit2FruitMRRunner` extends `Configured` implements `Tool` to assemble and run the Job**
    ```java
    // Assemble Job
    public int run(String[] args) throws Exception {
        // Get Configuration
        Configuration conf = this.getConf();
        // Create Job
        Job job = Job.getInstance(conf, this.getClass().getSimpleName());
        job.setJarByClass(Fruit2FruitMRRunner.class);
        // Configure Job
        Scan scan = new Scan();
        scan.setCacheBlocks(false);
        scan.setCaching(500);
        // Set Mapper
        TableMapReduceUtil.initTableMapperJob(
            "fruit", // Table name
            scan, // Scan controller
            ReadFruitMapper.class, // Mapper class
            ImmutableBytesWritable.class, // Mapper output key type
            Put.class, // Mapper output value type
            job // Job
        );
        // Set Reducer
        TableMapReduceUtil.initTableReducerJob("fruit_mr", WriteFruitMRReducer.class, job);
        // Set Reduce tasks
        job.setNumReduceTasks(1);
        boolean isSuccess = job.waitForCompletion(true);
        if (!isSuccess) {
            throw new IOException("Job running with error");
        }
        return isSuccess ? 0 : 1;
    }
    ```

4. **Call and run the Job task in the main function**
    ```java
    public static void main(String[] args) throws Exception {
        Configuration conf = HBaseConfiguration.create();
        int status = ToolRunner.run(conf, new Fruit2FruitMRRunner(), args);
        System.exit(status);
    }
    ```

5. **Package and run the task**
    ```bash
    $ /opt/module/hadoop-2.7.2/bin/yarn jar ~/softwares/jars/Hbase-0.0.1-SNAPSHOT.jar com.z.Hbase.mr1.Fruit2FruitMRRunner
    ```

**Tips**:
- Before running the task, ensure that the table to import data into exists. If not, create it in advance.
- Maven packaging command: `-P local clean package` or `-P dev clean package install` (to package third-party jars, use the `maven-shade-plugin`).

#### Custom HBase-MapReduce2
**Goal**: Write data from HDFS into an HBase table.

Steps to implement:
1. **Construct `ReadFruitFromHDFSMapper` to read file data from HDFS**
    ```java
    package com.atguigu;

    import java.io.IOException;
    import org.apache.hadoop.hbase.client.Put;
    import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
    import org.apache.hadoop.hbase.util.Bytes;
    import org.apache.hadoop.io.LongWritable;
    import org.apache.hadoop.io.Text;
    import org.apache.hadoop.mapreduce.Mapper;

    public class ReadFruitFromHDFSMapper extends Mapper<LongWritable, Text, ImmutableBytesWritable, Put> {
        @Override
        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
            // Read data from HDFS
            String lineValue = value.toString();
            // Split each line's data using \t, store in String array
            String[] values = lineValue.split("\t");
            // Get values based on data meaning
            String rowKey = values[0];
            String name = values[1];
            String color = values[2];
            // Initialize rowKey
            ImmutableBytesWritable rowKeyWritable = new ImmutableBytesWritable(Bytes.toBytes(rowKey));
            // Initialize put object
            Put put = new Put(Bytes.toBytes(rowKey));
            // Parameters: column family, column, value
            put.add(Bytes.toBytes("info"), Bytes.toBytes("name"), Bytes.toBytes(name));
            put.add(Bytes.toBytes("info"), Bytes.toBytes("color"), Bytes.toBytes(color));
            context.write(rowKeyWritable, put);
        }
    }
    ```

2. **Construct `WriteFruitMRFromTxtReducer` class**
    ```java
    package com.z.Hbase.mr2

;

    import java.io.IOException;
    import org.apache.hadoop.hbase.client.Put;
    import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
    import org.apache.hadoop.hbase.mapreduce.TableReducer;
    import org.apache.hadoop.io.NullWritable;

    public class WriteFruitMRFromTxtReducer extends TableReducer<ImmutableBytesWritable, Put, NullWritable> {
        @Override
        protected void reduce(ImmutableBytesWritable key, Iterable<Put> values, Context context) 
                throws IOException, InterruptedException {
            // Write each row's data to 'fruit_hdfs' table
            for (Put put : values) {
                context.write(NullWritable.get(), put);
            }
        }
    }
    ```

3. **Create `Txt2FruitRunner` to assemble Job**
    ```java
    public int run(String[] args) throws Exception {
        // Get Configuration
        Configuration conf = this.getConf();
        // Create Job
        Job job = Job.getInstance(conf, this.getClass().getSimpleName());
        job.setJarByClass(Txt2FruitRunner.class);
        Path inPath = new Path("hdfs://hadoop102:9000/input_fruit/fruit.tsv");
        FileInputFormat.addInputPath(job, inPath);
        // Set Mapper
        job.setMapperClass(ReadFruitFromHDFSMapper.class);
        job.setMapOutputKeyClass(ImmutableBytesWritable.class);
        job.setMapOutputValueClass(Put.class);
        // Set Reducer
        TableMapReduceUtil.initTableReducerJob("fruit_mr", WriteFruitMRFromTxtReducer.class, job);
        // Set Reduce tasks
        job.setNumReduceTasks(1);
        boolean isSuccess = job.waitForCompletion(true);
        if (!isSuccess) {
            throw new IOException("Job running with error");
        }
        return isSuccess ? 0 : 1;
    }
    ```

4. **Call and execute the Job**
    ```java
    public static void main(String[] args) throws Exception {
        Configuration conf = HBaseConfiguration.create();
        int status = ToolRunner.run(conf, new Txt2FruitRunner(), args);
        System.exit(status);
    }
    ```

5. **Package and run the task**
    ```bash
    $ /opt/module/hadoop-2.7.2/bin/yarn jar hbase-0.0.1-SNAPSHOT.jar com.atguigu.hbase.mr2.Txt2FruitRunner
    ```
