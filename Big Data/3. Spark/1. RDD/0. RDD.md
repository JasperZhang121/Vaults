### RDD
RDD (Resilient Distributed Dataset) is the most fundamental data processing model in Spark. It is an abstract class that represents a resilient, immutable, partitioned collection of elements that can be computed in parallel.
- **Resilient**
    - **Storage Resilience:** Automatic switching between memory and disk.
    - **Fault Tolerance:** Automatic recovery of lost data.
    - **Computation Resilience:** Retry mechanism for failed computations.
    - **Partitioning Resilience:** Ability to repartition as needed.
- **Distributed:** Data is stored across different nodes in a big data cluster.
- **Dataset:** RDD encapsulates the computation logic but does not store the data.
- **Data Abstraction:** RDD is an abstract class that requires subclass implementation.
- **Immutable:** RDDs encapsulate computation logic and cannot be changed. Any change results in the creation of a new RDD with the new computation logic.
- **Partitioned and Parallel Computation**

### Core Properties
- **Partition List:**
    The RDD data structure <mark style="background: #FFB8EBA6;">contains a list of partitions used for parallel computation</mark>, which is crucial for implementing distributed computing.
- **Partition Computation Function:**
    Spark uses a partition function to compute each partition individually.
- **Partitioner:**
    For key-value type data, a partitioner can be set to customize data partitioning.
- **Preferred Location:**
    During data computation, different node locations can be chosen based on the state of computation nodes.

### Execution Principle
From a computational perspective, data processing requires computing resources (memory & CPU) and a computation model (logic). During execution, computing resources and the computation model must be coordinated and integrated. The Spark framework first requests resources and then decomposes the application's data processing logic into individual computing tasks. These tasks are sent to the allocated computing nodes to execute the specified computation model, ultimately yielding the computation results. RDD is the core model used for data processing in the Spark framework. Let's look at how RDD works in a Yarn environment:
1. Start the Yarn cluster environment.
2. Spark creates scheduling nodes and computing nodes by requesting resources.
3. The Spark framework partitions the computation logic into different tasks based on partitions.
4. Scheduling nodes send tasks to the corresponding computing nodes based on their state for computation.

From the above process, it is clear that RDD mainly encapsulates the logic and generates Tasks to be sent to Executor nodes for computation.

### Basic Programming
#### RDD Creation
There are four ways to create RDDs in Spark:
1. **Creating RDD from a Collection (Memory)**
    
    Spark provides two methods to create RDDs from a collection: `parallelize` and `makeRDD`.

    ```scala
    val sparkConf = new SparkConf().setMaster("local[*]").setAppName("spark")
    val sparkContext = new SparkContext(sparkConf)
    val rdd1 = sparkContext.parallelize(List(1,2,3,4))
    val rdd2 = sparkContext.makeRDD(List(1,2,3,4))
    rdd1.collect().foreach(println)
    rdd2.collect().foreach(println)
    sparkContext.stop()
    ```

    Internally, the `makeRDD` method is just a wrapper around the `parallelize` method.

    ```scala
    def makeRDD[T: ClassTag](
     seq: Seq[T],
     numSlices: Int = defaultParallelism): RDD[T] = withScope {
     parallelize(seq, numSlices)
    }
    ```

2. **Creating RDD from External Storage (File)**
    
    Creating RDDs from external storage systems includes local file systems and all Hadoop-supported datasets like HDFS, HBase, etc.

    ```scala
    val sparkConf = new SparkConf().setMaster("local[*]").setAppName("spark")
    val sparkContext = new SparkContext(sparkConf)
    val fileRDD: RDD[String] = sparkContext.textFile("input")
    fileRDD.collect().foreach(println)
    sparkContext.stop()
    ```

3. **Creating RDD from Other RDDs**
    
    This is mainly achieved by performing operations on an existing RDD to produce a new RDD. Refer to subsequent chapters for details.

4. **Directly Creating RDD (new)**
    
    Using the `new` keyword to directly construct an RDD is generally used by the Spark framework itself.

#### Internal Logic

When creating an RDD using `parallelize` or `makeRDD` in Apache Spark, we start with an in-memory collection (like a List or Array) because this collection represents the initial dataset that we want to distribute and process in parallel across the Spark cluster. Understanding the internal logic behind this process helps to appreciate how Spark manages data distribution and parallel computation. Here’s an explanation of the internal logic:

1. **Starting with a Collection**:
   - When you use `parallelize` or `makeRDD`, you provide a collection that contains the data you want to process. This collection is typically a List, Array, or other sequence in Scala.
   - Example:
     ```scala
     val data = List(1, 2, 3, 4)
     ```

2. **Creating the RDD**:
   - Spark's `parallelize` method takes the collection and creates an RDD from it. Here’s a simplified view of what happens internally:
     ```scala
     val rdd = sparkContext.parallelize(data)
     ```

3. **Data Distribution**:
   - Spark splits the collection into partitions, distributing the data across the different nodes in the cluster. The number of partitions can be specified using the `numSlices` parameter. If not specified, Spark uses a default value based on the cluster configuration.
   - Each partition contains a subset of the original data collection.
   - Internally, this involves creating `ParallelCollectionRDD` objects.

4. **ParallelCollectionRDD**:
   - The `ParallelCollectionRDD` is a specialized RDD type used for collections created from in-memory datasets. It takes care of dividing the collection into partitions and distributing them.
   - Example internal code (simplified):
     ```scala
     val rdd = new ParallelCollectionRDD(sparkContext, data, numSlices)
     ```

5. **Tasks and Partitioning**:
   - Each partition in the RDD is treated as a task that can be executed independently and in parallel by different nodes in the cluster.
   - Spark's scheduler assigns these tasks to worker nodes, where they are processed concurrently.

6. **Execution**:
   - When an action (like `collect`) is called on the RDD, Spark's execution engine schedules and runs the tasks on the distributed data.
   - Example:
     ```scala
     rdd.collect().foreach(println)
     ```
   - This triggers the computation, and each node processes its partition of the data, returning the results to the driver program.

#### Detailed Steps in Code

1. **Driver Program**:
   - The driver program (where you write your Spark application) initiates the creation of an RDD from a collection.
   - Example:
     ```scala
     val sparkConf = new SparkConf().setMaster("local[*]").setAppName("spark")
     val sparkContext = new SparkContext(sparkConf)
     val data = List(1, 2, 3, 4)
     val rdd = sparkContext.parallelize(data, 2) // 2 partitions
     ```

2. **ParallelCollectionRDD**:
   - Spark constructs a `ParallelCollectionRDD` instance.
   - The collection is divided into 2 partitions (or the number you specify).
   - Internal constructor (simplified):
     ```scala
     new ParallelCollectionRDD(sparkContext, data, numSlices)
     ```

3. **Task Distribution**:
   - Each partition becomes a task.
   - Spark's scheduler assigns these tasks to executors on worker nodes.

4. **Execution**:
   - When an action like `collect` is invoked:
     ```scala
     rdd.collect().foreach(println)
     ```
   - Tasks are executed on the worker nodes, each processing its partition and sending the results back to the driver.

### RDD Parallelism and Partitioning
By default, Spark can split a job into multiple tasks and send them to Executor nodes for parallel computation. The number of tasks that can be executed in parallel is referred to as parallelism. This number can be specified when constructing an RDD. The number of tasks executed in parallel is not the same as the number of partitions.

```scala
val sparkConf = new SparkConf().setMaster("local[*]").setAppName("spark")
val sparkContext = new SparkContext(sparkConf)
val dataRDD: RDD[Int] = sparkContext.makeRDD(List(1, 2, 3, 4), 4)
val fileRDD: RDD[String] = sparkContext.textFile("input", 2)
fileRDD.collect().foreach(println)
sparkContext.stop()
```

- When reading data from memory, the data can be partitioned according to the specified parallelism. The core Spark source code for data partitioning is as follows:

```scala
def positions(length: Long, numSlices: Int): Iterator[(Int, Int)] = {
 (0 until numSlices).iterator.map { i =>
   val start = ((i * length) / numSlices).toInt
   val end = (((i + 1) * length) / numSlices).toInt
   (start, end)
 }
}
```

- When reading data from a file, the data is partitioned according to Hadoop's file reading rules. There are some differences between the slicing rules and the data reading rules. The core Spark source code is as follows:

```java
public InputSplit[] getSplits(JobConf job, int numSplits)
 throws IOException {
   long totalSize = 0; // compute total size
   for (FileStatus file : files) { // check we have valid files
     if (file.isDirectory()) {
       throw new IOException("Not a file: " + file.getPath());
     }
     totalSize += file.getLen();
   }
   long goalSize = totalSize / (numSplits == 0 ? 1 : numSplits);
   long minSize = Math.max(job.getLong(org.apache.hadoop.mapreduce.lib.input.
     FileInputFormat.SPLIT_MINSIZE, 1), minSplitSize);

   // Further implementation details
   ...
   for (FileStatus file : files) {
     ...
     if (isSplitable(fs, path)) {
       long blockSize = file.getBlockSize();
       long splitSize = computeSplitSize(goalSize, minSize, blockSize);
       ...
     }
   }

 protected long computeSplitSize(long goalSize, long minSize,
   long blockSize) {
   return Math.max(minSize, Math.min(goalSize, blockSize));
 }
```

In the above code:
- `positions` function is used for partitioning data read from memory.
- `getSplits` method is used for partitioning data read from files according to Hadoop's file reading rules, considering factors like block size and minimum split size.