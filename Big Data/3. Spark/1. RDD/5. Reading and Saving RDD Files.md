Spark's data reading and saving capabilities can be categorized along two dimensions: file formats and file systems.
File formats include: text files, CSV files, sequence files, and object files;
File systems include: local file system, HDFS, HBase, and databases.

#### Text Files
```java
// Reading input file
val inputRDD: RDD[String] = sc.textFile("input/1.txt")
// Saving data
inputRDD.saveAsTextFile("output")
```

#### Sequence Files

A SequenceFile is a flat file consisting of binary key-value pairs designed by Hadoop. In SparkContext, you can call `sequenceFile[keyClass, valueClass](path)`.

```java
// Saving data as SequenceFile
dataRDD.saveAsSequenceFile("output")
// Reading SequenceFile
sc.sequenceFile[Int, Int]("output").collect().foreach(println)
```

#### Object Files
Object files are files where objects are serialized and saved, using Java's serialization mechanism. You can use the `objectFile[T: ClassTag](path)` function to read object files and return the corresponding RDD. Similarly, you can call `saveAsObjectFile()` to output object files. Since serialization is involved, the type must be specified.

```java
// Saving data
dataRDD.saveAsObjectFile("output")
// Reading data
sc.objectFile[Int]("output").collect().foreach(println)
```
