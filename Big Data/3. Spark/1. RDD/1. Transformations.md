### RDD Transformations

RDD transformations in Spark are broadly categorized into Value types, Double Value types, and Key-Value types.

### Value Types

#### map
- **Function Signature:**
```scala
def map[U: ClassTag](f: T => U): RDD[U]
```
- **Function Description:**
Maps each element of the dataset to a new element, potentially changing the element's type or value.
```scala
val dataRDD: RDD[Int] = sparkContext.makeRDD(List(1, 2, 3, 4))
val dataRDD1: RDD[Int] = dataRDD.map(num => num * 2)
val dataRDD2: RDD[String] = dataRDD1.map(num => s"$num")
```
- **Example:**
Extract user request URL paths from server log data `apache.log`.

#### mapPartitions
- **Function Signature:**
```scala
def mapPartitions[U: ClassTag](
 f: Iterator[T] => Iterator[U],
 preservesPartitioning: Boolean = false): RDD[U]
```

- **Function Description:**
Processes data by partition, sending each partition to a computation node for processing. This can include any arbitrary processing, even filtering data.
```scala
val dataRDD1: RDD[Int] = dataRDD.mapPartitions(datas => datas.filter(_ == 2))
```
- **Example:**
Retrieve the maximum value from each data partition.

**Difference between map and mapPartitions:**
- **Data Processing Perspective:**
  - `map` processes one element at a time within a partition, similar to serial operations.
  - `mapPartitions` processes data by partition, handling the entire partition in batch.
- **Functionality Perspective:**
  - `map` transforms and changes the data source without changing the number of elements.
  - `mapPartitions` can change the number of elements, as it accepts and returns an iterator, allowing for addition or reduction of data.
- **Performance Perspective:**
  - `map` is similar to serial operations, thus performance is lower.
  - `mapPartitions` is similar to batch processing, thus performance is higher, but it may occupy memory for a long time, leading to potential memory overflow errors. In limited memory scenarios, `map` is recommended.

#### mapPartitionsWithIndex
- **Function Signature:**
```scala
def mapPartitionsWithIndex[U: ClassTag](
 f: (Int, Iterator[T]) => Iterator[U],
 preservesPartitioning: Boolean = false): RDD[U]
```
- **Function Description:**
Processes data by partition, allowing arbitrary processing, even filtering data. Additionally, it provides the current partition index.
```scala
val dataRDD1 = dataRDD.mapPartitionsWithIndex(
 (index, datas) => datas.map(data => (index, data))
)
```
- **Example:**
Retrieve data from the second partition.

#### flatMap
- **Function Signature:**
```scala
def flatMap[U: ClassTag](f: T => TraversableOnce[U]): RDD[U]
```
- **Function Description:**
Flattens the data and then applies a mapping transformation, thus also known as a flat map.
```scala
val dataRDD = sparkContext.makeRDD(List(List(1, 2), List(3, 4)), 1)
val dataRDD1 = dataRDD.flatMap(list => list)
```
This example flattens the list of lists into a single list: List(1, 2, 3, 4).

- **Example:**
Flatten the list `List(List(1,2),3,List(4,5))`.

#### glom
- **Function Signature:**
```scala
def glom(): RDD[Array[T]]
```
- **Function Description:**
Transforms the data in the same partition into an array of the same type, keeping partitions unchanged.
```scala
val dataRDD = sparkContext.makeRDD(List(1, 2, 3, 4), 1)
val dataRDD1: RDD[Array[Int]] = dataRDD.glom()
```

- **Example:**
Sum the maximum values of all partitions (take the maximum value in each partition, and sum these maximum values).

#### groupBy
- **Function Signature:**
```scala
def groupBy[K](f: T => K)(implicit kt: ClassTag[K]): RDD[(K, Iterable[T])]
```
- **Function Description:**
Groups data according to the specified rule. The partitions remain unchanged, but data is shuffled and reorganized. This operation can result in data being grouped in the same partition.
```scala
val dataRDD = sparkContext.makeRDD(List(1, 2, 3, 4), 1)
val dataRDD1 = dataRDD.groupBy(_ % 2)
```
- **Example:**
Group words by their first letter from the list `List("Hello", "hive", "hbase", "Hadoop")`.
- **Example:**
Retrieve access count for each time period from server log data `apache.log`.
- **Example:**
WordCount.

#### filter
- **Function Signature:**
```scala
def filter(f: T => Boolean): RDD[T]
```
- **Function Description:**
Filters data according to the specified rule, keeping data that meets the criteria and discarding the rest. After filtering, partitions remain unchanged, but the data within partitions might become unbalanced, potentially causing data skew in a production environment.
```scala
val dataRDD = sparkContext.makeRDD(List(1, 2, 3, 4), 1)
val dataRDD1 = dataRDD.filter(_ % 2 == 0)
```
The `filter` function filters the elements of the RDD using a predicate function `f`. Only the elements that satisfy the predicate are retained in the resulting RDD.

- **Example:**
Retrieve request paths for May 17, 2015, from server log data `apache.log`.

#### sample
- **Function Signature:**
```scala
def sample(
 withReplacement: Boolean,
 fraction: Double,
 seed: Long = Utils.random.nextLong): RDD[T]
```

- **Function Description:**
Samples data from the dataset according to the specified rules.
```scala
val dataRDD = sparkContext.makeRDD(List(1, 2, 3, 4), 1)
// Sampling without replacement (Bernoulli algorithm)
// Bernoulli algorithm: also called 0-1 distribution. For example, flipping a coin results in either heads or tails.
// Implementation: A number is generated based on the seed and random algorithm, and compared with the second parameter (fraction). If the number is less than the fraction, the element is included; otherwise, it is excluded.
// First parameter: Whether the sampled data is replaced, false: not replaced
// Second parameter: Sampling fraction, range [0,1], 0: none; 1: all
// Third parameter: Random seed
val dataRDD1 = dataRDD.sample(false, 0.5)
// Sampling with replacement (Poisson algorithm)
// First parameter: Whether the sampled data is replaced, true: replaced; false: not replaced
// Second parameter: Probability of repetition, greater than or equal to 0, represents the expected number of times each element is sampled
// Third parameter: Random seed
val dataRDD2 = dataRDD.sample(true, 2)
```

#### distinct
- **Function Signature:**
```scala
def distinct()(implicit ord: Ordering[T] = null): RDD[T]
def distinct(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T]
```
- **Function Description:**
Removes duplicates from the dataset.
```scala
val dataRDD = sparkContext.makeRDD(List(1, 2, 3, 4, 1, 2), 1)
val dataRDD1 = dataRDD.distinct()
val dataRDD2 = dataRDD.distinct(2)
```

#### coalesce
- **Function Signature:**
```scala
def coalesce(numPartitions: Int, shuffle: Boolean = false,
 partitionCoalescer: Option[PartitionCoalescer] = Option.empty)
 (implicit ord: Ordering[T] = null): RDD[T]
```
- **Function Description:**
Reduces the number of partitions according to the data size, improving the execution efficiency of small datasets after filtering large datasets. If there are too many small tasks in a Spark program, the `coalesce` method can be used to merge partitions, reducing the number of partitions and lowering the task scheduling cost.
```scala
val dataRDD = sparkContext.makeRDD(List(1, 2, 3, 4, 1, 2), 6)
val dataRDD1 = dataRDD.coalesce(2)
```

#### repartition
- **Function Signature:**
```scala
def repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T]
```
- **Function Description:**
This operation internally executes the `coalesce` operation with the `shuffle` parameter set to true by default. The `repartition` operation can increase or decrease the number of partitions regardless of the initial number of partitions because it always involves a shuffle process.
```scala
val dataRDD = sparkContext.makeRDD(List(1, 2, 3, 4, 1, 2), 2)
val dataRDD1 = dataRDD.repartition(4)
```

#### sortBy
- **Function Signature:**
```scala
def sortBy[K](
 f: (T) => K,
 ascending: Boolean = true,
 numPartitions: Int = this.partitions.length)
 (implicit ord: Ordering[K], ctag: ClassTag[K]): RDD[T]
```
-

 **Function Description:**
Sorts the data. Before sorting, the data can be processed by the function `f`, and the result of `f` is then sorted in ascending order by default. The number of partitions in the new RDD after sorting remains the same as the original RDD. There is a shuffle process in between.
```scala
val dataRDD = sparkContext.makeRDD(List(1, 2, 3, 4, 1, 2), 2)
val dataRDD1 = dataRDD.sortBy(num => num, ascending = false, numPartitions = 4)
```

### Double Value Types

#### intersection
- **Function Signature:**
```scala
def intersection(other: RDD[T]): RDD[T]
```
- **Function Description:**
Returns a new RDD containing the intersection of elements between the source RDD and the parameter RDD.
```scala
val dataRDD1 = sparkContext.makeRDD(List(1, 2, 3, 4))
val dataRDD2 = sparkContext.makeRDD(List(3, 4, 5, 6))
val dataRDD = dataRDD1.intersection(dataRDD2)
```

#### union
- **Function Signature:**
```scala
def union(other: RDD[T]): RDD[T]
```
- **Function Description:**
Returns a new RDD containing the union of elements between the source RDD and the parameter RDD.
```scala
val dataRDD1 = sparkContext.makeRDD(List(1, 2, 3, 4))
val dataRDD2 = sparkContext.makeRDD(List(3, 4, 5, 6))
val dataRDD = dataRDD1.union(dataRDD2)
```

#### subtract
- **Function Signature:**
```scala
def subtract(other: RDD[T]): RDD[T]
```
- **Function Description:**
Returns a new RDD that contains the elements of the source RDD after removing the elements present in the parameter RDD.
```scala
val dataRDD1 = sparkContext.makeRDD(List(1, 2, 3, 4))
val dataRDD2 = sparkContext.makeRDD(List(3, 4, 5, 6))
val dataRDD = dataRDD1.subtract(dataRDD2)
```

#### zip
- **Function Signature:**
```scala
def zip[U: ClassTag](other: RDD[U]): RDD[(T, U)]
```
- **Function Description:**
Zips the elements of the source RDD with the elements of the parameter RDD, forming pairs. The keys are the elements of the source RDD, and the values are the elements at the corresponding positions in the parameter RDD.
```scala
val dataRDD1 = sparkContext.makeRDD(List(1, 2, 3, 4))
val dataRDD2 = sparkContext.makeRDD(List(3, 4, 5, 6))
val dataRDD = dataRDD1.zip(dataRDD2)
```

### Key-Value Types

#### partitionBy
- **Function Signature:**
```scala
def partitionBy(partitioner: Partitioner): RDD[(K, V)]
```
- **Function Description:**
Repartitions the data according to the specified `Partitioner`. Spark's default partitioner is `HashPartitioner`.
```scala
val rdd: RDD[(Int, String)] = sc.makeRDD(Array((1, "aaa"), (2, "bbb"), (3, "ccc")), 3)
import org.apache.spark.HashPartitioner
val rdd2: RDD[(Int, String)] = rdd.partitionBy(new HashPartitioner(2))
 ```

#### reduceByKey
- **Function Signature:**
```scala
def reduceByKey(func: (V, V) => V): RDD[(K, V)]
def reduceByKey(func: (V, V) => V, numPartitions: Int): RDD[(K, V)]
```
- **Function Description:**
Aggregates the values of each key in the dataset using the specified binary function.
```scala
val dataRDD1 = sparkContext.makeRDD(List(("a", 1), ("b", 2), ("c", 3)))
val dataRDD2 = dataRDD1.reduceByKey(_ + _)
val dataRDD3 = dataRDD1.reduceByKey(_ + _, 2)
```
- **Example:**
WordCount.

#### groupByKey
- **Function Signature:**
```scala
def groupByKey(): RDD[(K, Iterable[V])]
def groupByKey(numPartitions: Int): RDD[(K, Iterable[V])]
def groupByKey(partitioner: Partitioner): RDD[(K, Iterable[V])]
```
- **Function Description:**
Groups the values of each key in the dataset.
```scala
val dataRDD1 = sparkContext.makeRDD(List(("a", 1), ("b", 2), ("c", 3)))
val dataRDD2 = dataRDD1.groupByKey()
val dataRDD3 = dataRDD1.groupByKey(2)
val dataRDD4 = dataRDD1.groupByKey(new HashPartitioner(2))
```
**From a shuffle perspective:**
Both `reduceByKey` and `groupByKey` involve shuffle operations. However, `reduceByKey` can perform pre-aggregation (combine) within partitions before the shuffle, reducing the amount of data written to disk, making `reduceByKey` more efficient. In contrast, `groupByKey` only performs grouping without reducing data volume. Therefore, `reduceByKey` is recommended for grouping and aggregation scenarios, while `groupByKey` should be used when only grouping is required without aggregation.

- **Example:**
WordCount.

#### aggregateByKey
- **Function Signature:**
```scala
def aggregateByKey[U: ClassTag](zeroValue: U)(seqOp: (U, V) => U,
 combOp: (U, U) => U): RDD[(K, U)]
 ```
- **Function Description:**
Aggregates data within and across partitions based on different rules.
```scala
val dataRDD1 = sparkContext.makeRDD(List(("a", 1), ("b", 2), ("c", 3)))
val dataRDD2 = dataRDD1.aggregateByKey(0)(_ + _, _ + _)
 ```
- **Example:**
Extract the maximum value of each key within each partition and sum these maximum values across partitions.
```scala
val rdd = sc.makeRDD(List(("a", 1), ("a", 2), ("c", 3), ("b", 4), ("c", 5), ("c", 6)), 2)
val resultRDD = rdd.aggregateByKey(10)(
 (x, y) => math.max(x, y),
 (x, y) => x + y
)
resultRDD.collect().foreach(println)
```

#### foldByKey
- **Function Signature:**
```scala
def foldByKey(zeroValue: V)(func: (V, V) => V): RDD[(K, V)]
```
- **Function Description:**
When the rules for computing within and across partitions are the same, `aggregateByKey` can be simplified to `foldByKey`.
```scala
val dataRDD1 = sparkContext.makeRDD(List(("a", 1), ("b", 2), ("c", 3)))
val dataRDD2 = dataRDD1.foldByKey(0)(_ + _)
```

#### combineByKey
- **Function Signature:**
```scala
def combineByKey[C](
 createCombiner: V => C,
 mergeValue: (C, V) => C,
 mergeCombiners: (C, C) => C): RDD[(K, C)]
 ```
- **Function Description:**
The most general aggregation function for aggregating key-value pairs in an RDD. Similar to `aggregate()`, `combineByKey()` allows the return value type to be different from the input value type.

**Exercise: Calculate the average value for each key from the dataset `List(("a", 88), ("b", 95), ("a", 91), ("b", 93), ("a", 95), ("b", 98))`.**
```scala
val list: List[(String, Int)] = List(("a", 88), ("b", 95), ("a", 91), ("b", 93), ("a", 95), ("b", 98))
val input: RDD[(String, Int)] = sc.makeRDD(list, 2)
val combineRdd: RDD[(String, (Int, Int))] = input.combineByKey(
 (_, 1),
 (acc: (Int, Int), v) => (acc._1 + v, acc._2 + 1),
 (acc1: (Int, Int), acc2: (Int, Int)) => (acc1._1 + acc2._1, acc1._2 + acc2._2)
)
```
- **Comparison:**
  - `reduceByKey`: Does not perform any computation on the first element with the same key, and the rules for computing within and across partitions are the same.
  - `foldByKey`: Computes the first element with the same key and the initial value within partitions, with the same rules for computing within and across partitions.
  - `aggregateByKey`: Computes the first element with the same key and the initial value within partitions, with different rules for computing within and across partitions.
  - `combineByKey`: Allows the first element to change structure if the data structure does not meet requirements during computation. The rules for computing within and across partitions are different.

#### sortByKey
- **Function Signature

:**
```scala
def sortByKey(ascending: Boolean = true, numPartitions: Int = self.partitions.length): RDD[(K, V)]
```
- **Function Description:**
Sorts the RDD by keys. The keys must implement the `Ordered` interface (trait), returning an RDD sorted by keys.
```scala
val dataRDD1 = sparkContext.makeRDD(List(("a", 1), ("b", 2), ("c", 3)))
val sortRDD1: RDD[(String, Int)] = dataRDD1.sortByKey(true)
val sortRDD2: RDD[(String, Int)] = dataRDD1.sortByKey(false)
```
- **Example:**
Set key as a custom class `User`.

#### join
- **Function Signature:**
```scala
def join[W](other: RDD[(K, W)]): RDD[(K, (V, W))]
```
- **Function Description:**
Performs an inner join on two RDDs of type `(K, V)` and `(K, W)`, returning an RDD of type `(K, (V, W))`.
```scala
val rdd: RDD[(Int, String)] = sc.makeRDD(Array((1, "a"), (2, "b"), (3, "c")))
val rdd1: RDD[(Int, Int)] = sc.makeRDD(Array((1, 4), (2, 5), (3, 6)))
rdd.join(rdd1).collect().foreach(println)
```

#### leftOuterJoin
- **Function Signature:**
```scala
def leftOuterJoin[W](other: RDD[(K, W)]): RDD[(K, (V, Option[W]))]
```
- **Function Description:**
Performs a left outer join, similar to SQL's left outer join.
```scala
val dataRDD1 = sparkContext.makeRDD(List(("a", 1), ("b", 2), ("c", 3)))
val dataRDD2 = sparkContext.makeRDD(List(("a", 1), ("b", 2), ("c", 3)))
val rdd: RDD[(String, (Int, Option[Int]))] = dataRDD1.leftOuterJoin(dataRDD2)
```

#### cogroup
- **Function Signature:**
```scala
def cogroup[W](other: RDD[(K, W)]): RDD[(K, (Iterable[V], Iterable[W]))]
```
- **Function Description:**
Performs a cogroup operation on two RDDs of type `(K, V)` and `(K, W)`, returning an RDD of type `(K, (Iterable[V], Iterable[W]))`.
```scala
val dataRDD1 = sparkContext.makeRDD(List(("a", 1), ("a", 2), ("c", 3)))
val dataRDD2 = sparkContext.makeRDD(List(("a", 1), ("c", 2), ("c", 3)))
val value: RDD[(String, (Iterable[Int], Iterable[Int]))] = dataRDD1.cogroup(dataRDD2)
```