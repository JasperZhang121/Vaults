### RDD Serialization

1) **Closure Check**
   From a computational perspective, code outside operators runs on the Driver, while code inside operators runs on Executors. In Scala's functional programming, this often leads to closures where operators use external variables. If these external variables cannot be serialized, errors occur because values cannot be sent to Executors. Therefore, before task execution, the closure is checked to ensure that all objects within it are serializable. This process is called closure checking. Note that in Scala 2.12, closure compilation has changed.

2) **Serialization Methods and Properties**
   From a computational perspective, code outside operators runs on the Driver, while code inside operators runs on Executors. Consider the following code:

   ```scala
   object SerializableExample {
     def main(args: Array[String]): Unit = {
       val conf: SparkConf = new SparkConf().setAppName("SparkCoreTest").setMaster("local[*]")
       val sc: SparkContext = new SparkContext(conf)
       val rdd: RDD[String] = sc.makeRDD(Array("hello world", "hello spark", "hive", "atguigu"))
       val search = new Search("hello")
       search.getMatch1(rdd).collect().foreach(println)
       search.getMatch2(rdd).collect().foreach(println)
       sc.stop()
     }
   }

   class Search(query: String) extends Serializable {
     def isMatch(s: String): Boolean = {
       s.contains(query)
     }

     // Function serialization example
     def getMatch1(rdd: RDD[String]): RDD[String] = {
       rdd.filter(isMatch)
     }

     // Property serialization example
     def getMatch2(rdd: RDD[String]): RDD[String] = {
       rdd.filter(_.contains(query))
     }
   }
   ```

3) **Kryo Serialization Framework**
   Refer to: [Kryo](https://github.com/EsotericSoftware/kryo). Java serialization can serialize any class but is heavy and results in large serialized objects. For performance reasons, Spark 2.0+ supports the Kryo serialization mechanism, which is ten times faster than Java serialization. While Kryo serialization is used internally by Spark for simple data types, arrays, and strings, you still need to implement the Serializable interface even when using Kryo.

   ```scala
   object KryoSerializationExample {
     def main(args: Array[String]): Unit = {
       val conf: SparkConf = new SparkConf()
         .setAppName("SerDemo")
         .setMaster("local[*]")
         .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
         .registerKryoClasses(Array(classOf[Searcher]))
       val sc: SparkContext = new SparkContext(conf)
       val rdd: RDD[String] = sc.makeRDD(Array("hello world", "hello atguigu", "atguigu", "hahah"), 2)
       val searcher = new Searcher("hello")
       val result: RDD[String] = searcher.getMatchedRDD1(rdd)
       result.collect.foreach(println)
     }
   }

   case class Searcher(query: String) {
     def isMatch(s: String): Boolean = s.contains(query)
     def getMatchedRDD1(rdd: RDD[String]): RDD[String] = rdd.filter(isMatch)
     def getMatchedRDD2(rdd: RDD[String]): RDD[String] = {
       val q = query
       rdd.filter(_.contains(q))
     }
   }
   ```

### RDD Dependencies

1) **RDD Lineage**
   RDD only supports coarse-grained transformations, i.e., operations performed on large sets of records. It records the lineage of operations to reconstruct lost partitions. The lineage captures metadata and transformation behavior, allowing reconstruction of lost data partitions based on these records.

   ```scala
   val fileRDD: RDD[String] = sc.textFile("input/1.txt")
   println(fileRDD.toDebugString)
   println("----------------------")
   val wordRDD: RDD[String] = fileRDD.flatMap(_.split(" "))
   println(wordRDD.toDebugString)
   println("----------------------")
   val mapRDD: RDD[(String, Int)] = wordRDD.map((_,1))
   println(mapRDD.toDebugString)
   println("----------------------")
   val resultRDD: RDD[(String, Int)] = mapRDD.reduceByKey(_+_)
   println(resultRDD.toDebugString)
   resultRDD.collect()
   ```

2) **RDD Dependencies**
   Dependencies refer to the relationships between adjacent RDDs.
   ```scala
   val sc: SparkContext = new SparkContext(conf)
   val fileRDD: RDD[String] = sc.textFile("input/1.txt")
   println(fileRDD.dependencies)
   println("----------------------")
   val wordRDD: RDD[String] = fileRDD.flatMap(_.split(" "))
   println(wordRDD.dependencies)
   println("----------------------")
   val mapRDD: RDD[(String, Int)] = wordRDD.map((_,1))
   println(mapRDD.dependencies)
   println("----------------------")
   val resultRDD: RDD[(String, Int)] = mapRDD.reduceByKey(_+_)
   println(resultRDD.dependencies)
   resultRDD.collect()
   ```

3) **Narrow Dependencies**
   Narrow dependencies occur when each parent RDD partition is used by at most one child RDD partition.
   ```scala
   class OneToOneDependency[T](rdd: RDD[T]) extends NarrowDependency[T](rdd)
   ```

4) **Wide Dependencies**
   Wide dependencies occur when multiple child RDD partitions depend on a single parent RDD partition, causing a shuffle.
   ```scala
   class ShuffleDependency[K: ClassTag, V: ClassTag, C: ClassTag](
     @transient private val _rdd: RDD[_ <: Product2[K, V]],
     val partitioner: Partitioner,
     val serializer: Serializer = SparkEnv.get.serializer,
     val keyOrdering: Option[Ordering[K]] = None,
     val aggregator: Option[Aggregator[K, V, C]] = None,
     val mapSideCombine: Boolean = false) extends Dependency[Product2[K, V]]
   ```

5) **RDD Stage Division**
   DAG (Directed Acyclic Graph) is a directed graph without cycles, used to represent the stages of RDD computations.

6) **RDD Stage Division Source Code**
   ```scala
   try {
     finalStage = createResultStage(finalRDD, func, partitions, jobId, callSite)
   } catch {
     case e: Exception =>
       logWarning("Creating new stage failed due to exception - job: " + jobId, e)
       listener.jobFailed(e)
       return
   }

   private def createResultStage(
     rdd: RDD[_],
     func: (TaskContext, Iterator[_]) => _,
     partitions: Array[Int],
     jobId: Int,
     callSite: CallSite): ResultStage = {
     val parents = getOrCreateParentStages(rdd, jobId)
     val id = nextStageId.getAndIncrement()
     val stage = new ResultStage(id, rdd, func, partitions, parents, jobId, callSite)
     stageIdToStage(id) = stage
     updateJobIdStageIdMaps(jobId, stage)
     stage
   }

   private def getOrCreateParentStages(rdd: RDD[_], firstJobId: Int): List[Stage] = {
     getShuffleDependencies(rdd).map { shuffleDep =>
       getOrCreateShuffleMapStage(shuffleDep, firstJobId)
     }.toList
   }

   private[scheduler] def getShuffleDependencies(rdd: RDD[_]): HashSet[ShuffleDependency[_, _, _]] = {
     val parents = new HashSet[ShuffleDependency[_, _, _]]
     val visited = new HashSet[RDD[_]]
     val waitingForVisit = new Stack[RDD[_]]
     waitingForVisit.push(rdd)
     while (waitingForVisit.nonEmpty) {
       val toVisit = waitingForVisit.pop()
       if (!visited(toVisit)) {
         visited += toVisit
         toVisit.dependencies.foreach {
           case shuffleDep: ShuffleDependency[_, _, _] =>
             parents += shuffleDep
           case dependency =>
             waitingForVisit.push(dependency.rdd)
         }
       }
     }
     parents
   }
   ```

7) **RDD Task Division**
   Tasks in RDD are divided into Application, Job, Stage, and Task.
   - Application: Initiating a SparkContext creates an Application.
   - Job: An Action operator generates a Job.
   - Stage: The number of wide dependencies (ShuffleDependencies) plus one.
   - Task: The number of partitions in the final RDD of a stage equals the number of tasks.

   ```scala
   val tasks: Seq[Task[_]] = try {
     stage match {
       case stage: ShuffleMapStage =>
         partitionsToCompute.map { id =>
           val locs = taskIdToLocations(id)
           val part = stage.rdd.partitions(id)
           new ShuffleMapTask(stage.id, stage.latestInfo.attemptId, taskBinary, part, locs, stage.latestInfo.taskMetrics, properties, Option(jobId), Option(sc.applicationId), sc.applicationAttemptId)
         }
       case stage: ResultStage =>
         partitionsToCompute.map { id =>
           val p: Int = stage.partitions(id)
           val part = stage.rdd.partitions(p)
           val locs = taskIdToLocations(id)
           new ResultTask(stage.id, stage.latestInfo.attemptId, taskBinary, part, locs, id, properties, stage.latestInfo.taskMetrics, Option(jobId), Option(sc.applicationId), sc.applicationAttemptId)
         }
     }

    

 val partitionsToCompute: Seq[Int] = stage.findMissingPartitions()

     override def findMissingPartitions(): Seq[Int] = {
       mapOutputTrackerMaster.findMissingPartitions(shuffleDep.shuffleId).getOrElse(0 until numPartitions)
     }
   }
   ```

### RDD Persistence

1) **RDD Cache**
   RDDs can cache or persist results to avoid recomputation. By default, data is cached in the JVM heap memory. This caching occurs when an action operator is triggered, storing the RDD in memory for reuse.
   ```scala
   println(wordToOneRdd.toDebugString)
   wordToOneRdd.cache()
   // Change storage level
   // mapRdd.persist(StorageLevel.MEMORY_AND_DISK_2)
   ```

   **Storage Levels:**
   ```scala
   object StorageLevel {
     val NONE = new StorageLevel(false, false, false, false)
     val DISK_ONLY = new StorageLevel(true, false, false, false)
     val DISK_ONLY_2 = new StorageLevel(true, false, false, false, 2)
     val MEMORY_ONLY = new StorageLevel(false, true, false, true)
     val MEMORY_ONLY_2 = new StorageLevel(false, true, false, true, 2)
     val MEMORY_ONLY_SER = new StorageLevel(false, true, false, false)
     val MEMORY_ONLY_SER_2 = new StorageLevel(false, true, false, false, 2)
     val MEMORY_AND_DISK = new StorageLevel(true, true, false, true)
     val MEMORY_AND_DISK_2 = new StorageLevel(true, true, false, true, 2)
     val MEMORY_AND_DISK_SER = new StorageLevel(true, true, false, false)
     val MEMORY_AND_DISK_SER_2 = new StorageLevel(true, true, false, false, 2)
     val OFF_HEAP = new StorageLevel(true, true, true, false, 1)
   }
   ```

   Cache data may be lost or evicted from memory if memory is insufficient. RDD's fault tolerance ensures correct computation even if cached data is lost, by recomputing lost partitions. While Spark automatically persists some shuffle data, it's recommended to use `persist` or `cache` for reusable data.

2) **RDD Checkpoint**
   Checkpointing writes intermediate results to disk, breaking the lineage to reduce fault tolerance costs. After checkpointing, if a node fails, it can recompute data from the checkpoint. Checkpointing is triggered by an Action operation.

   ```scala
   sc.setCheckpointDir("./checkpoint1")
   val lineRdd: RDD[String] = sc.textFile("input/1.txt")
   val wordRdd: RDD[String] = lineRdd.flatMap(_.split(" "))
   val wordToOneRdd: RDD[(String, Long)] = wordRdd.map(word => (word, System.currentTimeMillis()))
   wordToOneRdd.cache()
   wordToOneRdd.checkpoint()
   wordToOneRdd.collect().foreach(println)
   ```

3) **Cache vs Checkpoint**
   - Cache stores data without breaking lineage; Checkpoint breaks lineage.
   - Cache data is stored in memory/disk with low reliability; Checkpoint data is stored in fault-tolerant file systems like HDFS.
   - It's recommended to cache checkpointed RDDs to avoid recomputation.

### RDD Partitioners

Spark supports HashPartitioner and RangePartitioner, and allows custom partitioners. HashPartitioner is the default.

- Only Key-Value RDDs have partitioners; non Key-Value RDDs have None as their partitioner.
- Each RDD's partition ID ranges from 0 to (numPartitions - 1), determining which partition a value belongs to.

1) **HashPartitioner**
   Computes the hashCode of the key and mods by the number of partitions.
   ```scala
   class HashPartitioner(partitions: Int) extends Partitioner {
     require(partitions >= 0, s"Number of partitions ($partitions) cannot be negative.")
     def numPartitions: Int = partitions
     def getPartition(key: Any): Int = key match {
       case null => 0
       case _ => Utils.nonNegativeMod(key.hashCode, numPartitions)
     }
     override def equals(other: Any): Boolean = other match {
       case h: HashPartitioner => h.numPartitions == numPartitions
       case _ => false
     }
     override def hashCode: Int = numPartitions
   }
   ```

2) **RangePartitioner**
   Maps data within a range to a partition, aiming for even distribution and ordered partitions.
   ```scala
   class RangePartitioner[K: Ordering: ClassTag, V](
     partitions: Int,
     rdd: RDD[_ <: Product2[K, V]],
     private var ascending: Boolean = true) extends Partitioner {
     require(partitions >= 0, s"Number of partitions cannot be negative but found $partitions.")
     private var ordering = implicitly[Ordering[K]]
     private var rangeBounds: Array[K] = { ... }
     def numPartitions: Int = rangeBounds.length + 1
     private var binarySearch: ((Array[K], K) => Int) = CollectionsUtils.makeBinarySearch[K]

     def getPartition(key: Any): Int = {
       val k = key.asInstanceOf[K]
       var partition = 0
       if (rangeBounds.length <= 128) {
         while (partition < rangeBounds.length && ordering.gt(k, rangeBounds(partition))) {
           partition += 1
         }
       } else {
         partition = binarySearch(rangeBounds, k)
         if (partition < 0) partition = -partition - 1
         if (partition > rangeBounds.length) partition = rangeBounds.length
       }
       if (ascending) partition else rangeBounds.length - partition
     }
     override def equals(other: Any): Boolean = { ... }
     override def hashCode(): Int = { ... }
     @throws(classOf[IOException])
     private def writeObject(out: ObjectOutputStream): Unit = Utils.tryOrIOException { ... }
     @throws(classOf[IOException])
     private def readObject(in: ObjectInputStream): Unit = Utils.tryOrIOException { ... }
   }
   ```