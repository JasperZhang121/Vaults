### ShuffleMapStage and ResultStage

When dividing stages, the last stage is called the finalStage, which is essentially a `ResultStage` object. All preceding stages are referred to as `ShuffleMapStage`. The completion of a `ShuffleMapStage` is accompanied by the writing of shuffle files to disk.

A `ResultStage` generally corresponds to the action operator in the code, meaning it applies a function to the dataset of each partition in the RDD, indicating the end of a job's execution.

### HashShuffle Analysis

#### Unoptimized HashShuffle

Let's clarify an assumption: each Executor has only one CPU core, meaning that regardless of how many task threads are allocated to an Executor, only one task thread can execute at a time.

In the diagram below, there are three Reducers. Each Task calculates its hash (partitioner: hash/numReduce) and categorizes the data into three different categories. Each Task divides its data into three categories, and Reducers collect data from each Task that belongs to their respective categories, aggregating them into a large collection of the same category. Each Task outputs three local files. With four Mapper Tasks, a total of 4 Tasks x 3 categorized files = 12 local small files are produced.

![hashShuffle](hashShuffle.png)

#### Optimized HashShuffle

The optimized HashShuffle process introduces a consolidation mechanism, which reuses buffers. This optimization is enabled by setting `spark.shuffle.consolidateFiles` to true (default is false). It is generally recommended to enable this option when using `HashShuffleManager`.

There are still four Tasks, with data categorized into three types by the hash algorithm based on their keys. Regardless of how many tasks are in the same process, the same key will be placed in the same buffer. The data in the buffer is then written to local files based on the number of cores (one core holds data of only one type of key). In each process with one Task, three local files are written. With four Mapper Tasks, the total output is 2 Cores x 3 categorized files = 6 local small files.

![optimizedHashShuffle](optimizedHashShuffle.png)

### SortShuffle Analysis

#### Regular SortShuffle

In this mode, data is first written to a data structure. For `reduceByKey`, data is written to a Map, where it undergoes local aggregation while being written to memory. For `Join` operations, data is written to an ArrayList and directly to memory. If the threshold is reached, the data in the memory structure is written to disk, and the memory structure is cleared.

Before spilling to disk, data is sorted by key. The sorted data is then written to disk files in batches, with the default batch size being 10,000 entries. Data is written to disk files using buffer overflow, where each overflow creates a disk file. This means a single Task process can generate multiple temporary files.

Finally, in each Task, all temporary files are merged in the merge process. This involves reading all temporary files and writing them once to the final file, meaning all data for a Task is in one file. An index file is also written separately, indicating the start and end offsets of each Task's data in the file.

![sortShuffle](sortShuffle.png)

#### Bypass SortShuffle

The conditions for triggering the bypass mechanism are as follows:

1. The number of shuffle reduce tasks is less than or equal to the value of the `spark.shuffle.sort.bypassMergeThreshold` parameter, which defaults to 200.
2. The shuffle operator is not of an aggregation type (e.g., `reduceByKey`).

In this case, the Task creates a temporary disk file for each reduce-side Task, hashes the data by key, and writes the key to the corresponding disk file based on the hash value. Data is first written to a memory buffer, which overflows to disk when full. Finally, all temporary disk files are merged into one disk file, and a separate index file is created.

This disk writing mechanism is identical to the unoptimized `HashShuffleManager` since it creates a large number of disk files, with the only difference being the final merge of disk files. The fewer final disk files in this mechanism improve shuffle read performance compared to the unoptimized `HashShuffleManager`. The main distinction from the regular `SortShuffleManager` is the lack of sorting, eliminating the performance overhead of sorting during shuffle write.

![bypassShuffle](bypassShuffle.png)

This detailed analysis of Spark's shuffle mechanisms covers both HashShuffle and SortShuffle, including their optimized and unoptimized versions, providing a comprehensive understanding of how data shuffling works in Spark.