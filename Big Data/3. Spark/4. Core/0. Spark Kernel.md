### Overview

The Spark kernel generally refers to the core operating mechanism of Spark, including the running mechanism of Spark's core components, Spark's task scheduling mechanism, Spark's memory management mechanism, and the operating principles of Spark's core functions. Mastering the principles of the Spark kernel can help us better design Spark code and accurately identify the root cause of problems that occur during the execution of projects.

#### Core Components

##### Driver

The Spark driver node is used to execute the main method in Spark tasks and is responsible for the actual execution of the code.

The Driver is primarily responsible for the following during the execution of a Spark job:

1. Converting user programs into jobs (Jobs).
2. Scheduling tasks (Tasks) among Executors.
3. Tracking the execution status of Executors.
4. Displaying query execution status through the UI.

##### Executor

The Spark Executor object is responsible for running specific tasks in a Spark job, with each task being independent of others. When a Spark application starts, the ExecutorBackend node is launched simultaneously and remains throughout the lifecycle of the Spark application. If an ExecutorBackend node fails or crashes, the Spark application can continue executing by rescheduling the tasks on the failed node to other Executor nodes.

Executors have two core functions:

1. Running the tasks that make up a Spark application and returning the results to the Driver.
2. Providing in-memory storage for RDDs that are requested to be cached by the user program through their own Block Manager. RDDs are directly cached within the Executor process, allowing tasks to utilize cached data to speed up computation during runtime.

#### General Execution Process

The core steps of the basic submission process for deploying a Spark application are as follows:

1. After submitting a task, the Driver program starts first.
2. The Driver then registers the application with the cluster manager.
3. Subsequently, the cluster manager allocates and starts Executors based on the configuration file of the task.
4. The Driver begins executing the main function. Spark queries are lazily executed, and when an Action operator is executed, a reverse calculation begins, dividing Stages based on wide dependencies. Each Stage corresponds to a TaskSet, which contains multiple Tasks, and the available Executor resources are located and scheduled.
5. Following the principle of data locality, Tasks are distributed to specified Executors for execution. During task execution, Executors continuously communicate with the Driver, reporting the execution status of the tasks.
