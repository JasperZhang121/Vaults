### Task Scheduling Mechanism

In production environments, Spark clusters are generally deployed in YARN-Cluster mode. For the subsequent kernel analysis, we will assume the cluster is deployed in YARN-Cluster mode. The Driver thread primarily initializes the `SparkContext` object, prepares the necessary context for operation, maintains an RPC connection with the `ApplicationMaster` to request resources, and starts scheduling tasks based on user business logic, dispatching tasks to available Executors.

When the ResourceManager returns Container resources to the `ApplicationMaster`, the `ApplicationMaster` attempts to start the Executor processes in the corresponding Containers. Once the Executor process is up, it registers back to the Driver. After successful registration, it maintains a heartbeat with the Driver while waiting for the Driver to dispatch tasks. After the dispatched tasks are executed, the task status is reported back to the Driver.

### Overview

When the Driver starts, it prepares tasks according to the user program logic and gradually distributes tasks based on the available Executor resources. Before detailing task scheduling, let's explain a few concepts in Spark: a Spark application includes Job, Stage, and Task.

1. **Job**: Triggered by an Action method, a new Job is created whenever an Action method is encountered.
2. **Stage**: A subset of a Job, divided by RDD wide dependencies (i.e., Shuffle). A new Stage is created whenever a Shuffle is encountered.
3. **Task**: A subset of a Stage, measured by parallelism (number of partitions). The number of partitions determines the number of tasks.

Spark's task scheduling is divided into two main paths: Stage-level scheduling and Task-level scheduling. The overall scheduling process is illustrated below:

RDDs in Spark form a lineage (dependency) graph through their transformations, creating a DAG. An Action triggers a Job and schedules its execution, leading to the creation of two schedulers: `DAGScheduler` and `TaskScheduler`.

- **DAGScheduler**: Responsible for Stage-level scheduling, splitting Jobs into Stages and packaging each Stage into a TaskSet, which is then passed to `TaskScheduler`.
- **TaskScheduler**: Responsible for Task-level scheduling, distributing the TaskSet from `DAGScheduler` to Executors according to a specified scheduling strategy. The `SchedulerBackend` provides available resources, with multiple implementations interfacing with different resource management systems.

During the initialization of the `SparkContext`, `DAGScheduler`, `TaskScheduler`, `SchedulerBackend`, and `HeartbeatReceiver` are initialized, and the `SchedulerBackend` and `HeartbeatReceiver` are started.

The `SchedulerBackend` requests resources through the `ApplicationMaster` and continuously obtains suitable tasks from the `TaskScheduler` to dispatch to Executors. The `HeartbeatReceiver` receives heartbeat information from Executors, monitors their status, and notifies the `TaskScheduler`.

### Stage-Level Scheduling

Spark's task scheduling starts with DAG splitting, primarily handled by the `DAGScheduler`. When an Action operation is encountered, it triggers the calculation of a Job and submits it to the `DAGScheduler`. The following diagram illustrates the Job submission process:

1. **Job**: Consists of the final RDD and the Action method.
2. **SparkContext**: Submits the Job to the `DAGScheduler`, which splits the Job into several Stages based on the RDD's lineage graph (DAG). The split strategy is to trace back from the final RDD through dependencies, identifying wide dependencies (i.e., Shuffles) and splitting Stages accordingly. Narrow dependencies are included in the same Stage for pipelined computation. There are two types of Stages:
    - **ResultStage**: The final Stage in the DAG, determined by the Action method.
    - **ShuffleMapStage**: Prepares data for downstream Stages. 

![shuffleMap](shuffleMap.png)

The Job triggered by `saveAsTextFile` consists of `RDD-3` and the `saveAsTextFile` method. Based on the dependencies between RDDs, the process starts from `RDD-3` and traces back to `RDD-0`, splitting Stages at wide dependencies. `RDD-3` is included in the final Stage (ResultStage), while `RDD-0`, `RDD-1`, and `RDD-2` (narrow dependencies) are included in the same Stage for pipelined computation. This forms the `ShuffleMapStage`.

The submission of a Stage depends on the execution of its parent Stage(s). A Stage is submitted only after its parent Stage(s) have completed execution. The Task information (partition and method details) is serialized and packaged into a TaskSet, which is then submitted to the `TaskScheduler`. The `TaskScheduler` monitors the Stage's execution status, rescheduling failed Stages if necessary (due to Executor loss or Fetch failures). Other types of Task failures are retried within the `TaskScheduler`.

### Task-Level Scheduling

The `TaskScheduler` handles Task-level scheduling. The `DAGScheduler` packages Stages into TaskSets and submits them to the `TaskScheduler`, which encapsulates them into `TaskSetManager` and adds them to the scheduling queue. The structure of `TaskSetManager` is illustrated below.

The `TaskSetManager` monitors and manages Tasks within the same Stage. The `TaskScheduler` schedules tasks based on `TaskSetManager`.

Upon initialization, the `TaskScheduler` starts the `SchedulerBackend`, which interacts with external systems, receives Executor registration information, and maintains Executor status. The `SchedulerBackend` periodically queries the `TaskScheduler` for tasks to run, following a process illustrated below:

![taskManager](taskManager.png)

After adding the `TaskSetManager` to the `rootPool` scheduling pool, the `reviveOffers` method of `SchedulerBackend` sends a `ReviveOffer` message to `driverEndpoint`. Upon receiving this message, `driverEndpoint` calls the `makeOffers` method, filtering active Executors and encapsulating them into `WorkerOffer` objects. With these computing resources prepared, the `TaskScheduler` assigns tasks to Executors based on these resources through the `resourceOffer` method.

### Scheduling Policies

The `TaskScheduler` supports two scheduling policies: FIFO (default) and FAIR. During initialization, the `TaskScheduler` instantiates `rootPool`, representing the root node of the tree (of type `Pool`).

1. **FIFO Scheduling Policy**:
    If FIFO scheduling is used, `TaskSetManager` is queued in a first-come-first-served manner. The tree structure is shown below, with `TaskSetManager` stored in a FIFO queue.

2. **FAIR Scheduling Policy**:
    ![fair](fair.png)
    In FAIR mode, there is a `rootPool` and multiple sub-pools, each containing `TaskSetManager` instances awaiting allocation. 

In FAIR mode, sub-pools are sorted first, followed by sorting `TaskSetManager` within sub-pools. Both Pool and `TaskSetManager` inherit the `Schedulable` trait, allowing the same sorting algorithm to be used. The sorting process compares `runningTasks`, `minShare`, and `weight` values, following these rules:

1. If `A.runningTasks` > `A.minShare` and `B.runningTasks` < `B.minShare`, `B` precedes `A`.
2. If both `A` and `B` have `runningTasks` < `minShare`, the ratio `runningTasks / minShare` (minShare usage rate) is compared, with the lower value preceding.
3. If both `A` and `B` have `runningTasks` > `minShare`, the ratio `runningTasks / weight` (weight usage rate) is compared, with the lower value preceding.
4. If all comparisons are equal, names are compared.

After sorting in FAIR mode, all `TaskSetManager` instances are placed in an `ArrayBuffer` and sequentially sent to Executors for execution. The `TaskSetManager` encapsulates all Tasks within a Stage and manages their scheduling. It retrieves tasks one by one based on certain rules and provides them to the `TaskScheduler`, which then passes them to the `SchedulerBackend` for execution.

### Locality Scheduling

The `DAGScheduler` slices Jobs into Stages, submitting them via `submitStage` that calls `submitMissingTasks`, which determines the preferred locations for each Task. By calling `getPreferredLocations()`, the preferred location for a partition (and thus the Task) is obtained. The locality levels are:

| Name          | Description |
|---------------|-------------|
| PROCESS_LOCAL | Task and data are in the same Executor, offering the best performance. |
| NODE_LOCAL    | Task and data are in the same node but different Executors, requiring inter-process communication. |
| RACK_LOCAL    | Task and data are on different nodes within the same rack, requiring network transmission. |
| NO_PREF       | Task has no preference for data location. |
| ANY           | Task and data can be anywhere in the cluster, with the worst performance. |

During scheduling, Spark always tries to launch each task at the highest locality level. If a task fails to start due to lack of resources at the current locality level, it waits for a certain duration before attempting again at the same level. If it exceeds the timeout, it downgrades to the next locality level.

Increasing the maximum tolerable delay time for each locality level can improve performance by allowing Executors to free up resources and execute the task.

### Failure Retries and Blacklisting Mechanism

In addition to selecting appropriate tasks for scheduling, monitoring task execution status is crucial. The `SchedulerBackend` interacts with external systems. Once a task is submitted to an Executor for execution, the Executor reports its execution status to the `SchedulerBackend`, which informs the `TaskScheduler`. The `TaskScheduler` identifies the `TaskSetManager` corresponding to the

 task and notifies it of the task's success or failure. Failed tasks are retried if they have not exceeded the maximum retry count; otherwise, the entire application fails.

The failure count records the Executor ID and Host of the last failure. When rescheduling the task, the blacklisting mechanism avoids scheduling it on the previously failed node, enhancing fault tolerance. The blacklist includes the Executor ID, Host, and the "blacklist" duration, during which the task is not rescheduled on that node.