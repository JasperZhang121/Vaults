### Output operations

Output operations specify the actions to be performed on the data after it has been transformed through various operations (e.g., pushing the results to an external database or outputting to the screen). Similar to the lazy evaluation of RDDs, if a DStream and its derived DStreams do not execute an output operation, they will not be evaluated. If no output operation is set in the StreamingContext, the entire context will not start.
The output operations are as follows:

- **print()**:

    Prints the first 10 elements of each batch of the DStream on the driver node running the streaming application. This is used for development and debugging. In the Python API, the same operation is called `print()`.
- **saveAsTextFiles(prefix, [suffix])**:

    Saves the content of the DStream as text files. Each batch's storage file name is based on the prefix and suffix parameters, forming "prefix-Time_IN_MS[.suffix]".
- **saveAsObjectFiles(prefix, [suffix])**:

    Saves the data in the DStream as SequenceFiles in a Java object serialization format. Each batch's storage file name is based on the prefix and suffix parameters, forming "prefix-TIME_IN_MS[.suffix]". Currently unavailable in Python.
- **saveAsHadoopFiles(prefix, [suffix])**:

    Saves the data in the DStream as Hadoop files. Each batch's storage file name is based on the prefix and suffix parameters, forming "prefix-TIME_IN_MS[.suffix]". Currently unavailable in Python.
- **foreachRDD(func)**:

    The most general output operation that applies a function `func` to each RDD generated from the stream. The function `func` should push the data in each RDD to an external system, such as saving the RDD to a file or writing it to a database over the network.

The general output operation `foreachRDD()` is used to run arbitrary computations on the RDDs in a DStream. This is similar to `transform()`, as it allows us to access any RDD. In `foreachRDD()`, you can reuse all the action operations available in Spark. For instance, a common use case is writing data to an external database like MySQL.
Notes:

1. Connections should not be written at the driver level (serialization issues).
2. If written inside foreach, a connection will be created for each data item in every RDD, which is inefficient.
3. Use `foreachPartition` to create connections within each partition (improving efficiency).

### Graceful Shutdown

Streaming tasks need to run 24/7, but sometimes you need to stop the program for code upgrades. Since it is a distributed program, killing each process individually is impractical, so configuring a graceful shutdown becomes crucial. Use an external file system to control the internal program shutdown.

- **MonitorStop**

```java
import java.net.URI;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.{FileSystem, Path};
import org.apache.spark.streaming.{StreamingContext, StreamingContextState};

class MonitorStop(ssc: StreamingContext) extends Runnable {
    override def run(): Unit = {
        val fs: FileSystem = FileSystem.get(new URI("hdfs://linux1:9000"), new Configuration(), "atguigu");

        while (true) {
            try {
                Thread.sleep(5000);
            } catch {
                case e: InterruptedException => e.printStackTrace();
            }
            val state: StreamingContextState = ssc.getState;
            val bool: Boolean = fs.exists(new Path("hdfs://linux1:9000/stopSpark"));
            if (bool) {
                if (state == StreamingContextState.ACTIVE) {
                    ssc.stop(stopSparkContext = true, stopGracefully = true);
                    System.exit(0);
                }
            }
        }
    }
}
```

- **SparkTest**

```java
import org.apache.spark.SparkConf;
import org.apache.spark.streaming.dstream.{DStream, ReceiverInputDStream};
import org.apache.spark.streaming.{Seconds, StreamingContext};

object SparkTest {

    def createSSC(): StreamingContext = {
        val update: (Seq[Int], Option[Int]) => Some[Int] = (values: Seq[Int], status: Option[Int]) => {
            // Compute the sum of the current batch
            val sum: Int = values.sum;
            // Get the previous state
            val lastStatus: Int = status.getOrElse(0);
            Some(sum + lastStatus);
        }

        val sparkConf: SparkConf = new SparkConf().setMaster("local[4]").setAppName("SparkTest");
        // Set graceful shutdown
        sparkConf.set("spark.streaming.stopGracefullyOnShutdown", "true");

        val ssc = new StreamingContext(sparkConf, Seconds(5));
        ssc.checkpoint("./ck");
        val line: ReceiverInputDStream[String] = ssc.socketTextStream("linux1", 9999);
        val word: DStream[String] = line.flatMap(_.split(" "));
        val wordAndOne: DStream[(String, Int)] = word.map((_, 1));

        val wordAndCount: DStream[(String, Int)] = wordAndOne.updateStateByKey(update);
        wordAndCount.print();
        ssc;
    }

    def main(args: Array[String]): Unit = {
        val ssc: StreamingContext = StreamingContext.getActiveOrCreate("./ck", () => createSSC());
        new Thread(new MonitorStop(ssc)).start();
        ssc.start();
        ssc.awaitTermination();
    }
}
```
