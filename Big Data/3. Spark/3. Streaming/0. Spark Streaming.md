
### Streaming

Spark Streaming makes it easier to build scalable and fault-tolerant streaming applications.

Spark Streaming is used for processing streaming data. It supports <mark style="background: #FFB8EBA6;">various data input sources such as Kafka, Flume, Twitter, ZeroMQ, and simple TCP sockets</mark>. Once data is input, it can be processed using Spark's high-level operations like `map`, `reduce`, `join`, and `window`. The results can be stored in many places, including HDFS, databases, etc. Similar to how Spark operates on RDDs, Spark Streaming uses discretized streams (DStream) as its abstraction, representing a sequence of data over time. Internally, the data received in each time interval is stored as an RDD, and a DStream is a sequence of these RDDs (hence the term "discretized"). Simply put, a DStream is a wrapper around RDDs for real-time data processing.

#### Features of Spark Streaming
- **Ease of Use**
- **Fault Tolerance**
- **Seamless Integration into the Spark Ecosystem**

#### Spark Streaming Architecture


![[SparkStreaming.png]]
#### Backpressure Mechanism

In versions before Spark 1.5, users could limit the rate at which the Receiver receives data by setting a static configuration parameter `spark.streaming.receiver.maxRate`. While this could prevent memory overflow by matching the data receiving rate to the processing capacity, it introduced other issues. 

For example, if the data production rate from the producer exceeds `maxRate` and the cluster's processing capacity is also higher than `maxRate`, it could lead to underutilized resources. To better coordinate the data receiving rate and processing capacity, starting from version 1.5, Spark Streaming can dynamically control the data receiving rate to match the cluster's processing capacity.

The backpressure mechanism (Spark Streaming Backpressure) dynamically adjusts the Receiver's data receiving rate based on feedback from the `JobScheduler`. This is controlled by the property `spark.streaming.backpressure.enabled`, which is set to `false` by default.

### Getting Started with DStream

#### WordCount Example
- **Requirement**: Use the `netcat` tool to continuously send data to port 9999, and use Spark Streaming to read data from this port and count the occurrences of different words.

1. **Add Dependency**
    ```xml
    <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-streaming_2.12</artifactId>
      <version>3.0.0</version>
    </dependency>
    ```

2. **Write Code**
    ```scala
    object StreamWordCount {
      def main(args: Array[String]): Unit = {
        // 1. Initialize Spark configuration
        val sparkConf = new SparkConf().setMaster("local[*]").setAppName("StreamWordCount")
        
        // 2. Initialize SparkStreamingContext
        val ssc = new StreamingContext(sparkConf, Seconds(3))
        
        // 3. Create DStream by monitoring the port, the incoming data is a line of text
        val lineStreams = ssc.socketTextStream("localhost", 9999)
        
        // Split each line of data into words
        val wordStreams = lineStreams.flatMap(_.split(" "))
        
        // Map each word to a tuple (word, 1)
        val wordAndOneStreams = wordStreams.map((_, 1))
        
        // Reduce by key to count occurrences of each word
        val wordAndCountStreams = wordAndOneStreams.reduceByKey(_ + _)
        
        // Print the result
        wordAndCountStreams.print()
        
        // Start SparkStreamingContext
        ssc.start()
        ssc.awaitTermination()
      }
    }
    ```

3. **Run the Program and Send Data via `netcat`**

#### WordCount Analysis
Discretized Stream is the basic abstraction in Spark Streaming, representing continuous streams of data and the result data streams after various Spark operations.
![[netcat1.png]]
Internally, DStream is represented by a continuous series of RDDs. Each RDD contains data for a specific time interval. Data operations are also performed on a per-RDD basis.


![[DStream.png]]
The computation process is completed by the Spark Engine.


![[SparkStreamEngine.png]]

### Creating DStreamï¼šRDD Queue

#### Usage and Description
During testing, you can create a DStream using `ssc.queueStream(queueOfRDDs)`. Each RDD pushed into this queue is processed as a DStream.

#### Example
- **Requirement**: Continuously create a few RDDs, push them into a queue, and compute WordCount using Spark Streaming.

1. **Write Code**
    ```scala
    object RDDStream {
      def main(args: Array[String]): Unit = {
        // 1. Initialize Spark configuration
        val conf = new SparkConf().setMaster("local[*]").setAppName("RDDStream")
        
        // 2. Initialize SparkStreamingContext
        val ssc = new StreamingContext(conf, Seconds(4))
        
        // 3. Create RDD queue
        val rddQueue = new mutable.Queue[RDD[Int]]()
        
        // 4. Create QueueInputDStream
        val inputStream = ssc.queueStream(rddQueue, oneAtATime = false)
        
        // 5. Process RDD data in the queue
        val mappedStream = inputStream.map((_, 1))
        val reducedStream = mappedStream.reduceByKey(_ + _)
        
        // 6. Print the result
        reducedStream.print()
        
        // 7. Start the task
        ssc.start()
        
        // 8. Continuously create and push RDDs into the queue
        for (i <- 1 to 5) {
          rddQueue += ssc.sparkContext.makeRDD(1 to 300, 10)
          Thread.sleep(2000)
        }
        
        ssc.awaitTermination()
      }
    }
    ```

2. **Result Display**
    ```
    Time: 1539075280000 ms
    (4,60)
    (0,60)
    (6,60)
    (8,60)
    (2,60)
    (1,60)
    (3,60)
    (7,60)
    (9,60)
    (5,60)
    Time: 1539075284000 ms
    (4,60)
    (0,60)
    (6,60)
    (8,60)
    (2,60)
    (1,60)
    (3,60)
    (7,60)
    (9,60)
    (5,60)
    Time: 1539075288000 ms
    (4,30)
    (0,30)
    (6,30)
    (8,30)
    (2,30)
    (1,30)
    (3,30)
    (7,30)
    (9,30)
    (5,30)
    Time: 1539075292000 ms
    ```

### Custom Data Sources

#### Usage and Description

To create a custom data source, extend the `Receiver` class and implement the `onStart` and `onStop` methods to define the data collection process.

