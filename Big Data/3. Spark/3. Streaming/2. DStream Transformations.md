### DStream Transformations

Operations on DStream are similar to those on RDD and are divided into Transformations and Output Operations. Additionally, there are some special primitives in transformation operations, such as `updateStateByKey()`, `transform()`, and various window-related primitives.

#### Stateless Transformations
Stateless transformations <mark style="background: #FFB8EBA6;">apply simple RDD transformations to each batch</mark>, i.e., transforming each RDD within a DStream. Note that for key-value DStream transformations (such as `reduceByKey()`), you need to import `StreamingContext._` in Scala to use them. 

It's important to remember that even though these functions appear to act on the entire stream, each DStream internally consists of many RDDs (batches), and stateless transformations are applied to each RDD individually. For example, `reduceByKey()` will reduce data within each time interval but will not reduce data across different intervals.

#### Transform
The `transform` operation allows arbitrary RDD-to-RDD functions to be executed on DStreams. Even if these functions are not exposed in the DStream API, `transform` can conveniently extend the Spark API. This function is scheduled once per batch, essentially applying a transformation to the RDDs within the DStream.

```java
object Transform {

  def main(args: Array[String]): Unit = {

    // Create SparkConf
    val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("WordCount")

    // Create StreamingContext
    val ssc = new StreamingContext(sparkConf, Seconds(3))

    // Create DStream
    val lineDStream: ReceiverInputDStream[String] = ssc.socketTextStream("linux1", 9999)

    // RDD transformation
    val wordAndCountDStream: DStream[(String, Int)] = lineDStream.transform { rdd =>
      val words: RDD[String] = rdd.flatMap(_.split(" "))
      val wordAndOne: RDD[(String, Int)] = words.map((_, 1))
      val value: RDD[(String, Int)] = wordAndOne.reduceByKey(_ + _)
      value
    }

    // Print results
    wordAndCountDStream.print()

    // Start the computation
    ssc.start()
    ssc.awaitTermination()
  }
}
```

#### Join
Joining two streams requires the <mark style="background: #FFB86CA6;">batch sizes of both streams to be the same</mark> to trigger computation simultaneously. The join operation processes the RDDs in the current batch of both streams, similar to joining two RDDs.

```java
import org.apache.spark.SparkConf
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.streaming.dstream.{DStream, ReceiverInputDStream}

object JoinTest {

  def main(args: Array[String]): Unit = {

    // Create SparkConf
    val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("JoinTest")

    // Create StreamingContext
    val ssc = new StreamingContext(sparkConf, Seconds(5))

    // Create streams from ports
    val lineDStream1: ReceiverInputDStream[String] = ssc.socketTextStream("linux1", 9999)
    val lineDStream2: ReceiverInputDStream[String] = ssc.socketTextStream("linux2", 8888)

    // Convert both streams to key-value pairs
    val wordToOneDStream: DStream[(String, Int)] = lineDStream1.flatMap(_.split(" ")).map((_, 1))
    val wordToADStream: DStream[(String, String)] = lineDStream2.flatMap(_.split(" ")).map((_, "a"))

    // Perform join operation
    val joinDStream: DStream[(String, (Int, String))] = wordToOneDStream.join(wordToADStream)

    // Print results
    joinDStream.print()

    // Start the computation
    ssc.start()
    ssc.awaitTermination()
  }
}
```

### Stateful Transformations

#### UpdateStateByKey
The `updateStateByKey` primitive is used for maintaining state across batches in a DStream. Sometimes, we need to maintain state across batches (e.g., accumulating word count). The `updateStateByKey` function provides access to a state variable for key-value DStreams. Given a DStream of (key, event) pairs and a function specifying how to update the state based on new events, it can construct a new DStream with (key, state) pairs.

The `updateStateByKey` result will be a new DStream where each RDD contains (key, state) pairs corresponding to each time interval. This operation allows us to maintain arbitrary state while updating it with new information. To use this feature, the following steps are required:
1. Define the state, which can be any data type.
2. Define the state update function, which specifies how to update the state using the previous state and new values from the input stream.

Using `updateStateByKey` requires configuring a checkpoint directory to save the state.

1. **Code Example**:

```java
object WorldCount {

  def main(args: Array[String]): Unit = {

    // Define the update state method. The `values` parameter represents the word frequency in the current batch, and `state` represents the word frequency from previous batches.
    val updateFunc = (values: Seq[Int], state: Option[Int]) => {
      val currentCount = values.foldLeft(0)(_ + _)
      val previousCount = state.getOrElse(0)
      Some(currentCount + previousCount)
    }

    val conf = new SparkConf().setMaster("local[*]").setAppName("NetworkWordCount")
    val ssc = new StreamingContext(conf, Seconds(3))
    ssc.checkpoint("./ck")

    // Create a DStream that will connect to hostname:port, like hadoop102:9999
    val lines = ssc.socketTextStream("linux1", 9999)

    // Split each line into words
    val words = lines.flatMap(_.split(" "))

    // Count each word in each batch
    val pairs = words.map(word => (word, 1))

    // Use updateStateByKey to update state, counting the total number of words since the start of the run
    val stateDstream = pairs.updateStateByKey(updateFunc)
    stateDstream.print()

    // Start the computation
    ssc.start()
    ssc.awaitTermination()
  }
}
```

2. **Run the program and send data to port 9999**:

```
nc -lk 9999
Hello World
Hello Scala
```

3. **Result Display**:

```
Time: 1504685175000 ms

Time: 1504685181000 ms
(shi,1)
(shui,1)
(ni,1)

Time: 1504685187000 ms
(shi,1)
(ma,1)
(hao,1)
(shui,1)
```

#### Window Operations
Window Operations allow setting the window size and sliding interval to dynamically obtain the current state of the stream. All window-based operations require two parameters: window duration and sliding interval.
- **Window duration**: The time range for the computation.
- **Sliding interval**: The interval at which the computation is triggered.

Both parameters must be multiples of the batch interval size.

**WordCount Version 3**: 3-second batches, 12-second window, 6-second slide.

Window operations include the following methods:
1. `window(windowLength, slideInterval)`: Applies a window on the source DStream to compute a new DStream.
2. `countByWindow(windowLength, slideInterval)`: Returns a sliding window count of elements in the stream.
3. `reduceByWindow(func, windowLength, slideInterval)`: Applies a custom function to reduce elements within a sliding window to create a new single-element stream.
4. `reduceByKeyAndWindow(func, windowLength, slideInterval, [numTasks])`: When called on a (K,V) pair DStream, returns a new (K,V) pair DStream by applying the reduce function to values within a sliding window for each key.
5. `reduceByKeyAndWindow(func, invFunc, windowLength, slideInterval, [numTasks])`: A variant of the above function, where each window's reduce value is incrementally computed by using the previous window's reduce value, adding new data entering the window, and "subtracting" data leaving the window. This requires the reduce function to have an inverse reduce function.

Example code:

```java
val ipDStream = accessLogsDStream.map(logEntry => (logEntry.getIpAddress(), 1))
val ipCountDStream = ipDStream.reduceByKeyAndWindow(
  {(x, y) => x + y},
  {(x, y) => x - y},
  Seconds(30),
  Seconds(10)
)
```
