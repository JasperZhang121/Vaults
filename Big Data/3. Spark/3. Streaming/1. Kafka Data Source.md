### Version Selection

**Receiver API**: <mark style="background: #FFB8EBA6;">Requires a dedicated Executor to receive data</mark>, then forwards it to <mark style="background: #FFB86CA6;">other Executors for computation</mark>. The issue with this approach is that the speed of the receiving Executor and the computing Executors can differ. Particularly, when the receiving Executor's speed exceeds that of the computing Executors, it can lead to memory overflow in the computation nodes. This method was provided in earlier versions but is no longer suitable in current versions.

**Direct API**: The computing Executors actively consume data from Kafka, controlling the speed themselves.

### Kafka 0-8 Receiver Mode

1. **Requirement**: Read data from Kafka using Spark Streaming, perform simple calculations on the data, and print the results to the console.

2. **Dependencies**:

```xml
<dependency>
  <groupId>org.apache.spark</groupId>
  <artifactId>spark-streaming-kafka-0-8_2.11</artifactId>
  <version>2.4.5</version>
</dependency>
```

3. **Code Example**:

```java
package com.atguigu.kafka;

import org.apache.spark.SparkConf;
import org.apache.spark.streaming.dstream.ReceiverInputDStream;
import org.apache.spark.streaming.kafka.KafkaUtils;
import org.apache.spark.streaming.{Seconds, StreamingContext};

public class ReceiverAPI {
  public static void main(String[] args) {

    // 1. Create SparkConf
    SparkConf sparkConf = new SparkConf().setAppName("ReceiverWordCount").setMaster("local[*]");

    // 2. Create StreamingContext
    StreamingContext ssc = new StreamingContext(sparkConf, Seconds(3));

    // 3. Read Kafka data to create DStream (based on Receiver mode)
    ReceiverInputDStream<(String, String)> kafkaDStream = KafkaUtils.createStream(
        ssc,
        "linux1:2181,linux2:2181,linux3:2181", 
        "atguigu",
        Map<String, Integer>("atguigu" -> 1)
    );

    // 4. Compute WordCount
    kafkaDStream.map((key, value) -> (value, 1))
                .reduceByKey((a, b) -> a + b)
                .print();

    // 5. Start the task
    ssc.start();
    ssc.awaitTermination();
  }
}
```

### Kafka 0-10 Direct Mode

1. **Requirement**: Read data from Kafka using Spark Streaming, perform simple calculations on the data, and print the results to the console.

2. **Dependencies**:

```xml
<dependency>
  <groupId>org.apache.spark</groupId>
  <artifactId>spark-streaming-kafka-0-10_2.12</artifactId>
  <version>3.0.0</version>
</dependency>
<dependency>
  <groupId>com.fasterxml.jackson.core</groupId>
  <artifactId>jackson-core</artifactId>
  <version>2.10.1</version>
</dependency>
```

3. **Code Example**:

```java
import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.spark.SparkConf;
import org.apache.spark.streaming.dstream.DStream;
import org.apache.spark.streaming.dstream.InputDStream;
import org.apache.spark.streaming.kafka010.ConsumerStrategies;
import org.apache.spark.streaming.kafka010.KafkaUtils;
import org.apache.spark.streaming.kafka010.LocationStrategies;
import org.apache.spark.streaming.{Seconds, StreamingContext};

public class DirectAPI {
  public static void main(String[] args) {

    // 1. Create SparkConf
    SparkConf sparkConf = new SparkConf().setAppName("ReceiverWordCount").setMaster("local[*]");

    // 2. Create StreamingContext
    StreamingContext ssc = new StreamingContext(sparkConf, Seconds(3));

    // 3. Define Kafka parameters
    Map<String, Object> kafkaParams = new HashMap<>();
    kafkaParams.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "linux1:9092,linux2:9092,linux3:9092");
    kafkaParams.put(ConsumerConfig.GROUP_ID_CONFIG, "atguigu");
    kafkaParams.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
    kafkaParams.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");

    // 4. Read Kafka data to create DStream
    InputDStream<ConsumerRecord<String, String>> kafkaDStream = KafkaUtils.createDirectStream(
        ssc,
        LocationStrategies.PreferConsistent,
        ConsumerStrategies.Subscribe(Collections.singleton("atguigu"), kafkaParams)
    );

    // 5. Extract each message's value
    DStream<String> valueDStream = kafkaDStream.map(ConsumerRecord::value);

    // 6. Compute WordCount
    valueDStream.flatMap(value -> Arrays.asList(value.split(" ")).iterator())
                .mapToPair(word -> new Tuple2<>(word, 1))
                .reduceByKey((a, b) -> a + b)
                .print();

    // 7. Start the task
    ssc.start();
    ssc.awaitTermination();
  }
}
```

To check Kafka consumption progress:

```bash
bin/kafka-consumer-groups.sh --describe --bootstrap-server linux1:9092 --group name
```