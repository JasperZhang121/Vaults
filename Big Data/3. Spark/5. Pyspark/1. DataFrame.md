PySpark DataFrames are lazily evaluated. They are implemented on top of RDDs. When Spark transforms data, it does not immediately compute the transformation but plans how to compute later.

When actions such as `collect()` are explicitly called, the computation starts.

PySpark applications start with initializing SparkSession which is the entry point of PySpark as below.

```python
from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()
```

### DataFrame Creation
A PySpark DataFrame can be created via pyspark.sql. 

Create a PySpark DataFrame from a list of rows.

```python
from datetime import datetime, date
import pandas as pd
from pyspark.sql import Row

df = spark.createDataFrame([
    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),
    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),
    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))
])
df
```

Create a PySpark DataFrame with an explicit schema.

```python
df = spark.createDataFrame([
    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),
    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),
    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))
], schema='a long, b double, c string, d date, e timestamp')
df
```

Create a PySpark DataFrame from a pandas DataFrame

```python
pandas_df = pd.DataFrame({
    'a': [1, 2, 3],
    'b': [2., 3., 4.],
    'c': ['string1', 'string2', 'string3'],
    'd': [date(2000, 1, 1), date(2000, 2, 1), date(2000, 3, 1)],
    'e': [datetime(2000, 1, 1, 12, 0), datetime(2000, 1, 2, 12, 0), datetime(2000, 1, 3, 12, 0)]
})
df = spark.createDataFrame(pandas_df)
df
```
**The DataFrames created above all have the same results and schema.**

```python
# All DataFrames above result same.
df.show()
df.printSchema()
```

```
+---+---+-------+----------+-------------------+
|  a|  b|      c|         d|                  e|
+---+---+-------+----------+-------------------+
|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|
|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|
|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|
+---+---+-------+----------+-------------------+

root
 |-- a: long (nullable = true)
 |-- b: double (nullable = true)
 |-- c: string (nullable = true)
 |-- d: date (nullable = true)
 |-- e: timestamp (nullable = true)
```

### Viewing Data

Enable `spark.sql.repl.eagerEval.enabled` configuration for the eager evaluation of PySpark DataFrame in notebooks such as Jupyter.

```python
spark.conf.set('spark.sql.repl.eagerEval.enabled', True)
df
```

The rows can also be shown vertically.

```python
df.show(1, vertical=True)
```

```
-RECORD 0------------------
 a   | 1
 b   | 2.0
 c   | string1
 d   | 2000-01-01
 e   | 2000-01-01 12:00:00
only showing top 1 row
```

DataFrame’s schema
```python
df.columns
# ['a', 'b', 'c', 'd', 'e']

df.printSchema()
# root
#  |-- a: long (nullable = true)
#  |-- b: double (nullable = true)
#  |-- c: string (nullable = true)
#  |-- d: date (nullable = true)
#  |-- e: timestamp (nullable = true)

df.select("a", "b", "c").describe().show()
# +-------+---+---+-------+
# |summary|  a|  b|      c|
# +-------+---+---+-------+
# |  count|  3|  3|      3|
# |   mean|2.0|3.0|   null|
# | stddev|1.0|1.0|   null|
# |    min|  1|2.0|string1|
# |    max|  3|4.0|string3|
# +-------+---+---+-------+

df.collect() # avoid throwing an out-of-memory exception
# [Row(a=1, b=2.0, c='string1', d=datetime.date(2000, 1, 1), e=datetime.datetime(2000, 1, 1, 12, 0)),
#  Row(a=2, b=3.0, c='string2', d=datetime.date(2000, 2, 1), e=datetime.datetime(2000, 1, 2, 12, 0)),
#  Row(a=3, b=4.0, c='string3', d=datetime.date(2000, 3, 1), e=datetime.datetime(2000, 1, 3, 12, 0))]

df.take(1)
# [Row(a=1, b=2.0, c='string1', d=datetime.date(2000, 1, 1), e=datetime.datetime(2000, 1, 1, 12, 0))]

df.toPandas()
# conversion back to a pandas
```

### Selecting and Accessing Data¶

PySpark DataFrame is lazily evaluated and simply selecting a column does not trigger the computation but it returns a Column instance.
    
```python
df.a
# Column<b'a'>
```
Columns
```python
from pyspark.sql import Column
from pyspark.sql.functions import upper

type(df.c) == type(upper(df.c)) == type(df.c.isNull())
# True

df.select(df.c).show()
# +-------+
# |      c|
# +-------+
# |string1|
# |string2|
# |string3|
# +-------+
```

Assign new Column instance

```python
df.withColumn('upper_c', upper(df.c)).show()
# +---+---+-------+----------+-------------------+-------+
# |  a|  b|      c|         d|                  e|upper_c|
# +---+---+-------+----------+-------------------+-------+
# |  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|STRING1|
# |  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|STRING2|
# |  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|STRING3|
# +---+---+-------+----------+-------------------+-------+

df.filter(df.a == 1).show()
# +---+---+-------+----------+-------------------+
# |  a|  b|      c|         d|                  e|
# +---+---+-------+----------+-------------------+
# |  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|
# +---+---+-------+----------+-------------------+
```

### Applying a Function

PySpark supports various UDFs and APIs to allow users to execute Python native functions.

```python
import pandas as pd
from pyspark.sql.functions import pandas_udf

@pandas_udf('long')
def pandas_plus_one(series: pd.Series) -> pd.Series:
    # Simply plus one by using pandas Series.
    return series + 1

df.select(pandas_plus_one(df.a)).show()

# +------------------+
# |pandas_plus_one(a)|
# +------------------+
# |                 2|
# |                 3|
# |                 4|
# +------------------+

def pandas_filter_func(iterator):
    for pandas_df in iterator:
        yield pandas_df[pandas_df.a == 1]

df.mapInPandas(pandas_filter_func, schema=df.schema).show()

# +---+---+-------+----------+-------------------+
# |  a|  b|      c|         d|                  e|
# +---+---+-------+----------+-------------------+
# |  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|
# +---+---+-------+----------+-------------------+
```

### Grouping Data
split-apply-combine strategy

```python
df = spark.createDataFrame([
    ['red', 'banana', 1, 10], ['blue', 'banana', 2, 20], ['red', 'carrot', 3, 30],
    ['blue', 'grape', 4, 40], ['red', 'carrot', 5, 50], ['black', 'carrot', 6, 60],
    ['red', 'banana', 7, 70], ['red', 'grape', 8, 80]], schema=['color', 'fruit', 'v1', 'v2'])
df.show()
# +-----+------+---+---+
# |color| fruit| v1| v2|
# +-----+------+---+---+
# |  red|banana|  1| 10|
# | blue|banana|  2| 20|
# |  red|carrot|  3| 30|
# | blue| grape|  4| 40|
# |  red|carrot|  5| 50|
# |black|carrot|  6| 60|
# |  red|banana|  7| 70|
# |  red| grape|  8| 80|
# +-----+------+---+---+

df.groupby('color').avg().show()
# +-----+-------+-------+
# |color|avg(v1)|avg(v2)|
# +-----+-------+-------+
# |  red|    4.8|   48.0|
# |black|    6.0|   60.0|
# | blue|    3.0|   30.0|
# +-----+-------+-------+
```

Apply a Python native function against each group by using pandas API.

```python
def plus_mean(pandas_df):
    return pandas_df.assign(v1=pandas_df.v1 - pandas_df.v1.mean())

df.groupby('color').applyInPandas(plus_mean, schema=df.schema).show()

# +-----+------+---+---+
# |color| fruit| v1| v2|
# +-----+------+---+---+
# |  red|banana| -3| 10|
# |  red|carrot| -1| 30|
# |  red|carrot|  0| 50|
# |  red|banana|  2| 70|
# |  red| grape|  3| 80|
# |black|carrot|  0| 60|
# | blue|banana| -1| 20|
# | blue| grape|  1| 40|
# +-----+------+---+---+
```

Co-grouping and applying a function
```python
df1 = spark.createDataFrame(
    [(20000101, 1, 1.0), (20000101, 2, 2.0), (20000102, 1, 3.0), (20000102, 2, 4.0)],
    ('time', 'id', 'v1'))

df2 = spark.createDataFrame(
    [(20000101, 1, 'x'), (20000101, 2, 'y')],
    ('time', 'id', 'v2'))

def merge_ordered(l, r):
    return pd.merge_ordered(l, r)

df1.groupby('id').cogroup(df2.groupby('id')).applyInPandas(
    merge_ordered, schema='time int, id int, v1 double, v2 string').show()
```
```
+--------+---+---+---+
|    time| id| v1| v2|
+--------+---+---+---+
|20000101|  1|1.0|  x|
|20000102|  1|3.0|  x|
|20000101|  2|2.0|  y|
|20000102|  2|4.0|  y|
+--------+---+---+---+
```

### Getting Data In/Out

CSV

```python
df.write.csv('foo.csv', header=True)
spark.read.csv('foo.csv', header=True).show()
# +-----+------+---+---+
# |color| fruit| v1| v2|
# +-----+------+---+---+
# |  red|banana|  1| 10|
# | blue|banana|  2| 20|
# |  red|carrot|  3| 30|
# | blue| grape|  4| 40|
# |  red|carrot|  5| 50|
# |black|carrot|  6| 60|
# |  red|banana|  7| 70|
# |  red| grape|  8| 80|
# +-----+------+---+---+
```

Parquet

```python
df.write.parquet('bar.parquet')
spark.read.parquet('bar.parquet').show()
# +-----+------+---+---+
# |color| fruit| v1| v2|
# +-----+------+---+---+
# |  red|banana|  1| 10|
# | blue|banana|  2| 20|
# |  red|carrot|  3| 30|
# | blue| grape|  4| 40|
# |  red|carrot|  5| 50|
# |black|carrot|  6| 60|
# |  red|banana|  7| 70|
# |  red| grape|  8| 80|
# +-----+------+---+---+
```

ORC

```python
df.write.orc('zoo.orc')
spark.read.orc('zoo.orc').show()
# +-----+------+---+---+
# |color| fruit| v1| v2|
# +-----+------+---+---+
# |  red|banana|  1| 10|
# | blue|banana|  2| 20|
# |  red|carrot|  3| 30|
# | blue| grape|  4| 40|
# |  red|carrot|  5| 50|
# |black|carrot|  6| 60|
# |  red|banana|  7| 70|
# |  red| grape|  8| 80|
# +-----+------+---+---+
```

### Working with SQL

DataFrame and Spark SQL share the same execution engine so they can be interchangeably used seamlessly.

```python
df.createOrReplaceTempView("tableA")
spark.sql("SELECT count(*) from tableA").show()
# +--------+
# |count(1)|
# +--------+
# |       8|
# +--------+
```

UDFs can be registered and invoked in SQL out of the box

```python
@pandas_udf("integer")
def add_one(s: pd.Series) -> pd.Series:
    return s + 1

spark.udf.register("add_one", add_one)
spark.sql("SELECT add_one(v1) FROM tableA").show()
# +-----------+
# |add_one(v1)|
# +-----------+
# |          2|
# |          3|
# |          4|
# |          5|
# |          6|
# |          7|
# |          8|
# |          9|
# +-----------+
```

SQL expressions be mixed and used as PySpark columns

```python
from pyspark.sql.functions import expr

df.selectExpr('add_one(v1)').show()
df.select(expr('count(*)') > 0).show()
```

```
+-----------+
|add_one(v1)|
+-----------+
|          2|
|          3|
|          4|
|          5|
|          6|
|          7|
|          8|
|          9|
+-----------+

+--------------+
|(count(1) > 0)|
+--------------+
|          true|
+--------------+
```


