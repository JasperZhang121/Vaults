### Using PyPI

```shell
# Spark - The default distribution uses Hadoop 3.3 and Hive 2.3
pip install pyspark
# Spark SQL
pip install pyspark[sql]
# pandas API on Spark
pip install pyspark[pandas_on_spark] plotly  # to plot your data, you can install plotly together.
# Spark Connect
pip install pyspark[connect]
```

### PySpark with/without a specific Hadoop version

```shell
# manually choose the mirror for faster downloading
PYSPARK_RELEASE_MIRROR=http://mirror.apache-kr.org PYSPARK_HADOOP_VERSION=3 pip install

# use -v option in pip to track the installation and download status
PYSPARK_HADOOP_VERSION=3 pip install pyspark -v
```

### Using Conda


```shell
# create a new conda environment from your terminal and activate it
conda create -n pyspark_env
conda activate pyspark_env

# Spark
conda install -c conda-forge pyspark  # can also add "python=3.8 some_package [etc.]" here
```

Note that PySpark for conda is maintained separately by the community; while new versions generally get packaged quickly, the availability through conda(-forge) is not directly in sync with the PySpark release cycle.

### Manually Downloading

PySpark is included in the distributions available at the Apache Spark website. You can download a distribution you want from the site. 

```shell
tar xzvf spark-3.5.3-bin-hadoop3.tgz
```

Ensure the SPARK_HOME environment variable points to the directory where the tar file has been extracted. 

```shell
cd spark-3.5.3-bin-hadoop3
export SPARK_HOME=`pwd`
export PYTHONPATH=$(ZIPS=("$SPARK_HOME"/python/lib/.zip); IFS=:; echo "${ZIPS[]}"):$PYTHONPATH
```

### Installing from Source

```
https://spark.apache.org/docs/3.5.3/building-spark.html
```

### Dependencies

| Package                      | Supported version     | Note                                                                 |
|------------------------------|-----------------------|----------------------------------------------------------------------|
| py4j                          | >=0.10.9.7            | Required                                                             |
| pandas                        | >=1.0.5               | Required for pandas API on Spark and Spark Connect; Optional for Spark SQL |
| pyarrow                       | >=4.0.0,<13.0.0       | Required for pandas API on Spark and Spark Connect; Optional for Spark SQL |
| numpy                         | >=1.15                | Required for pandas API on Spark and MLLib DataFrame-based API; Optional for Spark SQL |
| grpcio                        | >=1.48,<1.57          | Required for Spark Connect                                            |
| grpcio-status                 | >=1.48,<1.57          | Required for Spark Connect                                            |
| googleapis-common-protos       | ==1.56.4              | Required for Spark Connect                                            |




