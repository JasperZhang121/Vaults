### User-Defined Functions

Users can add custom functions using the `spark.udf` functionality to achieve custom behavior.

#### UDF (User-Defined Function)
1. **Create DataFrame:**
    ```scala
    scala> val df = spark.read.json("data/user.json")
    df: org.apache.spark.sql.DataFrame = [age: bigint, username: string]
    ```

2. **Register UDF:**
    ```scala
    scala> spark.udf.register("addName", (x: String) => "Name:" + x)
    res9: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>, StringType, Some(List(StringType)))
    ```

3. **Create Temporary Table:**
    ```scala
    scala> df.createOrReplaceTempView("people")
    ```

4. **Apply UDF:**
    ```scala
    scala> spark.sql("SELECT addName(username), age FROM people").show()
    ```

#### UDAF (User-Defined Aggregate Function)
Both strongly typed `Dataset` and weakly typed `DataFrame` provide built-in aggregate functions such as `count()`, `countDistinct()`, `avg()`, `max()`, and `min()`. In addition, users can define their own custom aggregate functions by extending `UserDefinedAggregateFunction` for weakly typed UDAFs. From Spark 3.0, `UserDefinedAggregateFunction` is deprecated, and users are encouraged to use strongly typed `Aggregator`.

##### Example Use Case: Calculate Average Age

1. **Using RDD:**
    ```java
    val conf: SparkConf = new SparkConf().setAppName("app").setMaster("local[*]")
    val sc: SparkContext = new SparkContext(conf)
    val res: (Int, Int) = sc.makeRDD(List(("zhangsan", 20), ("lisi", 30), ("wangwu", 40))).map {
      case (name, age) => (age, 1)
    }.reduce {
      (t1, t2) => (t1._1 + t2._1, t1._2 + t2._2)
    }
    println(res._1 / res._2)
    // Close connection
    sc.stop()
    ```

2. **Using Accumulator:**
    ```java
    class MyAC extends AccumulatorV2[Int, Int] {
      var sum: Int = 0
      var count: Int = 0

      override def isZero: Boolean = sum == 0 && count == 0

      override def copy(): AccumulatorV2[Int, Int] = {
        val newMyAC = new MyAC
        newMyAC.sum = this.sum
        newMyAC.count = this.count
        newMyAC
      }

      override def reset(): Unit = {
        sum = 0
        count = 0
      }

      override def add(v: Int): Unit = {
        sum += v
        count += 1
      }

      override def merge(other: AccumulatorV2[Int, Int]): Unit = {
        other match {
          case o: MyAC => {
            sum += o.sum
            count += o.count
          }
          case _ =>
        }
      }

      override def value: Int = sum / count
    }
    ```

3. **Using UDAF - Weakly Typed:**
    ```java
    // Define a class extending UserDefinedAggregateFunction and override its methods
    class MyAverageUDAF extends UserDefinedAggregateFunction {
      // Data type of the input parameter for the aggregate function
      def inputSchema: StructType = StructType(Array(StructField("age", IntegerType)))

      // Data type of the buffer schema (sum, count)
      def bufferSchema: StructType = StructType(Array(StructField("sum", LongType), StructField("count", LongType)))

      // Data type of the return value
      def dataType: DataType = DoubleType

      // Whether this function is deterministic (returns the same output for the same input)
      def deterministic: Boolean = true

      // Initialize the buffer
      def initialize(buffer: MutableAggregationBuffer): Unit = {
        buffer(0) = 0L // Sum of ages
        buffer(1) = 0L // Count of ages
      }

      // Update the buffer with an input row
      def update(buffer: MutableAggregationBuffer, input: Row): Unit = {
        if (!input.isNullAt(0)) {
          buffer(0) = buffer.getLong(0) + input.getInt(0)
          buffer(1) = buffer.getLong(1) + 1
        }
      }

      // Merge two buffers
      def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = {
        buffer1(0) = buffer1.getLong(0) + buffer2.getLong(0)
        buffer1(1) = buffer1.getLong(1) + buffer2.getLong(1)
      }

      // Calculate the final result
      def evaluate(buffer: Row): Double = buffer.getLong(0).toDouble / buffer.getLong(1)
    }

    // Create and register the UDAF
    val myAverage = new MyAverageUDAF
    spark.udf.register("avgAge", myAverage)
    spark.sql("SELECT avgAge(age) FROM user").show()
    ```

4. **Using UDAF - Strongly Typed:**
    ```java
    // Input data type
    case class User01(username: String, age: Long)

    // Buffer schema
    case class AgeBuffer(var sum: Long, var count: Long)

    // Define a class extending org.apache.spark.sql.expressions.Aggregator and override its methods
    class MyAverageUDAF1 extends Aggregator[User01, AgeBuffer, Double] {
      override def zero: AgeBuffer = AgeBuffer(0L, 0L)

      override def reduce(b: AgeBuffer, a: User01): AgeBuffer = {
        b.sum += a.age
        b.count += 1
        b
      }

      override def merge(b1: AgeBuffer, b2: AgeBuffer): AgeBuffer = {
        b1.sum += b2.sum
        b1.count += b2.count
        b1
      }

      override def finish(buff: AgeBuffer): Double = buff.sum.toDouble / buff.count

      // Encoders for serialization, fixed format
      override def bufferEncoder: Encoder[AgeBuffer] = Encoders.product

      override def outputEncoder: Encoder[Double] = Encoders.scalaDouble
    }

    // Create and register the UDAF
    val myAverageUDAF1 = new MyAverageUDAF1
    val col: TypedColumn[User01, Double] = myAverageUDAF1.toColumn

    // Create Dataset and perform aggregation
    val ds: Dataset[User01] = df.as[User01]
    ds.select(col).show()

    // In Spark 3.0, use strong typed Aggregator instead of UserDefinedAggregateFunction
    val udaf = new MyAverageUDAF1
    spark.udf.register("avgAge", functions.udaf(udaf))
    spark.sql("SELECT avgAge(age) FROM user").show()

    // Case class for buffer
    case class Buff(var sum: Long, var cnt: Long)

    // Define strong typed UDAF
    class MyAvgAgeUDAF extends Aggregator[Long, Buff, Double] {
      override def zero: Buff = Buff(0, 0)

      override def reduce(b: Buff, a: Long): Buff = {
        b.sum += a
        b.cnt += 1
        b
      }

      override def merge(b1: Buff, b2: Buff): Buff = {
        b1.sum += b2.sum
        b1.cnt += b2.cnt
        b1
      }

      override def finish(reduction: Buff): Double = reduction.sum.toDouble / reduction.cnt

      override def bufferEncoder: Encoder[Buff] = Encoders.product

      override def outputEncoder: Encoder[Double] = Encoders.scalaDouble
    }

    // Register and use strong typed UDAF
    val udaf = new MyAvgAgeUDAF
    spark.udf.register("avgAge", functions.udaf(udaf))
    spark.sql("SELECT avgAge(age) FROM user").show()
    ```
