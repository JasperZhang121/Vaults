### SparkSession
In older versions, SparkSQL provided two starting points for SQL queries: SQLContext for Spark's own SQL queries and HiveContext for connecting to Hive.

SparkSession is the new entry point for SQL queries in Spark. Essentially, it combines SQLContext and HiveContext, so any APIs available in SQLContext and HiveContext can also be used in SparkSession. Internally, <mark style="background: #FFB8EBA6;">SparkSession encapsulates SparkContext, meaning the actual computation is done by SparkContext</mark>.

### DataFrame

#### Creation
In Spark SQL, SparkSession is the entry point for creating DataFrames and executing SQL queries. There are three ways to create DataFrames: through Spark data sources, converting from an existing RDD, or querying from a Hive table.

- **Creating from Spark Data Sources**
    - List available file formats for Spark data sources:
        ```scala
        scala> spark.read.
        csv   format   jdbc   json   load   option   options   orc   parquet   schema   table   text   textFile
        ```

    - Reading a JSON file to create a DataFrame:
        ```scala
        scala> val df = spark.read.json("/opt/module/spark/examples/src/main/resources/people.json")
        df: org.apache.spark.sql.DataFrame = [age: bigint, name: string]
        ```
    
    - Displaying the result:
        ```scala
        scala> df.show
        +----+-------+
        | age|   name|
        +----+-------+
        |null|Michael|
        |  30|   Andy|
        |  19| Justin|
        +----+-------+
        ```

#### SQL Style Syntax

- **Creating a DataFrame:**
    ```scala
    scala> val df = spark.read.json("/opt/module/spark/examples/src/main/resources/people.json")
    df: org.apache.spark.sql.DataFrame = [age: bigint, name: string]
    ```

- **Creating a temporary table from the DataFrame:**
    ```scala
    scala> df.createOrReplaceTempView("people")
    ```

- **Querying the table using SQL:**
    ```scala
    scala> val sqlDF = spark.sql("SELECT * FROM people")
    sqlDF: org.apache.spark.sql.DataFrame = [age: bigint, name: string]
    ```

- **Displaying the result:**
    ```scala
    scala> sqlDF.show
    +----+-------+
    | age|   name|
    +----+-------+
    |null|Michael|
    |  30|   Andy|
    |  19| Justin|
    +----+-------+
    ```

Note: <mark style="background: #FFB86CA6;">Temporary tables are session-scoped and will be invalidated once the session ends</mark>.c If you need a table valid across sessions, you can use global temporary views, which require full path access like `global_temp.people`.

- **Creating a global temporary table:**
    ```scala
    scala> df.createGlobalTempView("people")
    ```

- **Querying the global table using SQL:**
    ```scala
    scala> spark.sql("SELECT * FROM global_temp.people").show()
    +----+-------+
    | age|   name|
    +----+-------+
    |null|Michael|
    |  30|   Andy|
    |  19| Justin|
    +----+-------+
    ```

    ```scala
    scala> spark.newSession().sql("SELECT * FROM global_temp.people").show()
    +----+-------+
    | age|   name|
    +----+-------+
    |null|Michael|
    |  30|   Andy|
    |  19| Justin|
    +----+-------+
    ```

#### DSL Syntax
DataFrame provides a domain-specific language (DSL) for managing structured data. DSL can be used in Scala, Java, Python, and R. Using DSL syntax eliminates the need to create temporary views.

1. **Creating a DataFrame:**
    ```scala
    scala> val df = spark.read.json("data/user.json")
    df: org.apache.spark.sql.DataFrame = [age: bigint, name: string]
    ```

2. **Viewing the Schema of the DataFrame:**
    ```scala
    scala> df.printSchema
    root
     |-- age: Long (nullable = true)
     |-- name: string (nullable = true)
    ```

3. **Viewing only the "name" column:**
    ```scala
    scala> df.select("name").show()
    +--------+
    |   name |
    +--------+
    |zhangsan|
    |   lisi |
    | wangwu |
    +--------+
    ```

4. **Viewing the "name" column and "age + 1" column:**
    Note: When performing operations, each column must use `$`, or use the quote expression: single quote + field name.
    ```scala
    scala> df.select($"name", $"age" + 1).show()
    scala> df.select('name, 'age + 1).show()
    scala> df.select('name, ('age + 1).as("new_age")).show()
    +--------+---------+
    |   name | new_age |
    +--------+---------+
    |zhangsan|       21|
    |   lisi |       31|
    | wangwu |       41|
    +--------+---------+
    ```

5. **Filtering rows where "age" is greater than 30:**
    ```scala
    scala> df.filter($"age" > 30).show()
    +---+--------+
    |age|    name|
    +---+--------+
    | 40| wangwu |
    +---+--------+
    ```

6. **Grouping by "age" and counting the number of rows in each group:**
    ```scala
    scala> df.groupBy("age").count().show()
    +---+-----+
    |age|count|
    +---+-----+
    | 20|    1|
    | 30|    1|
    | 40|    1|
    +---+-----+
    ```

#### Converting RDD to DataFrame
When developing a program in an IDE like IntelliJ IDEA, if you need to convert between RDD and DataFrame or DataSet, you must import `import spark.implicits._`. Here, `spark` is not a package name in Scala, but the variable name of the created SparkSession object. Therefore, you must create a SparkSession object before importing. The `spark` object cannot be declared with `var` because Scala only supports importing objects declared with `val`.
In `spark-shell`, no import is needed as it is done automatically.

```scala
scala> val idRDD = sc.textFile("data/id.txt")
scala> idRDD.toDF("id").show()
+---+
| id|
+---+
|  1|
|  2|
|  3|
|  4|
+---+
```

In practical development, it is common to use case classes to convert RDD to DataFrame.
```scala
scala> case class User(name: String, age: Int)
defined class User
scala> sc.makeRDD(List(("zhangsan", 30), ("lisi", 40))).map(t => User(t._1, t._2)).toDF().show()
+--------+---+
|    name|age|
+--------+---+
|zhangsan| 30|
|    lisi| 40|
+--------+---+
```

#### Converting DataFrame to RDD
A DataFrame is essentially an RDD with schema information, so you can directly get the underlying RDD.
```scala
scala> val df = sc.makeRDD(List(("zhangsan", 30), ("lisi", 40))).map(t => User(t._1, t._2)).toDF()
df: org.apache.spark.sql.DataFrame = [name: string, age: int]
scala> val rdd = df.rdd
rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[46] at rdd at <console>:25
scala> val array = rdd.collect()
array: Array[org.apache.spark.sql.Row] = Array([zhangsan,30], [lisi,40])
```
Note: The resulting RDD has elements of type `Row`.
```scala
scala> array(0)
res28: org.apache.spark.sql.Row = [zhangsan,30]
scala> array(0)(0)
res29: Any = zhangsan
scala> array(0).getAs[String]("name")
res30: String = zhangsan
```

### DataSet
A DataSet is a strongly-typed collection of data that requires corresponding type information.

#### Creating DataSet

1. **Using Case Class Sequence to Create DataSet:**
    ```scala
    scala> case class Person(name: String, age: Long)
    defined class Person
    scala> val caseClassDS = Seq(Person("zhangsan", 2)).toDS()
    caseClassDS: org.apache.spark.sql.Dataset[Person] = [name: string, age: Long]
    scala> caseClassDS.show()
    +---------+---+
    |     name|age|
    +---------+---+
    | zhangsan|  2|
    +---------+---+
    ```

2. **Using a Sequence of Basic Types to Create DataSet:**
    ```scala
    scala> val ds = Seq(1, 2, 3, 4, 5).toDS
    ds: org.apache.spark.sql.Dataset[Int] = [value: int]
    scala> ds.show()
    +-----+
    |value|
    +-----+
    |    1|
    |    2|
    |    3|
    |    4|
    |    5|
    +-----+
    ```

Note: In practical use, it is rare to convert a sequence into a DataSet directly; it is more common to obtain a DataSet from an RDD.

#### Converting RDD to DataSet
SparkSQL can automatically convert an RDD containing case classes to a DataSet. The case class defines the structure of the table, and the attributes of the case class become the columns of the table through reflection. Case classes can include complex structures like Seq or Array.
```scala
scala> case class User(name: String, age: Int)
defined class User
scala> sc.makeRDD(List(("zhangsan", 30), ("lisi", 49))).map(t => User(t._1, t._2)).toDS()
res11: org.apache.spark.sql.Dataset[User] = [name: string, age: int]
```

#### Converting DataSet to RDD
A DataSet is essentially an RDD with additional schema information, so you can directly get the underlying RDD.
```scala
scala> case class User(name: String, age: Int)
defined class User
scala> val ds = sc.makeRDD(List(("zhangsan", 30), ("lisi", 49))).map(t => User(t._1, t._2)).toDS()
ds: org.apache.spark.sql.Dataset[User] = [name: string, age: int]
scala> val rdd = ds.rdd
rdd: org.apache.spark.rdd.RDD[User] = MapPartitionsRDD[51] at rdd at <console>:25
scala> rdd.collect
res12: Array[User] = Array(User(zhangsan, 30), User(lisi, 49))
```

#### Converting DataFrame to DataSet
DataFrame is a special case of DataSet, so they can be converted to each other.
- **Converting DataFrame to DataSet:**
    ```scala
    scala> case class User(name: String, age: Int)
    defined class User
    scala> val df = sc.makeRDD(List(("zhangsan", 30), ("lisi", 49))).toDF("name", "age")
    df: org.apache.spark.sql.DataFrame = [name: string, age: int]
    scala> val ds = df.as[User]
    ds: org.apache.spark.sql.Dataset[User] = [name: string, age: int]
    ```

- **Converting DataSet to DataFrame:**
    ```scala
    scala> val ds = df.as[User]
    ds: org.apache.spark.sql.Dataset[User] = [name: string, age: int]
    scala> val df = ds.toDF()
    df: org.apache.spark.sql.DataFrame = [name: string, age: int]
    ```
