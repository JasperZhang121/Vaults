### Commonalities Among RDD, DataFrame, and DataSet
- RDD, DataFrame, and DataSet are all <mark style="background: #FFB8EBA6;">distributed elastic</mark> datasets in the Spark platform, facilitating the processing of large-scale data.
- All three exhibit <mark style="background: #FFB86CA6;">lazy evaluation</mark>, meaning that transformations (like `map`) are not executed immediately. They only start processing when an action (like `foreach`) is invoked.
- They share many common functions such as `filter`, `sort`, etc.
- Operations on DataFrame and DataSet often require the import `import spark.implicits._` (import this after creating the SparkSession object).
- All three automatically cache computations based on Spark's memory availability, preventing memory overflow even with large data volumes.
- They all have the concept of partitions.
- DataFrame and DataSet can use pattern matching to access the values and types of individual fields.

### Differences Between RDD, DataFrame, and DataSet

1. **RDD**
   - RDD is commonly used with Spark MLlib.
   - RDD does not support SparkSQL operations.

2. **DataFrame**
   - Unlike RDD and DataSet, <mark style="background: #FFB86CA6;">each row in a DataFrame has a fixed type</mark> of `Row`, and the values of each column cannot be directly accessed without <mark style="background: #FFF3A3A6;">parsing</mark> to get the values of individual fields.
   - DataFrame and DataSet both support SparkSQL operations like `select`, `groupBy`, and can register temporary tables/views for SQL operations.
   - DataFrame and DataSet support convenient saving methods, such as saving as CSV with headers, making column names clear at a glance.

3. **DataSet**
   - DataSet and DataFrame have identical member functions; <mark style="background: #ADCCFFA6;">the only difference is the data type of each row</mark>. A DataFrame is actually a special case of DataSet: `type DataFrame = Dataset[Row]`.
   - DataFrame can also be called `Dataset[Row]`. Each row's type is `Row`, and without parsing, the specific fields and their types cannot be known. You can use the `getAs` method or pattern matching to extract specific fields.
   - In DataSet, the type of each row is not fixed. After defining a case class, you can freely access the information of each row.

### Converting Between RDD, DataFrame, and DataSet


![[rddCompare.png]]
### Developing SparkSQL in IntelliJ IDEA

In practice, development is usually done using IntelliJ IDEA.

#### Adding Dependencies
```xml
<dependency>
  <groupId>org.apache.spark</groupId>
  <artifactId>spark-sql_2.12</artifactId>
  <version>3.0.0</version>
</dependency>
```

#### Code Implementation
```java
object SparkSQL01_Demo {
  def main(args: Array[String]): Unit = {
    // Create context configuration object
    val conf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("SparkSQL01_Demo")
    // Create SparkSession object
    val spark: SparkSession = SparkSession.builder().config(conf).getOrCreate()

    // RDD to DataFrame to DataSet conversion requires implicit conversion rules
    // "spark" is not a package name but the name of the context object
    import spark.implicits._

    // Read JSON file to create DataFrame {"username": "lisi", "age": 18}
    val df: DataFrame = spark.read.json("input/test.json")
    // df.show()

    // SQL style syntax
    df.createOrReplaceTempView("user")
    // spark.sql("select avg(age) from user").show()

    // DSL style syntax
    // df.select("username", "age").show()

    // ***** RDD => DataFrame => DataSet *****
    // RDD
    val rdd1: RDD[(Int, String, Int)] = spark.sparkContext.makeRDD(List((1, "zhangsan", 30), (2, "lisi", 28), (3, "wangwu", 20)))
    // DataFrame
    val df1: DataFrame = rdd1.toDF("id", "name", "age")
    // df1.show()

    // DataSet
    val ds1: Dataset[User] = df1.as[User]
    // ds1.show()

    // ***** DataSet => DataFrame => RDD *****
    // DataFrame
    val df2: DataFrame = ds1.toDF()
    // RDD
    val rdd2: RDD[Row] = df2.rdd
    // rdd2.foreach(a => println(a.getString(1)))

    // ***** RDD => DataSet *****
    rdd1.map {
      case (id, name, age) => User(id, name, age)
    }.toDS()

    // ***** DataSet => RDD *****
    ds1.rdd

    // Release resources
    spark.stop()
  }
}

case class User(id: Int, name: String, age: Int)
```
