### General Methods for Loading and Saving Data
SparkSQL provides general methods for loading and saving data. The term "general" here means using the same API to read and save data in different formats based on the parameters provided. The default file format for reading and saving in SparkSQL is Parquet.

1. **Loading Data**
   The `spark.read.load` method is a general way to load data.

    ```scala
    scala> spark.read.
    csv
    table  format
    text   jdbc
    json   textFile
    load   option
    options  orc
    parquet  schema
    ```

    To read data in different formats, you can set different parameters for each data format.

    ```scala
    spark.read.format("…")[.option("…")].load("…")
    ```

    - `format("…")`: Specifies the data format to load, including "csv", "jdbc", "json", "orc", "parquet", and "textFile".
    - `load("…")`: The path to load data in the specified format.
    - `option("…")`: For "jdbc" format, necessary JDBC parameters like `url`, `user`, `password`, and `dbtable`.

    Typically, we use the `read` API to load a file into a DataFrame and then query it. However, we can also query the file directly using `format.` and the file path.

    ```scala
    scala> spark.sql("SELECT * FROM json.`/opt/module/data/user.json`").show()
    ```

2. **Saving Data**
   The `df.write.save` method is a general way to save data.

    ```scala
    df.write.format("…")[.option("…")].save("…")
    ```

    - `format("…")`: Specifies the data format to save, including "csv", "jdbc", "json", "orc", "parquet", and "textFile".
    - `save("…")`: The path to save data in the specified format.
    - `option("…")`: For "jdbc" format, necessary JDBC parameters like `url`, `user`, `password`, and `dbtable`.

    Save operations can use `SaveMode` to specify how to handle existing data, set using the `mode()` method. These `SaveMode` settings are not atomic operations and do not involve locking.

    | Scala/Java                     | Any Language   | Meaning                                      |
    |--------------------------------|----------------|----------------------------------------------|
    | SaveMode.ErrorIfExists(default)| "error" (default) | Throws an exception if the file already exists |
    | SaveMode.Append                | "append"       | Appends to the file if it already exists      |
    | SaveMode.Overwrite             | "overwrite"    | Overwrites the file if it already exists      |
    | SaveMode.Ignore                | "ignore"       | Ignores the operation if the file already exists |

    ```scala
    df.write.mode("append").json("/opt/module/data/output")
    ```

### Parquet
Parquet is the default data source in Spark SQL, an efficient columnar storage format for nested data. With Parquet files, Spark SQL can perform all operations easily without needing to specify the format.

- **Loading Data**

    ```scala
    val df = spark.read.parquet("path/to/parquet/file")
    ```

- **Saving Data**

    ```scala
    df.write.parquet("path/to/output")
    ```

### JSON
Spark SQL can automatically infer the schema of a JSON dataset and load it as a `Dataset[Row]`. Use `SparkSession.read.json()` to load JSON files. Note that Spark expects each line in the JSON file to be a valid JSON object.

1. **Import Implicit Conversions**

    ```scala
    import spark.implicits._
    ```

2. **Load JSON File**

    ```scala
    val df = spark.read.json("data/user.json")
    ```

3. **Create Temporary Table**

    ```scala
    df.createOrReplaceTempView("people")
    ```

4. **Query Data**

    ```scala
    val teenagerNamesDF = spark.sql("SELECT name FROM people WHERE age BETWEEN 13 AND 19")
    teenagerNamesDF.show()
    +------+
    |  name|
    +------+
    |Justin|
    +------+
    ```

### CSV
Spark SQL can configure and read CSV files, treating the first line as column headers.

1. **Load CSV File**

    ```scala
    val df = spark.read.option("header", "true").csv("data/user.csv")
    ```

### MySQL
Spark SQL can read from and write to relational databases via JDBC. To operate on MySQL in IntelliJ IDEA, follow these steps:

1. **Add Dependencies**

    ```xml
    <dependency>
      <groupId>mysql</groupId>
      <artifactId>mysql-connector-java</artifactId>
      <version>5.1.27</version>
    </dependency>
    ```

2. **Read Data**

    ```java
    val conf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("SparkSQL")
    val spark: SparkSession = SparkSession.builder().config(conf).getOrCreate()
    import spark.implicits._

    // Method 1: Using the general `load` method
    spark.read.format("jdbc")
      .option("url", "jdbc:mysql://localhost:3306/spark-sql")
      .option("driver", "com.mysql.jdbc.Driver")
      .option("user", "root")
      .option("password", "123123")
      .option("dbtable", "user")
      .load().show()

    // Method 2: Using the general `load` method with options
    spark.read.format("jdbc")
      .options(Map("url" -> "jdbc:mysql://localhost:3306/spark-sql?user=root&password=123123", "dbtable" -> "user", "driver" -> "com.mysql.jdbc.Driver"))
      .load().show()

    // Method 3: Using the `jdbc` method
    val props: Properties = new Properties()
    props.setProperty("user", "root")
    props.setProperty("password", "123123")
    val df: DataFrame = spark.read.jdbc("jdbc:mysql://localhost:3306/spark-sql", "user", props)
    df.show()

    // Release resources
    spark.stop()
    ```

3. **Write Data**

    ```java
    case class User2(name: String, age: Long)

    val conf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("SparkSQL")
    val spark: SparkSession = SparkSession.builder().config(conf).getOrCreate()
    import spark.implicits._

    val rdd: RDD[User2] = spark.sparkContext.makeRDD(List(User2("lisi", 20), User2("zs", 30)))
    val ds: Dataset[User2] = rdd.toDS()

    // Method 1: Using the general `format` method
    ds.write.format("jdbc")
      .option("url", "jdbc:mysql://localhost:3306/spark-sql")
      .option("user", "root")
      .option("password", "123123")
      .option("dbtable", "user")
      .mode(SaveMode.Append)
      .save()

    // Method 2: Using the `jdbc` method
    val props: Properties = new Properties()
    props.setProperty("user", "root")
    props.setProperty("password", "123123")
    ds.write.mode(SaveMode.Append).jdbc("jdbc:mysql://localhost:3306/spark-sql", "user", props)

    // Release resources
    spark.stop()
    ```

### Hive

Apache Hive is an SQL engine on Hadoop. Spark SQL can be compiled with or without Hive support. With Hive support, Spark SQL can access Hive tables, use user-defined functions (UDFs), and execute Hive Query Language (HiveQL) statements. It's important to note that Spark SQL can include Hive libraries without requiring Hive to be installed beforehand. Ideally, Hive support should be included during Spark SQL compilation to utilize these features. If you download the binary version of Spark, it should already include Hive support. 

To connect Spark SQL to a deployed Hive instance, copy the `hive-site.xml` file to Spark's configuration directory (`$SPARK_HOME/conf`). Even without a deployed Hive, Spark SQL can run, creating its own Hive metastore (`metastore_db`) in the current working directory. If you try to create tables using HiveQL's `CREATE TABLE` (not `CREATE EXTERNAL TABLE`), these tables will be placed in the default file system's `/user/hive/warehouse` directory (HDFS if configured, otherwise local file system). The `spark-shell` by default includes Hive support; in code, it needs to be enabled manually.

#### 1) Embedded Hive

If using Spark's embedded Hive, no additional setup is required. Hive metadata is stored in Derby, and the default warehouse location is `$SPARK_HOME/spark-warehouse`.

```scala
scala> spark.sql("SHOW TABLES").show
+--------+---------+-----------+
|database|tableName|isTemporary|
+--------+---------+-----------+
+--------+---------+-----------+

scala> spark.sql("CREATE TABLE aa(id INT)")

scala> spark.sql("SHOW TABLES").show
+--------+---------+-----------+
|database|tableName|isTemporary|
+--------+---------+-----------+
| default|       aa|      false|
+--------+---------+-----------+
```

Load local data into the table:

```scala
scala> spark.sql("LOAD DATA LOCAL INPATH 'input/ids.txt' INTO TABLE aa")

scala> spark.sql("SELECT * FROM aa").show
+---+
| id|
+---+
|  1|
|  2|
|  3|
|  4|
+---+
```

In practice, almost no one uses the embedded Hive.

#### 2) External Hive

To connect to an externally deployed Hive, follow these steps:

- Copy `hive-site.xml` to Spark's `conf/` directory.
- Copy the MySQL driver to the `jars/` directory.
- If HDFS is inaccessible, copy `core-site.xml` and `hdfs-site.xml` to the `conf/` directory.
- Restart `spark-shell`.

#### 3) Running Spark SQL CLI

The Spark SQL CLI can conveniently run Hive metadata services and execute query tasks from the command line. Start Spark SQL CLI by running the following command in the Spark directory:

```shell
bin/spark-sql
```

#### 4) Running Spark Beeline

Spark Thrift Server is a Thrift service implemented by the Spark community based on HiveServer2, aiming for seamless compatibility with HiveServer2. After deploying Spark Thrift Server, use Hive's `beeline` to access Spark Thrift Server and execute queries. Spark Thrift Server can interact with the Hive Metastore to retrieve Hive metadata.

To connect to Thrift Server, follow these steps:

- Copy `hive-site.xml` to Spark's `conf/` directory.
- Copy the MySQL driver to the `jars/` directory.
- If HDFS is inaccessible, copy `core-site.xml` and `hdfs-site.xml` to the `conf/` directory.
- Start Thrift Server:

    ```shell
    sbin/start-thriftserver.sh
    ```

- Use `beeline` to connect to Thrift Server:

    ```shell
    bin/beeline -u jdbc:hive2://localhost:10000 -n root
    ```

#### 5) Code to Operate Hive

- Add dependencies:

    ```xml
    <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-hive_2.12</artifactId>
      <version>3.0.0</version>
    </dependency>
    <dependency>
      <groupId>org.apache.hive</groupId>
      <artifactId>hive-exec</artifactId>
      <version>1.2.1</version>
    </dependency>
    <dependency>
      <groupId>mysql</groupId>
      <artifactId>mysql-connector-java</artifactId>
      <version>5.1.27</version>
    </dependency>
    ```

- Copy the `hive-site.xml` file to the project's `resources` directory. Implement the code:

    ```scala
    // Create SparkSession
    val spark: SparkSession = SparkSession
      .builder()
      .enableHiveSupport()
      .master("local[*]")
      .appName("sql")
      .getOrCreate()

    // Optional: Modify the default warehouse location
    spark.conf.set("spark.sql.warehouse.dir", "hdfs://localhost:8020/user/hive/warehouse")

    // Optional: Resolve permission issues
    System.setProperty("HADOOP_USER_NAME", "root")

    // Example operations
    spark.sql("CREATE TABLE IF NOT EXISTS src (key INT, value STRING)")
    spark.sql("LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src")
    spark.sql("SELECT * FROM src").show()
    ```

- Note: The default database created in development tools is local. Modify the warehouse path to use HDFS if needed:

    ```scala
    spark.conf.set("spark.sql.warehouse.dir", "hdfs://localhost:8020/user/hive/warehouse")
    ```

If errors occur during operations, add the following code at the beginning:

    ```scala
    System.setProperty("HADOOP_USER_NAME", "root")
    ```

Replace `root` with your Hadoop username.

