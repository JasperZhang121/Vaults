### Spark Runtime Architecture

The core of the Spark framework is a computing engine that adopts a standard <mark style="background: #FFB8EBA6;">master-slave structure</mark>. The figure below illustrates the basic structure of Spark during execution. <mark style="background: #BBFABBA6;">The Driver in the diagram represents the master, responsible for managing the job scheduling across the entire cluster. The Executors, represented as slaves, are responsible for executing the tasks</mark>.

![[sparkContext.jpg]]

### Core Components

#### Driver
The Spark Driver node is used to execute the <mark style="background: #FFF3A3A6;">main method</mark> of a Spark job, responsible for the actual code execution. During Spark job execution, the Driver is mainly responsible for:

- Converting user programs into jobs
- Scheduling tasks among Executors
- Tracking the execution status of Executors
- Displaying the query execution status through the UI

In essence, we cannot precisely define the Driver as there is no direct reference to it in the programming process. Simply put, the Driver is the program that drives the entire application, also known as the Driver class.

#### Executor
A Spark <mark style="background: #BBFABBA6;">Executor is a JVM process running on the worker nodes in the cluster</mark>, responsible for executing specific tasks (Tasks) in a Spark job. These <mark style="background: #ADCCFFA6;">tasks are independent</mark> of each other. When a Spark application starts, Executor nodes are simultaneously started and persist throughout the application's lifecycle. If an Executor node fails or crashes, the Spark application can continue running by rescheduling the tasks from the failed node to other Executor nodes.

#### Master & Worker
In an independent deployment environment of a Spark cluster, there is no need to rely on other resource scheduling frameworks as Spark itself implements resource scheduling functions. Therefore, the environment includes two other core components: Master and Worker. The Master is a process primarily responsible for resource scheduling and allocation and cluster monitoring, similar to the ResourceManager (RM) in a Yarn environment. The Worker is also a process running on a server in the cluster, allocated resources by the Master to process and compute data in parallel, similar to the NodeManager (NM) in a Yarn environment.

#### ApplicationMaster
When a Hadoop user submits an application to the YARN cluster, the submission program should include an ApplicationMaster, which is responsible for requesting resource containers from the resource scheduler to execute the task, running the user's program tasks, monitoring the task execution, tracking the task status, and handling task failures. Simply put, the decoupling between ResourceManager (resources) and Driver (computation) relies on ApplicationMaster.

### Core Concepts
#### Executor and Core
A Spark Executor is a JVM process running on worker nodes in the cluster, dedicated to computation. When submitting an application, <mark style="background: #BBFABBA6;">parameters can be specified to determine the number of computing nodes and the corresponding resources</mark>. The resources generally refer to the memory size of the Executor and the number of virtual CPU cores used. The related startup parameters for the application are as follows:

| Name              | Description                         |
|-------------------|-------------------------------------|
| --num-executors   | Number of Executors                 |
| --executor-memory | Memory size per Executor            |
| --executor-cores  | Number of virtual CPU cores per Executor |

### Parallelism
In a distributed computing framework, multiple tasks are executed simultaneously. Since <mark style="background: #D2B3FFA6;">tasks are distributed across different computing nodes, true parallel execution</mark> of multiple tasks can be achieved. Note that this is parallelism, not concurrency. We refer to the number of tasks executed in parallel across the entire cluster as parallelism. The level of parallelism for a job depends on the default configuration of the framework and can be dynamically modified during application runtime.

### Directed Acyclic Graph (DAG)

![[sparkDAG.jpg]]

Big data computing engines are generally classified into four categories based on their usage. The first category includes MapReduce, supported by Hadoop, which divides computation into two stages: Map and Reduce. For higher-level applications, this requires breaking down algorithms and chaining multiple jobs to complete a full algorithm, such as iterative computations. This limitation led to the emergence of DAG-supported frameworks.

Thus, DAG-supported frameworks are classified as second-generation computing engines, like Tez and higher-level Oozie. Tez and Oozie were mainly batch processing tasks at the time. <mark style="background: #ADCCFFA6;">Next came the third-generation computing engines represented by Spark. The main feature of the third-generation engines is the internal DAG support within a job (not across jobs) and real-time computation.</mark> The so-called directed acyclic graph here is not a true graph but a high-level abstract model of data flow directly mapped from a Spark program. Simply put, it visually represents the execution process of the entire program, making it easier to understand and can be used to represent the program's topology. A Directed Acyclic Graph (DAG) is a topological graph composed of points and lines, with direction and no cycles.

### Submission Process

The submission process refers to the workflow where developers submit applications to the Spark runtime environment for computation via the Spark client. This process is similar across different deployment environments but has subtle differences. We won't detail these differences here, but since deploying Spark to a Yarn environment is more common in domestic work, the submission process in this course is based on the Yarn environment. When a Spark application is submitted to a Yarn environment, there are generally two deployment modes: Client and Cluster. The main difference between these modes is the location of the Driver program.

When a Spark application is submitted to a Yarn environment, there are generally two deployment modes: Client and Cluster. The main difference between these modes is the location of the Driver program.

### Yarn Client Mode
In Client mode, the Driver module for monitoring and scheduling runs on the client side, not within Yarn, so it's generally used for testing.
- The Driver runs on the local machine where the task is submitted
- After starting, the Driver communicates with ResourceManager to request the start of ApplicationMaster
- ResourceManager allocates a container and starts ApplicationMaster on an appropriate NodeManager, responsible for requesting Executor memory from ResourceManager
- ResourceManager allocates a container upon receiving the ApplicationMaster's resource request, and ApplicationMaster starts Executor processes on the specified NodeManager
- After starting, Executor processes register back with the Driver. Once all Executors are registered, the Driver begins executing the main function
- When an Action operator is executed, a job is triggered. Based on wide dependencies, stages are divided, each stage generates a corresponding TaskSet, and tasks are then distributed to Executors for execution.

### Yarn Cluster Mode
In Cluster mode, the Driver module for monitoring and scheduling runs within Yarn's cluster resources. This mode is generally used in production environments.
- In Yarn Cluster mode, after submitting the task, it communicates with ResourceManager to request the start of ApplicationMaster
- ResourceManager allocates a container and starts ApplicationMaster on an appropriate NodeManager. At this point, the ApplicationMaster is the Driver.
- The Driver starts and requests Executor memory from ResourceManager. Upon receiving the resource request, ResourceManager allocates a container and starts Executor processes on the specified NodeManager
- After starting, Executor processes register back with the Driver. Once all Executors are registered, the Driver begins executing the main function
- When an Action operator is executed, a job is triggered. Based on wide dependencies, stages are divided, each stage generates a corresponding TaskSet, and tasks are then distributed to Executors for execution.