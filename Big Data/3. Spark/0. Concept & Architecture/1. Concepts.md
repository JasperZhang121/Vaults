### Introduction

Spark is a fast, <mark style="background: #FFB8EBA6;">in-memory, general-purpose, and scalable</mark> big data processing engine.

It was developed by the AMP Lab (Algorithms, Machines, and People Lab) at UC Berkeley. The Spark ecosystem, also called BDAS, is an open-source platform designed to showcase large-scale data applications through integration across algorithms, machines, and people. Leveraging big data and cloud resources, AMP Lab developed Spark to transform massive data into useful information, helping people better understand the world. It was open-sourced in 2010, became an Apache incubator project in June 2013, and was promoted to an Apache top-level project in February 2014. The framework is written in Scala.

### Features of Spark

#### Speed
Apache Spark performs computations in memory, using a DAG (Directed Acyclic Graph) execution engine to enable faster processing. Its in-memory operations make it approximately 100 times faster than Hadoop MapReduce, and 10 times faster on disk.

The speed advantage of Spark over MapReduce is due to:
- **Reduced Disk I/O**: <mark style="background: #FFB8EBA6;">Intermediate results are stored in memory instead of being written to disk</mark> as in MapReduce.
- **Increased Parallelism**: Tasks in Spark are <mark style="background: #BBFABBA6;">executed as threads and split into stages</mark>, allowing parallel execution.
- **Avoided Redundant Computations**: The DAG in Spark reduces redundant operations.

#### Ease of Use
Spark supports APIs for Java, Python, and Scala, along with over 80 high-level algorithms. Interactive shells for Python and Scala allow users to verify solutions on a Spark cluster quickly.

#### Versatility
Spark provides a wide range of libraries, such as Spark SQL, Spark Streaming, MLlib, and GraphX, enabling seamless integration within an application. For example, Spark SQL handles structured data, Spark Streaming handles real-time data, MLlib provides machine learning algorithms, and GraphX supports graph computations.

#### Compatibility
Spark can integrate with other open-source tools, using Hadoop's YARN and Apache Mesos as resource managers. It supports data formats like HDFS and HBase, making it easy for users with existing Hadoop clusters to utilize Spark’s capabilities without data migration.

### Spark Components

The Spark framework includes the following modules:

- **Spark Core**: Implements fundamental functionalities like task scheduling, memory management, error recovery, and storage system interaction. It also defines the API for Resilient Distributed Datasets (RDDs).
- **Spark SQL**: A package for working with structured data, allowing queries in SQL or Hive SQL dialects. Supports sources like Hive tables, Parquet, and JSON.
- **Spark Streaming**: Provides streaming data processing capabilities, closely aligned with RDD APIs for seamless data processing.
- **Spark MLlib**: A machine learning library with classification, regression, clustering, and collaborative filtering functionalities.
- **Spark GraphX**: An API for graph processing, with optimized performance and a rich set of operators for complex graph algorithms on large datasets.
- **Structured Streaming**: A scalable, fault-tolerant streaming engine that processes structured streaming data in DataFrames. It operates on Spark SQL's engine, enabling real-time analysis.

### Spark Operating Modes

#### Operating Modes

Spark can run in three main modes: Local, Cluster, and Cloud.

- **Local Mode**: Runs tasks in <mark style="background: #FFB86CA6;">a single JVM process</mark>, used mainly for development and testing.
- **Cluster Mode**: Includes Standalone, YARN, and Mesos:
  - **Standalone Mode**: Uses Spark's built-in scheduler for resource management.
  - **YARN Mode**: Leverages Hadoop YARN for task scheduling in production environments.
  - **Mesos Mode**: Uses Apache Mesos for resource management (rarely used in China).
- **Kubernetes Mode**: Since Spark 2.3, Spark applications can run on Kubernetes clusters.

### Deployment Modes

There are two deployment modes for Spark applications: Client and Cluster.

- **Client Mode**: The Driver runs on the client machine where the application is submitted.
- **Cluster Mode**: The Driver runs on one of the cluster's worker nodes.

### Spark Terminology

#### Cluster-Related Terms

- **Cluster Manager**: Manages resources across the cluster. In Standalone mode, it's the Master; in YARN mode, it's the ResourceManager.
- **Worker**: The node that performs tasks. In YARN, NodeManager replaces it, handling resource registration, executor creation, and task distribution.

#### Task-Related Terms

- **Driver**: Manages the application's <mark style="background: #BBFABBA6;">SparkContext and RDD transformations</mark>. Responsible for:
  - Translating user programs into jobs
  - Tracking executor status
  - Scheduling tasks
  - Displaying application status
- **Executor**: <mark style="background: #FFB8EBA6;">Runs tasks on worker nodes, handling storage for RDDs. If an executor fails, tasks are rescheduled on other nodes</mark>.
- **Application**: A Spark application comprises the Driver and the Executors across nodes, initialized through a SparkContext.
- **DAG (Directed Acyclic Graph)**: Represents <mark style="background: #ADCCFFA6;">dependencies and execution flows</mark> between RDDs.
- **Job**: Generated by an Action operation.
- **Stage**: A group of <mark style="background: #FFF3A3A6;">parallel tasks divided by wide dependencies</mark> in the DAG.
- **Task**: The unit of work for each partition, with each task handling a pipeline of operations.
- **Partition**: A subdivision of data in an RDD, determining the number of tasks.

#### Hierarchy

Each layer in Spark follows a 1-to-n relationship:

- **Application** -> **Job** -> **Stage** -> **Task**
  
Each main method can generate one or more Applications. Each Application can contain multiple jobs, each job multiple stages, and each stage multiple tasks.

---
### RDDs (Resilient Distributed Datasets)

RDDs are the fundamental data structure of Apache Spark. They are immutable, distributed collections of objects that can be processed in parallel.

 #### Characteristics of RDDs:
   - **Immutable:** Once created, RDDs cannot be changed. However, transformations can be applied to create new RDDs. This immutability helps to ensure consistency and fault tolerance.
   - **Distributed:** Data is distributed across multiple nodes in a cluster, allowing for parallel processing and scalability.
   - **Fault-Tolerant:** RDDs can recover from node failures using lineage information, which keeps track of the transformations used to build the dataset.

#### Creating RDDs:
   - **From a Collection:** RDDs can be created from an existing collection in the driver program.
     ```python
     data = [1, 2, 3, 4, 5]
     rdd = spark.sparkContext.parallelize(data)
     ```
   - **From an External Source:** RDDs can be created from external storage systems such as HDFS, S3, or HBase.
     ```python
     rdd = spark.sparkContext.textFile("hdfs://path/to/file")
     ```

#### RDD Operations:
   - **Transformations:** Operations that create a new RDD from an existing one. They are lazy, meaning they are not executed immediately. Instead, they are recorded as a lineage of transformations to be applied when an action is called.
     - `map(func)`: Returns a new RDD by applying a function to each element.
       ```python
       rdd2 = rdd.map(lambda x: x * 2)
       ```
     - `filter(func)`: Returns a new RDD containing only elements that satisfy the predicate.
       ```python
       rdd2 = rdd.filter(lambda x: x % 2 == 0)
       ```
     - `flatMap(func)`: Similar to `map`, but each input item can be mapped to 0 or more output items.
       ```python
       rdd2 = rdd.flatMap(lambda x: (x, x * 2))
       ```
     - `union(otherRDD)`: Returns a new RDD containing all elements from both RDDs.
       ```python
       rdd2 = rdd.union(otherRDD)
       ```
     - `distinct()`: Returns a new RDD with distinct elements.
       ```python
       rdd2 = rdd.distinct()
       ```
   - **Actions:** Operations that trigger the execution of transformations and return a result to the driver program. Actions force the computation of RDD transformations.
     - `collect()`: Returns all elements of the RDD as an array.
       ```python
       result = rdd.collect()
       ```
     - `count()`: Returns the number of elements in the RDD.
       ```python
       count = rdd.count()
       ```
     - `take(n)`: Returns the first `n` elements of the RDD.
       ```python
       sample = rdd.take(5)
       ```
     - `reduce(func)`: Aggregates the elements of the RDD using a function.
       ```python
       sum = rdd.reduce(lambda x, y: x + y)
       ```

### DataFrames and Datasets

DataFrames and Datasets provide higher-level APIs for working with structured data in Spark.

#### DataFrames:
   - DataFrames are distributed collections of data organized into named columns. They provide a more optimized and user-friendly API compared to RDDs, with rich semantics for dealing with structured data.
   - Similar to a table in a relational database or a data frame in R/Python.
   
   ```python
   df = spark.read.json("hdfs://path/to/json/file")
   df.show()
   ```

#### DataFrame Operations:
 - `select()`: Selects a set of columns.
   ```python
   df.select("name", "age").show()
   ```
 - `filter()`: Filters rows based on a condition.
   ```python
   df.filter(df["age"] > 30).show()
   ```
 - `groupBy()`: Groups rows based on a column(s).
   ```python
   df.groupBy("age").count().show()
   ```
 - `agg()`: Performs aggregate functions.
   ```python
   df.groupBy("age").agg({"salary": "avg"}).show()
   ```
 - `join()`: Joins two DataFrames.
   ```python
   df1.join(df2, df1["id"] == df2["id"]).show()
   ```

#### Datasets:
   - Datasets are a type-safe, object-oriented API for working with structured data. They combine the benefits of RDDs (type-safety, functional programming) and DataFrames (optimized execution).
   
   ```scala
   case class Person(name: String, age: Long)
   val ds = Seq(Person("Alice", 29), Person("Bob", 31)).toDS()
   ds.show()
   ```
#### Dataset Operations:
 - Similar to DataFrame operations but with type-safety.
 - `map()`, `flatMap()`, `filter()`, etc.
   ```scala
   val names = ds.map(_.name)
   names.show()
   ```

#### Comparing RDDs, DataFrames, and Datasets:
- **RDDs:** Low-level API, supports any type of data, no optimizations.
- **DataFrames:** High-level API, optimized, suited for structured data.
- **Datasets:** Combines RDD and DataFrame benefits, type-safe, optimized.

### SparkSession

SparkSession is the entry point for any Spark application. It encapsulates the SparkContext and acts as the gateway to Spark’s functionalities.

1. **Creating a SparkSession:**
   - A SparkSession can be created using the `builder` pattern.
     ```python
     from pyspark.sql import SparkSession
     spark = SparkSession.builder \
         .appName("MyApp") \
         .getOrCreate()
     ```

2. **Using SparkSession:**
   - **Configuration:** Configure Spark settings.
     ```python
     spark.conf.set("spark.sql.shuffle.partitions", "50")
     ```
   - **Reading Data:** Read data from various sources.
     ```python
     df = spark.read.csv("path/to/csv")
     ```
   - **Creating DataFrames and Datasets:**
     ```python
     data = [("Alice", 29), ("Bob", 31)]
     df = spark.createDataFrame(data, ["Name", "Age"])
     ```

3. **SparkSession Methods:**
   - `read`: Returns a `DataFrameReader` to read data.
   - `createDataFrame`: Converts an RDD, a list, or a Pandas DataFrame into a Spark DataFrame.
   - `sql`: Executes a SQL query and returns the result as a DataFrame.
     ```python
     df = spark.sql("SELECT * FROM my_table")
     ```
