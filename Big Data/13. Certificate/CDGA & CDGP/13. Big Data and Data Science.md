### Principles for Managing Metadata in Big Data

Organizations should carefully manage metadata associated with big data sources to accurately and efficiently handle data files, their origins, and their value.

### Characteristics of Big Data (The 6 Vs)

1. **Volume**: The large scale of data.
2. **Velocity**: The rapid speed of data updates.
3. **Variety**: The diversity and variability in data types.
4. **Viscosity**: The resistance to flow or integration in data processes.
5. **Volatility**: The high variability and short-lived nature of data.
6. **Veracity**: The low accuracy or reliability of data.

### Data Scientist

A data scientist is a professional who explores data, develops predictive models, machine learning models, prescriptive models, and analytical methods, and deploys these solutions for stakeholder analysis.

### ETL vs. ELT in Data Management

- Most data warehouses rely on **ETL** (Extract, Transform, Load).
- Big data solutions, such as data lakes, typically rely on **ELT** (Extract, Load, Transform).

### Business Drivers for Big Data

The primary business driver for improving an organization's big data and data science capabilities is the expectation of seizing opportunities identified from insights within datasets generated by various processes.


### <mark style="background: #FFB8EBA6;">Dependencies of Data Science</mark>

1. Rich data sources.
2. Organized information and analytical processes.
3. Delivery of insights.
4. Visualization of findings and data insights.


### Stages in the Data Science Process

1. Define a big data strategy and business needs.
2. Select data sources.
3. Acquire and ingest data.
4. Formulate data hypotheses and methodologies.
5. Integrate and adjust data for analysis.
6. Use models to explore data.
7. Deploy and monitor the results.


### DW/BI and Big Data Conceptual Architecture

**Data Lake**:  
An environment designed to extract, store, evaluate, and analyze massive amounts of diverse and structured/unstructured data. It supports various use cases, providing:

1. An environment for data scientists to mine and analyze data.
2. A centralized storage area for raw data, requiring minimal transformation (if any).
3. A backup storage area for detailed historical data from data warehouses.
4. An online archive for information records.
5. An environment to extract streaming data through automated model recognition.

### Key Concepts and Activities in Big Data and Analytics

#### **Data Lakes**

- **Risk**: Data lakes can quickly turn into "data swamps" if metadata is not managed during ingestion.
- **Services-Based Architecture (SBA)**:
    - A method for instant data access while ensuring accurate historical data updates.
    - **Components**:
        1. **Batch Processing Layer**: Provides services for both recent and historical data using the data lake.
        2. **Acceleration Layer**: Handles real-time data only.
        3. **Service Layer**: Interfaces connecting batch and acceleration layer data.


#### **Machine Learning**

- **Definition**: Combines unsupervised learning and supervised learning, with reinforcement learning as an emerging third branch.
- **Types**:
    1. **Supervised Learning**: Based on numerical theories.
    2. **Unsupervised Learning**: Focused on discovering hidden patterns (e.g., data mining).
    3. **Reinforcement Learning**: Goal-oriented optimization (e.g., winning a chess match).
- **Purpose**: Enables machines to learn and adapt quickly from queries in ever-changing datasets.


#### **Semantic Analysis**

- **Definition**: An automated method for extracting insights from unstructured or semi-structured data to perceive people's opinions on brands, products, services, or topics.


#### **Data Mining**

- **Branch**: A subset of machine learning under unsupervised learning.
- **Techniques**:
    1. **Profiling**: Describes typical behaviors for anomaly detection.
    2. **Data Reduction**: Replaces large datasets with smaller ones.
    3. **Association**: Finds relationships among transaction elements.
    4. **Clustering**: Groups data based on shared features.
    5. **Self-Organizing Maps**: Reduces dimensionality of evaluation spaces.


#### **Predictive Analytics**

- **Definition**: A supervised learning subfield where users model data elements and evaluate probabilities to predict future outcomes.
- **Focus**: Probability-based modeling of potential events (e.g., purchases, price changes) using historical data.


#### **Prescriptive Analytics**

- **Definition**: Goes beyond predicting outcomes by defining actions that influence results.
- **Purpose**: Predicts what, when, and why something might happen and suggests measures to address it.


#### **Unstructured Data Analysis**

- **Techniques**: Combines text mining, association analysis, clustering, and other unsupervised learning methods.
- **Approach**: Adds "hooks" to unstructured data via scanning and tagging for easier analysis.


#### **Operational Analytics**

- **Definition**: Integrates operational processes with real-time analysis.
- **Features**: Tracks real-time information streams, derives conclusions based on predictive behavior models, and triggers automated responses or alerts.


#### **Data Visualization**

- **Definition**: Represents concepts, ideas, and facts using images or graphics to enhance understanding.
- **Purpose**: Offers visual overviews (e.g., charts, graphs) to interpret underlying data effectively.


#### **Data Mashups**

- **Definition**: Combines data and services to present insights or analysis results in a visual format.


### Activities in Big Data Strategy and Implementation

#### **Activity 1: Define Big Data Strategy and Business Needs**

1. Identify organizational problems to solve and analyze.
2. Specify data sources to use or acquire.
3. Define the timeliness and scope of data.
4. Assess impacts on and relationships with other data structures.
5. Evaluate the influence on existing modeled data.


#### **Activity 2: Select Data Sources**

- Understand key aspects:
    1. Source of the data.
    2. Data format.
    3. Meaning of data elements.
    4. Connections to other data.
    5. Update frequency.
- Evaluate data value and reliability:
    1. Base data.
    2. Granularity.
    3. Consistency.
    4. Reliability.
    5. Inspect and analyze new data sources.


#### **Activity 3: Acquire and Receive Data Sources**

- Capture metadata about sources, such as origin, size, timeliness, and additional contextual information.
- Iteratively identify gaps between current data assets and the required sources.
- Use data science methods (analysis, visualization, mining) to explore sources for defining model inputs or assumptions.
- Evaluate data quality before integration.


#### **Activity 4: Formulate Data Hypotheses and Methods**

- Develop statistical models to identify correlations and trends within and between datasets.
- Model effectiveness depends on:
    1. Input data quality.
    2. Soundness of the model itself.


#### **Activity 5: Integrate and Adjust Data for Analysis**

- Prepare data for analysis by understanding its content, identifying links between data from various sources, and adjusting common data for usage.
- **Methods**:
    1. Use a common key-value model to integrate data.
    2. Employ index scans and joins within database engines.


#### **Activity 6: Explore Data Using Models**

1. **Populate Predictive Models**: Pre-fill models with historical information about customers, markets, products, or triggers beyond those in the model.
2. **Train Models**: Run data through models repeatedly to verify hypotheses, adjust models, and avoid overfitting by training with limited datasets.
3. **Evaluate Models**: After loading data onto the platform, build, evaluate, and validate models using training datasets.
4. **Create Data Visualizations**: Tailor visualizations to the model's purpose, ensuring each visualization answers a question or provides insight.


#### **Activity 7: Deploy and Monitor**

- Deploy models that meet business needs in a practical manner for ongoing monitoring.

1. **Reveal Insights and Discoveries**: Insights should link to actionable items to benefit the organization.
2. **Iterate with Additional Data Sources**: Data science is an iterative process that evolves with new inputs.


### Tools for Big Data Analysis

1. **Massively Parallel Processing (MPP)**:
    
    - Advanced in-database analytics.
    - Unstructured data analysis (Hadoop, MapReduce).
    - Integration of analytical results with operational systems.
    - Cross-media and cross-device data visualization.
    - Semantic linkage of structured and unstructured information.
    - Leveraging IoT data sources.
    - Advanced visualization capabilities.
    - Data scalability.
    - Collaboration across technical tools and platforms.
2. **File-Based Solutions** (e.g., **MapReduce**):
    
    - **Map**: Identify and extract data for analysis.
    - **Shuffle**: Combine data based on required analysis patterns.
    - **Reduce**: Eliminate duplicates or aggregate data to reduce dataset size.
3. **Big Data Technologies**:
    
    - Distributed file-based databases.
    - In-database algorithms.
    - Cloud-based big data solutions.
    - Statistical computing languages.
    - Data visualization toolsets.


### Modeling Techniques

1. **Parsing Modeling**:
    
    - **Descriptive Modeling**: Summarizes or represents data compactly.
    - **Explanatory Modeling**: Validates causal hypotheses in theoretical constructs using statistical models.
2. **Big Data Modeling**:
    
    - Focuses on enabling data population for query performance in data warehouse physical modeling.


### Implementation Guidelines for Big Data

- General rules for managing data warehouses also apply to big data management.
- Unique challenges arise from unknowns like data usage, valuable data identification, and retention duration.

**Strategic Deliverables**:

1. Information lifecycle.
2. Metadata management.
3. Data quality.
4. Data acquisition.
5. Data access and security.
6. Data governance.
7. Data privacy.
8. Learning and adoption.
9. Operations.


### Readiness and Risk Assessment

**Assess Organizational Readiness Based on Key Success Factors**:

1. Business relevance.
2. Business readiness.
3. Economic feasibility.
4. Prototypes.
5. Key decisions around data sourcing, platform development, and resource allocation.
6. External vs. internal data sourcing and operation.
7. Availability of diverse tools and technologies.
8. Retention of skilled professionals and use of alternatives like cloud services or partnerships.
9. Time required for internal talent development vs. delivery timelines.


### Roles in Big Data Implementation

1. **Big Data Platform Architect**: Focuses on hardware, OS, file systems, and services.
2. **Data Ingestion Architect**: Handles data analysis, system records, modeling, and mapping, supporting data ingestion into Hadoop for analysis.
3. **Metadata Specialist**: Manages metadata interfaces, architecture, and content.
4. **Analytics Design Leader**: Guides end-user analysis design, implements best practices, and simplifies results.
5. **Data Scientist**: Provides statistical and computational expertise to design and consult on models and architecture.


### Big Data and Data Science Governance

**Focus Areas**:

- Sourcing.
- Sharing.
- Metadata management.
- Enrichment.
- Access control.


### Standards for Data Science and Visualization

- Establish communities to define and publish visualization standards and guidelines.
- Ensure neutral and professional representation to avoid bias.

**Standards Include**:

1. Analytical paradigms and tools for user groups.
2. Data request processes for new datasets.
3. Workflow standards for dataset processes.
4. Fair and consistent practices, including:
    - Data inclusion and exclusion.
    - Assumptions in models.
    - Statistical validity of results.
    - Validity of result interpretation.
    - Appropriate methodologies.


### Metadata Management in Data Lakes

Metadata must be carefully managed during data extraction to prevent a data lake from turning into a "data swamp."


### Metrics for Big Data

1. **Technical Usage Metrics**: Evaluate tool and technology adoption.
2. **Loading and Scanning Metrics**: Assess extraction rates and user interactions.
3. **Learning and Scenario Metrics**: Common measures include:
    - Number and accuracy of developed models.
    - Revenue generated from identified opportunities.
    - Cost reduction from mitigated threats.

