### MapReduce

MapReduce is a <mark style="background: #FFB8EBA6;">programming model and processing framework</mark> for distributed data processing in Hadoop. It allows for <mark style="background: #BBFABBA6;">parallel processing of large datasets by dividing tasks into two phases</mark>: <mark style="background: #FF5582A6;">Map and Reduce</mark>.

#### Key Concepts

- **Job Tracker:** 
  - Manages resource allocation and job scheduling.
  - Assigns tasks to nodes in the cluster.
  - Monitors the progress of the tasks.

- **Task Tracker:**
  - Executes tasks assigned by the Job Tracker.
  - Processes data and returns the results to the Job Tracker.

- **Map Task:**
  - Processes input data and produces intermediate `<key, value>` <mark style="background: #FFF3A3A6;">pairs</mark>.
  - Each mapper works on a split of data and applies a user-defined function to transform the input into intermediate outputs.

- **Reduce Task:**
  - Merges all intermediate values associated with the same key.
  - Reduces the set of intermediate data to a smaller set of <mark style="background: #ABF7F7A6;">aggregated results</mark>.

#### Word Count Simple Example

Consider a simple example where we want to count the occurrences of each word in a large collection of documents.

1. **Map Phase:**
   - Each mapper processes a split of the input data (documents).
   - For each word in the document, the mapper emits a <mark style="background: #BBFABBA6;">key-value pair</mark> where the key is the word and the value is `1`.

   ```plaintext
   Input Document 1: "cat dog cat"
   Mapper Output: ("cat", 1), ("dog", 1), ("cat", 1)

   Input Document 2: "dog mouse"
   Mapper Output: ("dog", 1), ("mouse", 1)
   ```

2. **Shuffle and Sort Phase:**
   - The framework groups all intermediate key-value pairs by key.
   - Intermediate data is sorted and shuffled to be sent to reducers.

   ```plaintext
   Shuffled Output: ("cat", [1, 1]), ("dog", [1, 1]), ("mouse", [1])
   ```

3. **Reduce Phase:**
   - Each reducer processes a list of values associated with a key.
   - The reducer sums up the values for each key to produce the final result.

   ```plaintext
   Reducer Output: ("cat", 2), ("dog", 2), ("mouse", 1)
   ```

#### Real-Life Example: Analyzing Web Server Logs

##### Problem Statement:
You have a large dataset of web server logs, and you want to determine which pages are the most visited.

##### Web Server Log Format:
Each line in the log file represents a request and looks something like this:
```
192.168.1.1 - - [08/Jul/2023:13:55:36 -0400] "GET /index.html HTTP/1.1" 200 1043
192.168.1.2 - - [08/Jul/2023:13:55:37 -0400] "GET /about.html HTTP/1.1" 200 523
192.168.1.1 - - [08/Jul/2023:13:55:38 -0400] "GET /contact.html HTTP/1.1" 200 643
192.168.1.1 - - [08/Jul/2023:13:55:39 -0400] "GET /index.html HTTP/1.1" 200 1043
```

##### MapReduce Steps

###### 1. Map Phase

**Mapper Input:**
- Each line from the log file.

**Mapper Output:**
- Key: URL (the page visited)
- Value: 1 (indicating a single visit)

**Mapper Function:**
```python
def map_function(log_line):
    # Extract the URL from the log line
    parts = log_line.split(" ")
    url = parts[6]  # Assuming the URL is the 7th part of the log line
    return (url, 1)
```

**Example Mapper Output:**
```
("GET /index.html HTTP/1.1", 1)
("GET /about.html HTTP/1.1", 1)
("GET /contact.html HTTP/1.1", 1)
("GET /index.html HTTP/1.1", 1)
```

###### 2. Shuffle and Sort Phase
- The framework groups all intermediate key-value pairs by key (URL).
- Intermediate data is sorted and shuffled to be sent to reducers.

**Shuffled and Sorted Output:**
```
("GET /index.html HTTP/1.1", [1, 1])
("GET /about.html HTTP/1.1", [1])
("GET /contact.html HTTP/1.1", [1])
```

###### 3. Reduce Phase

**Reducer Input:**
- Key: URL
- Value: List of visit counts

**Reducer Output:**
- Key: URL
- Value: Total number of visits

**Reducer Function:**
```python
def reduce_function(url, visit_counts):
    total_visits = sum(visit_counts)
    return (url, total_visits)
```

**Example Reducer Output:**
```
("GET /index.html HTTP/1.1", 2)
("GET /about.html HTTP/1.1", 1)
("GET /contact.html HTTP/1.1", 1)
```

##### Full MapReduce Job Example in Python (Pseudo-Code)

```python
# Simulated log data
log_data = [
    "192.168.1.1 - - [08/Jul/2023:13:55:36 -0400] \"GET /index.html HTTP/1.1\" 200 1043",
    "192.168.1.2 - - [08/Jul/2023:13:55:37 -0400] \"GET /about.html HTTP/1.1\" 200 523",
    "192.168.1.1 - - [08/Jul/2023:13:55:38 -0400] \"GET /contact.html HTTP/1.1\" 200 643",
    "192.168.1.1 - - [08/Jul/2023:13:55:39 -0400] \"GET /index.html HTTP/1.1\" 200 1043"
]

# Map phase
mapped_data = []
for line in log_data:
    mapped_data.append(map_function(line))

# Shuffle and sort phase
from collections import defaultdict
shuffled_data = defaultdict(list)
for key, value in mapped_data:
    shuffled_data[key].append(value)

# Reduce phase
reduced_data = []
for key, value_list in shuffled_data.items():
    reduced_data.append(reduce_function(key, value_list))

# Print the results
for url, total_visits in reduced_data:
    print(f"{url}: {total_visits} visits")
```

In this real-life example, the MapReduce framework processes a large set of web server logs to determine the most visited pages. The process involves:

1. **Mapping:** Extracting the URL from each log entry and emitting a key-value pair (`<URL, 1>`).
2. **Shuffling and Sorting:** Grouping and sorting the key-value pairs by URL.
3. **Reducing:** Summing the visit counts for each URL to get the total visits.

### YARN (Yet Another Resource Negotiator)

YARN is the resource management layer of Hadoop that allows multiple applications to run on the same cluster while efficiently managing resources.

#### Key Components

- **ResourceManager:**
  - Manages the use of resources across the cluster.
  - Allocates resources to various applications.
  - Keeps track of available and used resources.

- **NodeManager:**
  - Manages resource and task scheduling on a single node.
  - Monitors the resource usage of containers on the node.
  - Reports the resource usage to the ResourceManager.

- **ApplicationMaster:**
  - Manages the lifecycle of a single application.
  - Requests resources from the ResourceManager.
  - Works with the NodeManager to execute and monitor tasks.

#### Example

Consider an example where we have a cluster with multiple applications running concurrently.

1. **ResourceManager:**
   - Receives requests for resources from different applications.
   - Allocates resources based on availability and application priority.

2. **NodeManager:**
   - Monitors and manages resources on individual nodes.
   - Launches containers to execute tasks assigned by the ApplicationMaster.
   - Reports resource status and task progress to the ResourceManager.

3. **ApplicationMaster:**
   - Manages a specific application (e.g., a MapReduce job).
   - Negotiates resources with the ResourceManager.
   - Coordinates the execution of tasks on various NodeManagers.

#### Scenario

- Two applications, App1 (MapReduce job) and App2 (Spark job), are submitted to the YARN cluster.
- **ResourceManager** allocates resources to both applications based on their requirements and cluster availability.
- **NodeManagers** on different nodes launch containers to execute tasks for App1 and App2.
- **ApplicationMaster** for App1 requests additional containers from the ResourceManager as needed to complete the MapReduce job.
- **ApplicationMaster** for App2 coordinates the execution of Spark tasks on the allocated containers.
