### Producer API

#### Message Sending Workflow

Kafka's Producer sends messages asynchronously. The message-sending process involves two threads—`main` thread and `Sender` thread—as well as a shared variable—`RecordAccumulator`. The `main` thread sends messages to the `RecordAccumulator`, and the `Sender` thread continuously pulls messages from the `RecordAccumulator` and sends them to the Kafka broker.

**Related Parameters:**
- `batch.size`: The sender sends data only after accumulating data up to `batch.size`.
- `linger.ms`: If the data does not reach `batch.size`, the sender will send the data after waiting for `linger.ms`.

#### Asynchronous Sending API

1. **Import Dependencies:**
   ```xml
   <dependency>
       <groupId>org.apache.kafka</groupId>
       <artifactId>kafka-clients</artifactId>
       <version>0.11.0.0</version>
   </dependency>
   ```

2. **Write Code:**
   Required Classes:
   - `KafkaProducer`: Creates a producer object to send data.
   - `ProducerConfig`: Retrieves a series of configuration parameters.
   - `ProducerRecord`: Encapsulates each piece of data into a `ProducerRecord` object.

   **1. API without Callback Function:**
   ```java
   package com.a.kafka;

   import org.apache.kafka.clients.producer.*;
   import java.util.Properties;
   import java.util.concurrent.ExecutionException;

   public class CustomProducer {

       public static void main(String[] args) throws ExecutionException, InterruptedException {
           Properties props = new Properties();

           // Kafka cluster, broker-list
           props.put("bootstrap.servers", "hadoop102:9092");
           props.put("acks", "all");
           props.put("retries", 1);  // Retry count
           props.put("batch.size", 16384);  // Batch size
           props.put("linger.ms", 1);  // Wait time
           props.put("buffer.memory", 33554432);  // RecordAccumulator buffer size
           props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
           props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

           Producer<String, String> producer = new KafkaProducer<>(props);

           for (int i = 0; i < 100; i++) {
               producer.send(new ProducerRecord<>("first", Integer.toString(i), Integer.toString(i)));
           }

           producer.close();
       }
   }
   ```

   **2. API with Callback Function:**
   The callback function is called when the producer receives an ack. It is an asynchronous call with two parameters, `RecordMetadata` and `Exception`. If `Exception` is `null`, the message was sent successfully; otherwise, it failed.

   **Note:** If the message fails to send, it will automatically retry, so no need to manually retry in the callback.

   ```java
   package com.a.kafka;

   import org.apache.kafka.clients.producer.*;
   import java.util.Properties;
   import java.util.concurrent.ExecutionException;

   public class CustomProducer {

       public static void main(String[] args) throws ExecutionException, InterruptedException {
           Properties props = new Properties();

           // Kafka cluster, broker-list
           props.put("bootstrap.servers", "hadoop102:9092");
           props.put("acks", "all");
           props.put("retries", 1);  // Retry count
           props.put("batch.size", 16384);  // Batch size
           props.put("linger.ms", 1);  // Wait time
           props.put("buffer.memory", 33554432);  // RecordAccumulator buffer size
           props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
           props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

           Producer<String, String> producer = new KafkaProducer<>(props);

           for (int i = 0; i < 100; i++) {
               producer.send(new ProducerRecord<>("first", Integer.toString(i), Integer.toString(i)), new Callback() {
                   @Override
                   public void onCompletion(RecordMetadata metadata, Exception exception) {
                       if (exception == null) {
                           System.out.println("Success -> " + metadata.offset());
                       } else {
                           exception.printStackTrace();
                       }
                   }
               });
           }
           producer.close();
       }
   }
   ```

#### Synchronous Sending API

Synchronous sending means that after a message is sent, the current thread is blocked until an ack is received. Since the `send` method returns a `Future` object, you can achieve synchronous sending by calling the `get` method on the `Future` object.

```java
package com.a.kafka;

import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.Producer;
import org.apache.kafka.clients.producer.ProducerRecord;

import java.util.Properties;
import java.util.concurrent.ExecutionException;

public class CustomProducer {

    public static void main(String[] args) throws ExecutionException, InterruptedException {
        Properties props = new Properties();

        // Kafka cluster, broker-list
        props.put("bootstrap.servers", "hadoop102:9092");
        props.put("acks", "all");
        props.put("retries", 1);  // Retry count
        props.put("batch.size", 16384);  // Batch size
        props.put("linger.ms", 1);  // Wait time
        props.put("buffer.memory", 33554432);  // RecordAccumulator buffer size
        props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
        props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

        Producer<String, String> producer = new KafkaProducer<>(props);
        for (int i = 0; i < 100; i++) {
            producer.send(new ProducerRecord<>("first", Integer.toString(i), Integer.toString(i))).get();
        }
        producer.close();
    }
}
```

### Consumer API

When consuming data, Kafka's persistence ensures that data reliability is easily guaranteed, so there is no need to worry about data loss. However, since consumers may experience failures, such as power outages, during consumption, they need to resume from where they left off after recovery. Therefore, offset maintenance is a crucial consideration when consuming data.

#### Automatic Offset Committing

1. **Import Dependencies:**
   ```xml
   <dependency>
       <groupId>org.apache.kafka</groupId>
       <artifactId>kafka-clients</artifactId>
       <version>0.11.0.0</version>
   </dependency>
   ```

2. **Write Code:**
   Required Classes:
   - `KafkaConsumer`: Creates a consumer object to consume data.
   - `ConsumerConfig`: Retrieves a series of configuration parameters.
   - `ConsumerRecord`: Encapsulates each piece of data into a `ConsumerRecord` object.

   Kafka provides an automatic offset committing feature to allow you to focus on your business logic.

   **Related Parameters for Automatic Offset Committing:**
   - `enable.auto.commit`: Enables or disables automatic offset committing.
   - `auto.commit.interval.ms`: The time interval for automatically committing the offset.

   **Code for Automatic Offset Committing:**
   ```java
   package com.a.kafka;

   import org.apache.kafka.clients.consumer.ConsumerRecord;
   import org.apache.kafka.clients.consumer.ConsumerRecords;
   import org.apache.kafka.clients.consumer.KafkaConsumer;

   import java.util.Arrays;
   import java.util.Properties;

   public class CustomConsumer {

       public static void main(String[] args) {
           Properties props = new Properties();

           // Kafka cluster
           props.put("bootstrap.servers", "hadoop102:9092");
           props.put("group.id", "test");
           props.put("enable.auto.commit", "true");
           props.put("auto.commit.interval.ms", "1000");
           props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
           props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");

           KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);
           consumer.subscribe(Arrays.asList("first"));

           while (true) {
               ConsumerRecords<String, String> records = consumer.poll(100);
               for (ConsumerRecord<String, String> record : records) {
                   System.out.printf("offset = %d, key = %s, value = %s%n", record.offset(), record.key(), record.value());
               }
           }
       }
   }
   ```

#### Manual Offset Committing

Although automatic offset committing is simple and convenient, it commits offsets based on time, making it difficult for developers to control the timing. Therefore, Kafka also provides a manual offset committing API. There are two methods for manual offset committing: `commitSync` (synchronous commit) and `commitAsync` (asynchronous commit). Both methods commit the highest offset of the polled batch of data; the difference is that `commitSync` blocks the current thread until the commit succeeds and automatically retries in case of failure, while `commitAsync` does not retry, which may result in a failed commit.

**1. Synchronous Offset Committing:**
   Since synchronous offset committing has a retry mechanism, it is more reliable. Below is an example of synchronous offset committing.

   ```java
   package com.a.kafka.consumer;

   import org.apache.kafka.clients.consumer.ConsumerRecord;
   import org.apache.kafka.clients.consumer.ConsumerRecords;
   import org.apache.kafka.clients

.consumer.KafkaConsumer;

   import java.util.Arrays;
   import java.util.Properties;

   public class CustomConsumer {

       public static void main(String[] args) {
           Properties props = new Properties();

           // Kafka cluster
           props.put("bootstrap.servers", "hadoop102:9092");
           props.put("group.id", "test");
           props.put("enable.auto.commit", "false");  // Disable automatic offset committing
           props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
           props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");

           KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);
           consumer.subscribe(Arrays.asList("first"));

           while (true) {
               ConsumerRecords<String, String> records = consumer.poll(100);
               for (ConsumerRecord<String, String> record : records) {
                   System.out.printf("offset = %d, key = %s, value = %s%n", record.offset(), record.key(), record.value());
               }
               consumer.commitSync();  // Synchronous commit, blocks current thread until offset is committed
           }
       }
   }
   ```

**2. Asynchronous Offset Committing:**
   Although synchronous offset committing is more reliable, it blocks the current thread until the commit succeeds, significantly affecting throughput. Therefore, asynchronous offset committing is often preferred.

   **Code for Asynchronous Offset Committing:**
   ```java
   package com.a.kafka.consumer;

   import org.apache.kafka.clients.consumer.*;
   import org.apache.kafka.common.TopicPartition;

   import java.util.Arrays;
   import java.util.Map;
   import java.util.Properties;

   public class CustomConsumer {

       public static void main(String[] args) {
           Properties props = new Properties();

           // Kafka cluster
           props.put("bootstrap.servers", "hadoop102:9092");
           props.put("group.id", "test");
           props.put("enable.auto.commit", "false");  // Disable automatic offset committing
           props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
           props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");

           KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);
           consumer.subscribe(Arrays.asList("first"));

           while (true) {
               ConsumerRecords<String, String> records = consumer.poll(100);
               for (ConsumerRecord<String, String> record : records) {
                   System.out.printf("offset = %d, key = %s, value = %s%n", record.offset(), record.key(), record.value());
               }
               consumer.commitAsync(new OffsetCommitCallback() {
                   @Override
                   public void onComplete(Map<TopicPartition, OffsetAndMetadata> offsets, Exception exception) {
                       if (exception != null) {
                           System.err.println("Commit failed for " + offsets);
                       }
                   }
               });
           }
       }
   }
   ```

**3. Analyzing Data Loss and Duplication:**
   Whether using synchronous or asynchronous offset committing, there is a risk of data loss or duplication. Committing the offset before consumption may result in data loss, while consuming before committing the offset may result in data duplication.

#### Custom Offset Storage

Before Kafka version 0.9, offsets were stored in Zookeeper. Starting from version 0.9, offsets are stored in an internal Kafka topic by default. Kafka also allows custom offset storage.

Maintaining offsets is quite complex because it requires consideration of consumer rebalancing. Rebalancing occurs when a new consumer joins a consumer group, an existing consumer leaves, or the partitions of the subscribed topic change, triggering partition reassignment. After rebalancing, the partitions each consumer consumes may change. Therefore, consumers must first obtain the partitions assigned to them and locate the most recently committed offset for each partition to resume consumption.

To implement custom offset storage, the `ConsumerRebalanceListener` interface is used. Below is an example where the methods for committing and retrieving offsets need to be implemented according to the chosen offset storage system.

```java
package com.a.kafka.consumer;

import org.apache.kafka.clients.consumer.*;
import org.apache.kafka.common.TopicPartition;

import java.util.*;

public class CustomConsumer {

    private static Map<TopicPartition, Long> currentOffset = new HashMap<>();

    public static void main(String[] args) {
        Properties props = new Properties();

        // Kafka cluster
        props.put("bootstrap.servers", "hadoop102:9092");
        props.put("group.id", "test");
        props.put("enable.auto.commit", "false");  // Disable automatic offset committing
        props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");

        KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);

        consumer.subscribe(Arrays.asList("first"), new ConsumerRebalanceListener() {
            
            // Called before rebalance
            @Override
            public void onPartitionsRevoked(Collection<TopicPartition> partitions) {
                commitOffset(currentOffset);
            }

            // Called after rebalance
            @Override
            public void onPartitionsAssigned(Collection<TopicPartition> partitions) {
                currentOffset.clear();
                for (TopicPartition partition : partitions) {
                    consumer.seek(partition, getOffset(partition));  // Resume consumption from the most recently committed offset
                }
            }
        });

        while (true) {
            ConsumerRecords<String, String> records = consumer.poll(100);
            for (ConsumerRecord<String, String> record : records) {
                System.out.printf("offset = %d, key = %s, value = %s%n", record.offset(), record.key(), record.value());
                currentOffset.put(new TopicPartition(record.topic(), record.partition()), record.offset());
            }
            commitOffset(currentOffset);  // Asynchronous commit
        }
    }

    // Retrieve the most recent offset for a partition
    private static long getOffset(TopicPartition partition) {
        // Implement offset retrieval logic here
        return 0;
    }

    // Commit the offsets for all partitions of this consumer
    private static void commitOffset(Map<TopicPartition, Long> currentOffset) {
        // Implement offset commit logic here
    }
}
```

### Custom Interceptor

#### Interceptor Principle

Producer interceptors were introduced in Kafka version 0.10, primarily used for implementing client-side custom control logic. For producers, interceptors allow users to make custom modifications to messages before they are sent and before the producer's callback logic is triggered. Producers can specify multiple interceptors to act sequentially on the same message, forming an interceptor chain. The `ProducerInterceptor` interface is defined in `org.apache.kafka.clients.producer.ProducerInterceptor` and includes the following methods:

- `configure(configs)`: Called when configuration information is obtained and data is initialized.
- `onSend(ProducerRecord)`: This method is encapsulated within the `KafkaProducer.send` method and runs in the user main thread. The producer ensures that this method is called before the message is serialized and the partition is computed. Users can perform any operations on the message in this method, but it is recommended not to modify the message's topic and partition, as this would affect the target partition's computation.
- `onAcknowledgement(RecordMetadata, Exception)`: Called after the message is successfully sent to the Kafka broker from the `RecordAccumulator`, or in case of a failure during sending. This method usually runs before the producer's callback logic is triggered. `onAcknowledgement` runs in the producer's IO thread, so heavy logic should not be included in this method as it may slow down the producer's message-sending efficiency.
- `close()`: Closes the interceptor, mainly for resource cleanup.

As mentioned, interceptors may run in multiple threads, so users need to ensure thread safety in their implementation. Additionally, if multiple interceptors are specified, the producer will call them in the specified order and only capture and log any exceptions thrown by the interceptors without propagating them. This behavior should be carefully considered during usage.

#### Interceptor Case Study

1. **Requirement:** Implement a simple interceptor chain consisting of two interceptors. The first interceptor adds a timestamp to the beginning of the message value before sending, and the second interceptor updates the count of successfully sent and failed messages after sending.

2. **Implementation:**

   **1. Add Timestamp Interceptor:**
   ```java
   package com.a.kafka.interceptor;

   import org.apache.kafka.clients.producer.ProducerInterceptor;
   import org.apache.kafka.clients.producer.ProducerRecord;
   import org.apache.kafka.clients.producer.RecordMetadata;

   public class TimeInterceptor implements ProducerInterceptor<String, String> {

       @Override
       public void configure(Map<String, ?> configs) {
       }

       @Override
       public ProducerRecord<String, String> onSend(ProducerRecord<String, String> record) {
           // Create a new record, adding the timestamp to the beginning of the message value
           return new ProducerRecord<>(record.topic(), record.partition(), record.timestamp(), record.key(),
                   System.currentTimeMillis() + "," + record.value());
       }

       @Override
       public void onAcknowledgement(RecordMetadata metadata, Exception exception) {
       }

       @Override
       public void close() {
       }
   }
   ```

   **2. Count Successful and Failed Message Sends:**
   ```java
   package com.a.kafka.interceptor;

   import org.apache.kafka.clients.producer.ProducerInterceptor;
   import org.apache.kafka.clients.producer.ProducerRecord;
   import org.apache.kafka.clients.producer.RecordMetadata;

   public class CounterInterceptor implements ProducerInterceptor<String, String> {
       private int errorCounter = 0;
       private int successCounter = 0;

       @Override
       public

 void configure(Map<String, ?> configs) {
       }

       @Override
       public ProducerRecord<String, String> onSend(ProducerRecord<String, String> record) {
           return record;
       }

       @Override
       public void onAcknowledgement(RecordMetadata metadata, Exception exception) {
           // Count successful and failed sends
           if (exception == null) {
               successCounter++;
           } else {
               errorCounter++;
           }
       }

       @Override
       public void close() {
           // Save results
           System.out.println("Successfully sent: " + successCounter);
           System.out.println("Failed sends: " + errorCounter);
       }
   }
   ```

   **3. Producer Main Program:**
   ```java
   package com.a.kafka.interceptor;

   import org.apache.kafka.clients.producer.KafkaProducer;
   import org.apache.kafka.clients.producer.Producer;
   import org.apache.kafka.clients.producer.ProducerConfig;
   import org.apache.kafka.clients.producer.ProducerRecord;

   import java.util.ArrayList;
   import java.util.List;
   import java.util.Properties;

   public class InterceptorProducer {

       public static void main(String[] args) throws Exception {
           // 1. Set configuration information
           Properties props = new Properties();
           props.put("bootstrap.servers", "hadoop102:9092");
           props.put("acks", "all");
           props.put("retries", 3);
           props.put("batch.size", 16384);
           props.put("linger.ms", 1);
           props.put("buffer.memory", 33554432);
           props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
           props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

           // 2. Build interceptor chain
           List<String> interceptors = new ArrayList<>();
           interceptors.add("com.a.kafka.interceptor.TimeInterceptor");
           interceptors.add("com.a.kafka.interceptor.CounterInterceptor");
           props.put(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG, interceptors);

           String topic = "first";
           Producer<String, String> producer = new KafkaProducer<>(props);

           // 3. Send messages
           for (int i = 0; i < 10; i++) {
               ProducerRecord<String, String> record = new ProducerRecord<>(topic, "message" + i);
               producer.send(record);
           }

           // 4. Close producer to trigger interceptor's close method
           producer.close();
       }
   }
   ```

3. **Testing:**
   - Start a consumer on Kafka and then run the client Java program.
   ```bash
   [a@hadoop102 kafka]$ bin/kafka-console-consumer.sh \
   --bootstrap-server hadoop102:9092 --from-beginning --topic first
   ```

   Example output:
   ```
   1501904047034,message0
   1501904047225,message1
   1501904047230,message2
   1501904047234,message3
   1501904047236,message4
   1501904047240,message5
   1501904047243,message6
   1501904047246,message7
   1501904047249,message8
   1501904047252,message9
   ```