### Workflow and File Storage Mechanism

In Kafka, messages are categorized by topics. Producers produce messages, and consumers consume messages, both targeting specific topics. A topic is a logical concept, whereas a partition is a physical concept. Each partition corresponds to a log file where the data produced by the producer is stored. The producer's data is continuously appended to the end of this log file, and each piece of data has its own offset. Each consumer in a consumer group records the offset of the last consumed message to resume consumption from that point in case of a failure.

To prevent log files from becoming too large and inefficient for data location, Kafka uses sharding and indexing mechanisms, dividing each partition into multiple segments. Each segment corresponds to two files: a `.index` file and a `.log` file. These files are located in a folder named after the topic and partition number. For example, if the topic `first` has three partitions, the corresponding folders would be `first-0`, `first-1`, and `first-2`.

```
00000000000000000000.index
00000000000000000000.log
00000000000000170410.index
00000000000000170410.log
00000000000000239430.index
00000000000000239430.log
```

The `.index` file stores index information, while the `.log` file stores the actual data. Metadata in the index file points to the physical offset of the message in the corresponding log file. Below is a diagram illustrating the structure of the index and log files.

### Kafka Producer

#### Partition Strategy

1. **Reasons for Partitioning:**
   - **Scalability:** Each partition can be adjusted to fit the machine it resides on, and a topic can consist of multiple partitions, allowing the cluster to handle any amount of data.
   - **Increased Concurrency:** Partitioning allows read and write operations at the partition level, increasing throughput.

2. **Partitioning Principles:**
   When a producer sends data, it is encapsulated into a `ProducerRecord` object.

   - If a partition is specified, the specified value is used as the partition value.
   - If no partition is specified but a key is provided, the partition is determined by taking the hash of the key modulo the number of partitions in the topic.
   - If neither a partition nor a key is specified, a random integer is generated during the first call, incremented on subsequent calls, and then modulo the total number of available partitions to get the partition value (round-robin algorithm).

#### Data Reliability Assurance

To ensure that the data sent by the producer is reliably delivered to the designated topic, each partition of the topic must send an acknowledgment (ack) to the producer after receiving the data. If the producer receives the ack, it proceeds to the next round of sending; otherwise, it retransmits the data.

1. **Replica Data Synchronization Strategy:**

| Strategy                                | Advantage                                                 | Disadvantage                                              |
|-----------------------------------------|-----------------------------------------------------------|-----------------------------------------------------------|
| Majority of replicas synced before ack  | Low latency                                               | Requires 2n+1 replicas to tolerate n node failures         |
| All replicas synced before ack          | Can tolerate n node failures with only n+1 replicas       | High latency                                               |

Kafka uses the second strategy for the following reasons:
   - To tolerate n node failures, the first strategy requires 2n+1 replicas, while the second only requires n+1. Given the large amount of data in each Kafka partition, the first strategy would result in significant data redundancy.
   - Although the second strategy has higher network latency, the impact on Kafka is minimal.

2. **ISR (In-Sync Replica):**
   After choosing the second strategy, consider this scenario: The leader receives data, and all followers begin syncing. If one follower fails to sync with the leader due to some issue, the leader will wait until it finishes before sending an ack. How can this problem be solved?

   The leader maintains a dynamic in-sync replica set (ISR), which consists of followers that are in sync with the leader. After a follower in the ISR finishes syncing, the leader sends an ack. If a follower fails to sync with the leader within a specified time (set by the `replica.lag.time.max.ms` parameter), it is removed from the ISR. If the leader fails, a new leader is elected from the ISR.

3. **Acknowledgment Mechanism:**
   For some less critical data where reliability is not a top priority and a small amount of data loss is tolerable, it is unnecessary to wait for all ISR followers to successfully receive the data. Kafka provides three reliability levels for users to choose from based on their reliability and latency requirements:

   - `acks=0`: The producer does not wait for the broker's ack, providing the lowest latency. However, if the broker fails before writing to disk, data may be lost.
   - `acks=1`: The producer waits for the broker's ack after the partition leader has written to disk. If the leader fails before the follower syncs, data may be lost.
   - `acks=-1 (all)`: The producer waits for the broker's ack after both the partition leader and followers have written to disk. However, if the leader fails after followers sync but before sending the ack, data duplication may occur.

4. **Fault Handling Details:**

   - **LEO (Log End Offset):** The largest offset in each replica.
   - **HW (High Watermark):** The largest offset visible to consumers, representing the smallest LEO in the ISR.

   **Follower Failure:** When a follower fails, it is temporarily removed from the ISR. Once it recovers, the follower reads its last HW from local disk and truncates the log file above that HW. It then starts syncing with the leader. Once its LEO is greater than or equal to the partition's HW, it re-joins the ISR.

   **Leader Failure:** When the leader fails, a new leader is elected from the ISR. To ensure consistency across replicas, other followers truncate their log files above the HW before syncing with the new leader. This only ensures data consistency across replicas but does not guarantee data loss or duplication.

#### Exactly Once Semantics

Setting the server's ACK level to -1 ensures that data from the producer to the server is not lost, achieving At Least Once semantics. Conversely, setting the ACK level to 0 ensures that each message is sent only once, achieving At Most Once semantics.

- **At Least Once:** Ensures data is not lost but cannot prevent duplication.
- **At Most Once:** Prevents duplication but cannot guarantee no data loss.

For critical information, such as transaction data, downstream data consumers require both no duplication and no data loss, i.e., Exactly Once semantics. Before Kafka 0.11, this was impossible, and deduplication had to be handled downstream. With multiple downstream applications, each needed to deduplicate data, significantly impacting performance.

Kafka 0.11 introduced idempotence, meaning that regardless of how many times a producer sends duplicate data, the server persists only one copy. Idempotence combined with At Least Once semantics results in Exactly Once semantics:

```
At Least Once + Idempotence = Exactly Once
```

To enable idempotence, set `enable.idompotence=true` in the producer's configuration. Kafka's idempotence implementation allocates a PID (Producer ID) to the producer during initialization. Messages sent to the same partition carry a sequence number. The broker caches `<PID, Partition, SeqNumber>` and only persists one message when a duplicate key is detected.

However, PID changes upon restart, and different partitions have different keys, so idempotence cannot guarantee Exactly Once across partitions or sessions.

### Kafka Consumer

#### Consumption Method

Consumers use a pull model to read data from brokers. The push model is unsuitable for consumers with varying consumption rates, as the broker controls the message send rate, which can lead to consumers being overwhelmed, causing service denial and network congestion. The pull model allows consumers to consume messages at an appropriate rate based on their capabilities.

A drawback of the pull model is that if Kafka has no data, consumers may enter a loop, repeatedly returning empty data. To address this, Kafka's consumers can pass a timeout parameter when consuming data. If no data is available, the consumer waits for a specified time (timeout) before returning.

#### Partition Assignment Strategy

In a consumer group with multiple consumers and a topic with multiple partitions, partition assignment is required to determine which consumer consumes which partition.

Kafka offers two partition assignment strategies: RoundRobin and Range.

1. **RoundRobin:** Consumers are assigned partitions in a round-robin manner.
2. **Range:** Consumers are assigned partitions based on range.

#### Offset Management

Since consumers may experience failures, such as power outages, during consumption, they need to resume consumption from where they left off after recovery. Therefore, consumers must record the offset of the last consumed message in real-time.

Before Kafka 0.9, offsets were stored in Zookeeper by default. From Kafka 0.9 onwards, offsets are stored in a built-in Kafka topic called `__consumer_offsets`.

1. **Modify Configuration File `consumer.properties`:**
```properties
   exclude.internal.topics=false
```

2. **Read Offset:**

   - Versions before 0.11.0.0:
     ```bash
     bin/kafka-console-consumer.sh --topic __consumer_offsets --zookeeper hadoop102:2181 --formatter "k

afka.coordinator.GroupMetadataManager\$OffsetsMessageFormatter" --consumer.config config/consumer.properties --from-beginning
     ```

   - Versions 0.11.0.0 and later:
     ```bash
     bin/kafka-console-consumer.sh --topic __consumer_offsets --zookeeper hadoop102:2181 --formatter "kafka.coordinator.group.GroupMetadataManager\$OffsetsMessageFormatter" --consumer.config config/consumer.properties --from-beginning
     ```

#### Consumer Group Case Study

1. **Requirement:** Test that in the same consumer group, only one consumer can consume messages at a time.

2. **Implementation:**
   1. Modify the `group.id` attribute in the `consumer.properties` configuration file on both `hadoop102` and `hadoop103` to any group name.
      ```bash
      [atguigu@hadoop103 config]$ vi consumer.properties
      group.id=atguigu
      ```

   2. Start consumers on `hadoop102` and `hadoop103`:
      ```bash
      [atguigu@hadoop102 kafka]$ bin/kafka-console-consumer.sh \
      --zookeeper hadoop102:2181 --topic first --consumer.config config/consumer.properties

      [atguigu@hadoop103 kafka]$ bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic first --consumer.config config/consumer.properties
      ```

   3. Start a producer on `hadoop104`:
      ```bash
      [atguigu@hadoop104 kafka]$ bin/kafka-console-producer.sh \
      --broker-list hadoop102:9092 --topic first
      >hello world
      ```

   4. Observe the receivers on `hadoop102` and `hadoop103`. Only one consumer will receive messages at any given time.

### Efficient Data Read and Write

1. **Sequential Disk Writes:**  
   When the Kafka producer generates data, it is written to the log file, continuously appended to the file's end, enabling sequential writes. According to Kafka's documentation, the same disk can achieve a write speed of 600MB/s for sequential writes compared to just 100KB/s for random writes. This speed difference is due to the disk's mechanical design, where sequential writes save significant time by reducing the need for head repositioning.

2. **Zero-Copy Technology:**  
   Kafka utilizes zero-copy technology to optimize data transfer between the disk and network, improving performance.

### The Role of Zookeeper in Kafka

In a Kafka cluster, one broker is elected as the Controller, responsible for managing broker up/down events, partition replica assignments, leader elections, and more. The Controller relies on Zookeeper for these management tasks.

Below is the process of partition leader election.

### Kafka Transactions

Starting with version 0.11, Kafka introduced transaction support. Transactions ensure that Kafka can provide exactly-once semantics, allowing production and consumption across partitions and sessions to succeed or fail as a whole.

#### Producer Transactions

To achieve cross-partition and cross-session transactions, a globally unique Transaction ID must be introduced, binding the Producer's PID to the Transaction ID. This binding allows the Producer to recover its original PID after a restart.

To manage transactions, Kafka introduces a new component called the Transaction Coordinator. The Producer interacts with the Transaction Coordinator to obtain the transaction status associated with the Transaction ID. The Transaction Coordinator is also responsible for writing all transactions to an internal Kafka topic. This ensures that even if the entire service restarts, the ongoing transaction status can be restored and continued.

#### Consumer Transactions

While the aforementioned transaction mechanism primarily addresses the Producer side, transaction guarantees for Consumers are relatively weaker, particularly in ensuring that commit information is accurately consumed. This limitation arises because Consumers can access any information via offsets, and different segment file lifecycles may cause messages within the same transaction to be deleted after a restart.
