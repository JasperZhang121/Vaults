### Installation and Deployment

#### Cluster Planning

| Server       | Services                  |
|--------------|---------------------------|
| hadoop102    | Zookeeper, Kafka           |
| hadoop103    | Zookeeper, Kafka           |
| hadoop104    | Zookeeper, Kafka           |

#### Downloading the JAR Package

You can download Kafka from the official website:  
[Kafka Downloads](http://kafka.apache.org/downloads.html)

#### Cluster Deployment

1. **Extract the Installation Package:**
   ```bash
   [a@hadoop102 software]$ tar -zxvf kafka_2.11-0.11.0.0.tgz -C /opt/module/
   ```

2. **Rename the Extracted Directory:**
   ```bash
   [a@hadoop102 module]$ mv kafka_2.11-0.11.0.0/ kafka
   ```

3. **Create a `logs` Directory in `/opt/module/kafka`:**
   ```bash
   [a@hadoop102 kafka]$ mkdir logs
   ```

4. **Modify the Configuration File:**
   ```bash
   [a@hadoop102 kafka]$ cd config/
   [a@hadoop102 config]$ vi server.properties
   ```
   Add or modify the following configuration:

   ```properties
   # Globally unique broker ID, must not be duplicated
   broker.id=0

   # Enable topic deletion
   delete.topic.enable=true

   # Number of threads handling network requests
   num.network.threads=3

   # Number of threads handling disk I/O
   num.io.threads=8

   # Size of the send socket buffer
   socket.send.buffer.bytes=102400

   # Size of the receive socket buffer
   socket.receive.buffer.bytes=102400

   # Maximum size of the request socket buffer
   socket.request.max.bytes=104857600

   # Path for Kafka log storage
   log.dirs=/opt/module/kafka/logs

   # Number of partitions for the topic on this broker
   num.partitions=1

   # Number of threads for data recovery and cleanup
   num.recovery.threads.per.data.dir=1

   # Maximum retention time for segment files, after which they are deleted
   log.retention.hours=168

   # Zookeeper cluster connection addresses
   zookeeper.connect=hadoop102:2181,hadoop103:2181,hadoop104:2181
   ```

5. **Set Environment Variables:**
   ```bash
   [a@hadoop102 module]$ sudo vi /etc/profile
   ```
   Add the following lines:
   ```bash
   # KAFKA_HOME
   export KAFKA_HOME=/opt/module/kafka
   export PATH=$PATH:$KAFKA_HOME/bin
   ```

   Apply the changes:
   ```bash
   [a@hadoop102 module]$ source /etc/profile
   ```

6. **Distribute the Installation Package:**
   ```bash
   [a@hadoop102 module]$ xsync kafka/
   ```
   *Note: After distribution, remember to configure environment variables on the other machines.*

7. **Modify the Configuration on `hadoop103` and `hadoop104`:**
   - On `hadoop103`, set `broker.id=1`.
   - On `hadoop104`, set `broker.id=2`.
   *Note: The `broker.id` must be unique and not duplicated.*

8. **Start the Cluster:**
   - Start Kafka on each node (`hadoop102`, `hadoop103`, `hadoop104`):
     ```bash
     [a@hadoop102 kafka]$ bin/kafka-server-start.sh -daemon config/server.properties
     [a@hadoop103 kafka]$ bin/kafka-server-start.sh -daemon config/server.properties
     [a@hadoop104 kafka]$ bin/kafka-server-start.sh -daemon config/server.properties
     ```

9. **Stop the Cluster:**
   - Stop Kafka on each node:
     ```bash
     [a@hadoop102 kafka]$ bin/kafka-server-stop.sh stop
     [a@hadoop103 kafka]$ bin/kafka-server-stop.sh stop
     [a@hadoop104 kafka]$ bin/kafka-server-stop.sh stop
     ```

10. **Kafka Cluster Start Script:**
    - You can use the following script to start Kafka on all nodes:
      ```bash
      for i in hadoop102 hadoop103 hadoop104
      do
        echo "========== $i ==========" 
        ssh $i '/opt/module/kafka/bin/kafka-server-start.sh -daemon /opt/module/kafka/config/server.properties'
      done
      ```

### Kafka Command Line Operations

1. **List All Topics on the Current Server:**
   ```bash
   [a@hadoop102 kafka]$ bin/kafka-topics.sh --zookeeper hadoop102:2181 --list
   ```

2. **Create a Topic:**
   ```bash
   [a@hadoop102 kafka]$ bin/kafka-topics.sh --zookeeper hadoop102:2181 --create --replication-factor 3 --partitions 1 --topic first
   ```
   *Options:*
   - `--topic`: Defines the topic name.
   - `--replication-factor`: Defines the number of replicas.
   - `--partitions`: Defines the number of partitions.

3. **Delete a Topic:**
   ```bash
   [a@hadoop102 kafka]$ bin/kafka-topics.sh --zookeeper hadoop102:2181 --delete --topic first
   ```
   *Note: The `server.properties` file must have `delete.topic.enable=true` set, otherwise the topic will only be marked for deletion.*

4. **Send a Message:**
   ```bash
   [a@hadoop102 kafka]$ bin/kafka-console-producer.sh --broker-list hadoop102:9092 --topic first
   >hello world
   >a a
   ```

5. **Consume Messages:**
   ```bash
   [a@hadoop102 kafka]$ bin/kafka-console-consumer.sh --zookeeper hadoop102:2181 --topic first

   [a@hadoop102 kafka]$ bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic first

   [a@hadoop102 kafka]$ bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --from-beginning --topic first
   ```
   *Option:*
   - `--from-beginning`: Reads all past data from the topic.

6. **View Details of a Specific Topic:**
   ```bash
   [a@hadoop102 kafka]$ bin/kafka-topics.sh --zookeeper hadoop102:2181 --describe --topic first
   ```

7. **Change the Number of Partitions:**
   ```bash
   [a@hadoop102 kafka]$ bin/kafka-topics.sh --zookeeper hadoop102:2181 --alter --topic first --partitions 6
   ```