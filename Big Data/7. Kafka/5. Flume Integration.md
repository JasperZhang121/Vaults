1. **Configure Flume (`flume-kafka.conf`):**
   ```properties
   # Define source, sink, and channel
   a1.sources = r1
   a1.sinks = k1
   a1.channels = c1

   # Source configuration
   a1.sources.r1.type = exec
   a1.sources.r1.command = tail -F -c +0 /opt/module/data/flume.log
   a1.sources.r1.shell = /bin/bash -c

   # Sink configuration
   a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink
   a1.sinks.k1.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092
   a1.sinks.k1.kafka.topic = first
   a1.sinks.k1.kafka.flumeBatchSize = 20
   a1.sinks.k1.kafka.producer.acks = 1
   a1.sinks.k1.kafka.producer.linger.ms = 1

   # Channel configuration
   a1.channels.c1.type = memory
   a1.channels.c1.capacity = 1000
   a1.channels.c1.transactionCapacity = 100

   # Bind source and sink to the channel
   a1.sources.r1.channels = c1
   a1.sinks.k1.channel = c1
   ```

   This configuration sets up Flume to collect data from a log file and send it to a Kafka topic. The `exec` source runs a shell command to tail a log file, ensuring real-time data capture. The data is then routed through an in-memory channel to a Kafka sink, which pushes the data to the Kafka cluster.

2. **Start Kafka Consumer in IDEA:**
   Start a Kafka consumer in your development environment (IDEA) to monitor the incoming data. This step is crucial for validating that the Kafka cluster is correctly receiving and storing the data sent from Flume.

3. **Start Flume from the Flume Root Directory:**
   ```bash
   $ bin/flume-ng agent -c conf/ -n a1 -f jobs/flume-kafka.conf
   ```
   Launching the Flume agent with the specified configuration allows Flume to begin processing data. This step initiates the data flow from the log file through Flume to Kafka.

4. **Append Data to `flume.log` and Check Kafka Consumer:**
   ```bash
   $ echo hello >> /opt/module/data/flume.log
   ```
   By appending data to `flume.log`, you simulate new log entries. Check the Kafka consumer to see if the data appears as expected, indicating successful integration and data flow.
